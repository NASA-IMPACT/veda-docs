[
  {
    "objectID": "services/index.html",
    "href": "services/index.html",
    "title": "Services",
    "section": "",
    "text": "The VEDA ecosystem of services provides a platform data products hosted in the VEDA data store and exposed via the VEDA Spatio-Temporal Asset Catalog (STAC).\nThe docs (this site) provide information about best practice in cloud-based Earth data science and platform-specific details about how to use and contribute information on VEDA.",
    "crumbs": [
      "Services"
    ]
  },
  {
    "objectID": "services/dashboard.html",
    "href": "services/dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "The VEDA Dashboard is a publishing platform for results of scientific analysis of Earth data at NASA, in the form of Dataset information pages (“Datasets”) and stories (“Discoveries”).",
    "crumbs": [
      "Services",
      "Dashboard"
    ]
  },
  {
    "objectID": "services/dashboard.html#contributing",
    "href": "services/dashboard.html#contributing",
    "title": "Dashboard",
    "section": "Contributing",
    "text": "Contributing\nPlease see our docs on Dashboard Configuration.",
    "crumbs": [
      "Services",
      "Dashboard"
    ]
  },
  {
    "objectID": "services/data-store.html",
    "href": "services/data-store.html",
    "title": "VEDA Data Store",
    "section": "",
    "text": "The VEDA Data Store consists of cloud object storage (AWS S3 in us-west-2) and a central Spatio-Temporal Asset Catalog (STAC) that exposes the datasets."
  },
  {
    "objectID": "services/data-store.html#dataset-selection",
    "href": "services/data-store.html#dataset-selection",
    "title": "VEDA Data Store",
    "section": "Dataset selection",
    "text": "Dataset selection\nThe VEDA Data Store is meant for\n\nNovel datasets produced by NASA Earth data scientists to be presented on the VEDA Dashboard or shared with other science users on VEDA, but not (yet) suited for publication in one of the EOSDIS Distributed Active Archive Centers (DAAC)\nNon-authoritative cloud-optimized versions of Datasets from a DAAC\nOther datasets that do not have an authoritative, cloud-optimized source, to be published or used within the VEDA platform"
  },
  {
    "objectID": "services/data-store.html#browsing-the-data-store",
    "href": "services/data-store.html#browsing-the-data-store",
    "title": "VEDA Data Store",
    "section": "Browsing the Data Store",
    "text": "Browsing the Data Store\nThe main public interface is the STAC browser and many APIs are exposed that provide access to the data using various protocols.\n\nSTAC browser: veda-staging-stac-browser"
  },
  {
    "objectID": "services/data-store.html#contributing-data",
    "href": "services/data-store.html#contributing-data",
    "title": "VEDA Data Store",
    "section": "Contributing data",
    "text": "Contributing data\nThe process of data ingestion into the VEDA Data Store is under active development.\nPlease see our docs on dataset ingestion."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection\nPass STAC item into raster API /stac/tilejson.json endpoint\nGet time series statistics over available time period to identify seasonal trends\nVisualize peak by displaying the tile in folium\nVisualize time series of raster images",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "About the Data",
    "text": "About the Data\nOcean Net Primary Production (NPP) is the result of CO2 fixation, through photosynthesis, by marine phytoplankton which contain chlorophyll. It is the proportion of phytoplankton-sequestered carbon that enters the oceanic food web and supports a variety of marine life.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "The Case Study - Walvis Bay, Namibia",
    "text": "The Case Study - Walvis Bay, Namibia\nWalvis Bay is home to Namibia’s largest marine farming center and a well established commercial fishing industry. It’s location in the nutrient-rich Benguela upwelling system of the Atlantic Ocean, means producers can rely on this area to cultivate an abundance of shellfish including oysters, mussels, and scallops.\nOccasionally the nutrient-rich waters of the Atlantic produce higher than normal NPP levels, resulting in short-lived harmful algal blooms. This is often a result of both favorable temperatures and abundance of sufficient nutrients. The resulting algal blooms can have severe consequences causing massive fish kills, contaminating seafood with toxins and creating an unsafe environment for humans and marine life. Toxins accumulated in the shellfish organs can be subsequently transmitted to humans through consumption and resulting in serious health threats.\nIn this example we explore the Ocean NPP dataset over the year 2020 to identify spatial and temporal patterns of Ocean NPP in the Walvis Bay area.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provife STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Ocean NPP\ncollection_name = \"MO_NPP_npp_vgpm\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'MO_NPP_npp_vgpm',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'}],\n 'title': '',\n 'extent': {'spatial': {'bbox': [[-180.0000050868518,\n     -90.00000508655744,\n     180.0000050868518,\n     89.9999974571629]]},\n  'temporal': {'interval': [['2020-01-01T00:00:00+00:00',\n     '2020-12-31T00:00:00+00:00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 1500]],\n   'colormap_name': 'jet'}},\n 'providers': [{'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2020-01-01T00:00:00Z', '2020-12-01T00:00:00Z'],\n  'cog_default': {'max': 34561.35546875, 'min': 14.516647338867188}},\n 'description': 'Ocean Net Primary Production (NPP): https://oceancolor.gsfc.nasa.gov/atbd/npp/',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/render/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\n# Verify frequency of data available\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\n# Get collection summary\ncollection[\"summaries\"]\n\n{'datetime': ['2020-01-01T00:00:00Z', '2020-12-01T00:00:00Z'],\n 'cog_default': {'max': 34561.35546875, 'min': 14.516647338867188}}\n\n\nGreat, we can explore the year 2020 time series. Let’s create a bounding box to explore the Walvis Bay area of interest (AOI) in Namibia\n\n# Walvis Bay, Namibia\nwalvis_bay_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                [13.686159004559698, -21.700046934333145],\n                [13.686159004559698, -23.241974326585833],\n                [14.753560168039911, -23.241974326585833],\n                [14.753560168039911, -21.700046934333145],\n                [13.686159004559698, -21.700046934333145],\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\nLet’s visualize the AOI we have just created using folium\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nfolium.GeoJson(walvis_bay_aoi, name=\"Walvis Bay\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nReturning back to our STAC API requests, let’s check how many total items are available.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 12 items\n\n\nThis makes sense is our collection is monthly, so we should have 12 total items.\n\n# Explore one item to see what it contains\nitems[0]\n\n{'id': 'A_202012.L3m_MO_NPP_npp_vgpm_4km',\n 'bbox': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm/items/A_202012.L3m_MO_NPP_npp_vgpm_4km'},\n  {'title': 'Map of Item',\n   'href': 'https://3hwvk17uek.execute-api.us-west-2.amazonaws.com/stac/map?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&rescale=0%2C1500&colormap_name=jet',\n   'rel': 'preview',\n   'type': 'text/html'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/MO_NPP_npp_vgpm/A_202012.L3m_MO_NPP_npp_vgpm_4km.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -32767.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 18305.302734375,\n      'min': 20.69771957397461,\n      'count': 11.0,\n      'buckets': [223827.0,\n       2620.0,\n       363.0,\n       105.0,\n       45.0,\n       20.0,\n       13.0,\n       4.0,\n       1.0,\n       4.0]},\n     'statistics': {'mean': 448.69620531977694,\n      'stddev': 450.1186820854004,\n      'maximum': 18305.302734375,\n      'minimum': 20.69771957397461,\n      'valid_percent': 43.29719543457031}}]},\n  'rendered_preview': {'title': 'Rendered preview',\n   'href': 'https://3hwvk17uek.execute-api.us-west-2.amazonaws.com/stac/preview.png?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&rescale=0%2C1500&colormap_name=jet',\n   'rel': 'preview',\n   'roles': ['overview'],\n   'type': 'image/png'}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180.0000050868518, -90.00000508655744],\n    [180.0000050868518, -90.00000508655744],\n    [180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, -90.00000508655744]]]},\n 'collection': 'MO_NPP_npp_vgpm',\n 'properties': {'proj:bbox': [-180.0000050868518,\n   -90.00000508655744,\n   180.0000050868518,\n   89.9999974571629],\n  'proj:epsg': 4326.0,\n  'proj:shape': [4320.0, 8640.0],\n  'end_datetime': '2020-12-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0000050868518, -90.00000508655744],\n     [180.0000050868518, -90.00000508655744],\n     [180.0000050868518, 89.9999974571629],\n     [-180.0000050868518, 89.9999974571629],\n     [-180.0000050868518, -90.00000508655744]]]},\n  'proj:transform': [0.041666667844178655,\n   0.0,\n   -180.0000050868518,\n   0.0,\n   -0.04166666725549082,\n   89.9999974571629,\n   0.0,\n   0.0,\n   1.0],\n  'start_datetime': '2020-12-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}\n\n\nNow that we have explored the collection metadata by querying the STAC API, we can use the RASTER API to access the data itself.\nNOTE: The RASTER_API expects AOI as a Feature or a FeatureCollection. The datatype of the input matches the datatype of the output. So if you use a FeatureCollection as input, you will get back a FeatureCollection.\n\ndef generate_stats(item, aoi):\n    \"\"\" Generate statistics for a particular item and AOI\n\n    NOTE: This function assumes that the AOI is a geojson `Feature`.\n    \"\"\"\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][\"cog_default\"][\"href\"]},\n        json=aoi,\n    ).json()\n\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\n%%time\nstats = [generate_stats(item, walvis_bay_aoi) for item in items]\n\nCPU times: user 153 ms, sys: 13.9 ms, total: 167 ms\nWall time: 9.62 s\n\n\nWith the function provided above, we can generate statistics for our AOI. In the example below, we’ll explore sample statistics available from one of the tiles.\n\nstats[1]\n\n{'statistics': {'b1': {'min': 3040.676025390625,\n   'max': 20105.990234375,\n   'mean': 7803.8864,\n   'count': 962.0,\n   'sum': 4877429.0,\n   'std': 3784.444606105174,\n   'median': 6578.16162109375,\n   'majority': 8848.3544921875,\n   'minority': 3040.676025390625,\n   'unique': 615.0,\n   'histogram': [[149.0, 154.0, 91.0, 76.0, 50.0, 37.0, 24.0, 26.0, 10.0, 8.0],\n    [3040.676025390625,\n     4747.20751953125,\n     6453.73876953125,\n     8160.2705078125,\n     9866.8017578125,\n     11573.3330078125,\n     13279.8642578125,\n     14986.3955078125,\n     16692.927734375,\n     18399.458984375,\n     20105.990234375]],\n   'valid_percent': 64.97,\n   'masked_pixels': 337.0,\n   'valid_pixels': 625.0,\n   'percentile_2': 3439.763544921875,\n   'percentile_98': 17349.302734374993}},\n 'start_datetime': '2020-11-01T00:00:00'}\n\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the full Ocean NPP time series available (January-December 2020) for the Walvis Bay area of Namibia. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly Ocean NPP values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NPP values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NPP values\",\n)\n\nplt.legend()\nplt.title(\"Ocean NPP Values for Walvis Bay, Namibia (2020)\")\n\nText(0.5, 1.0, 'Ocean NPP Values for Walvis Bay, Namibia (2020)')\n\n\n\n\n\n\n\n\n\nHere, we observe the seasonal variability in Ocean NPP for the Walvis Bay area. The larger peaks in the max values suggests the intensity of these events may vary spatially. Let’s explore one of the time steps (e.g., October) where there are higher maximum monthly NPP values to see if this is the case.\nImportant note: Keep in mind that the size and extent of your AOI will influence the ‘signal’ of your time series. If the phenomena you are investigating displays greater spatial variability a larger AOI will provide more ‘noise’ making it more difficult to detect.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Raster Imagery",
    "text": "Visualizing the Raster Imagery\nLet’s first explore a single tile during one of the relative peaks in October, where we observe an increased sustained peak in NPP values.\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2020-10-01T00:00:00\n\n\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\nrescale_values\n\n{'max': 34561.35546875, 'min': 14.516647338867188}\n\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'center': [0.0, -3.814697265625e-06, 0]}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFrom the image above, we see higher Ocean NPP values (displayed in teal) located in and around Walvis Bay and the surrounding shorelines - highlighting areas of concern for the local shellfish industry.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the raster time series",
    "text": "Visualizing the raster time series\nNow we will look at each of the raster tiles that make up this time series to explore the spatial and temporal patterns of Ocean NPP observed in Walvis Bay throughout 2020.\nWe used the code below to examine the tiles and the order in which they are presented.\n\nimport matplotlib.pyplot as plt\n\nfor item in items:\n    tiles = requests.get(\n        f\"{RASTER_API_URL}/stac/tilejson.json?collection={item['collection']}&item={item['id']}\"\n        \"&assets=cog_default\"\n        \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n        f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n    ).json()\n    print(tiles[\"tiles\"])\n\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202011.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202009.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202008.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202007.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202006.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202005.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202004.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202003.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202002.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202001.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n\n\nSince we found the tiles to be presented in reverse time order, we’ll revise this in the code below. We’ll use reversed() to do so.\n\nimport tempfile\nfrom datetime import datetime\nfrom IPython.display import display, Image\n\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"MO_NPP_npp_vgpm\"\n][0][\"summaries\"][\"cog_default\"]\n\n\nfor item in reversed(items):\n    response = requests.post(\n        f\"{RASTER_API_URL}/cog/feature\",\n        params={\n            \"format\": \"png\",\n            \"height\": 512,\n            \"width\": 512,\n            \"url\": item[\"assets\"][\"cog_default\"][\"href\"],\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n        json=walvis_bay_aoi,\n    )\n    assert response.ok, response.text\n    image_bytes = response.content\n\n    # formating the datetime to make for easier reading\n    datetime_str = item[\"properties\"][\"start_datetime\"]\n    datetime_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n    print(datetime_object.strftime(\"%B %Y\"))\n\n    display(Image(image_bytes, height=512, width=512))\n\nJanuary 2020\nFebruary 2020\nMarch 2020\nApril 2020\nMay 2020\nJune 2020\nJuly 2020\nAugust 2020\nSeptember 2020\nOctober 2020\nNovember 2020\nDecember 2020",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized the spatial and temporal variability of Ocean NPP values in the Benguela Current, which displays a seasonal pattern of peaking in the October, November, December, and January months when favorable temperatures and nutrient conditions are present.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing Ocean NPP time series for seasonal patterns"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html",
    "href": "notebooks/datasets/air-quality-covid.html",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "href": "notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#approach",
    "href": "notebooks/datasets/air-quality-covid.html#approach",
    "title": "Air Quality and COVID-19",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - NO₂\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#about-the-data",
    "href": "notebooks/datasets/air-quality-covid.html#about-the-data",
    "title": "Air Quality and COVID-19",
    "section": "About the Data",
    "text": "About the Data\nThis dataset is of monthly nitrogen dioxide (NO₂) levels values across the globe. Darker colors indicate higher NO₂ levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "href": "notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "title": "Air Quality and COVID-19",
    "section": "The Case Study - Air Quality and COVID-19",
    "text": "The Case Study - Air Quality and COVID-19\nIn this notebook, we’ll walk through the development of side-by-side comparisons of NO₂ levels before and after government lockdowns as demonstrated Seeing Rebounds in NO₂ in this VEDA Discovery story: Air Quality and COVID-19 available on the VEDA Dashboard.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "href": "notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "title": "Air Quality and COVID-19",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Nitrogen Oxide\ncollection_name = \"no2-monthly\"\n\n\n#Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2022-12-01 00:00:00+00']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2022-12-01T00:00:00Z'],\n  'cog_default': {'max': 50064805976866816, 'min': -10183824872833024}},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from January 2015 to December 2022. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Check total number of items available\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 84 items\n\n\nThis makes sense as there are 7 years between 2016 - 2022, with 12 months per year, meaning 84 records in total.\nBelow, we’ll use the cog_default values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\nrescale_values\n\n{'max': 50064805976866816, 'min': -10183824872833024}",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "href": "notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "title": "Air Quality and COVID-19",
    "section": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API",
    "text": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API\nWe will explore changes in air quality due to changes in human behaviour resulting from the COVID-19 pandemic. With people largely confined to their homes to reduce the spread of the novel coronavirus, scientists were anticipated there were likely to be fewer cars, planes, and ships emitting fossil fuel pollution. In this notebook, we’ll explore the impacts these government lockdowns had on specific air pollutants (i.e., NO₂) and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice times, once for February 2020 and again for February 2022, so that we can visualize each event independently.\n\nfebruary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-02']['collection']}&item={items['2020-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2020_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202002_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-10183824872833024%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\nfebruary_2022_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2022-02']['collection']}&item={items['2022-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2022_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202202_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-10183824872833024%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "href": "notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "title": "Air Quality and COVID-19",
    "section": "Seeing Rebounds in NO₂",
    "text": "Seeing Rebounds in NO₂\nAir pollutants with short lifespans, like NO₂, decreased dramatically with COVID-related shutdowns in the spring of 2020 (see lefthand side map). As the world began to re-open and mobility restrictions eased, travel increased and alongside it NO₂ pollutants. Air quality levels are now returning to pre-pandemic levels (see righthand side map).\nScroll and zoom within the maps below, the side-by-side comparison will follow wherever you explore. Darker purples indicate higher NO₂ levels and more activity. Lighter blues indicate lower levels of NO₂ and less activity.\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and map for NO2 Layer\nm = folium.plugins.DualMap(location=(33.6901, 118.9325), zoom_start=5)\n\n# February 2020\nmap_layer_2020 = TileLayer(\n    tiles=february_2020_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2020.add_to(m.m1)\n\n# February 2022\nmap_layer_2022 = TileLayer(\n    tiles=february_2022_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2022.add_to(m.m2)\n\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#summary",
    "href": "notebooks/datasets/air-quality-covid.html#summary",
    "title": "Air Quality and COVID-19",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors NO₂ emissions from space. By showcasing lockdown (February 2020) and post-lockdown (February 2022) snapshots of air quality side-by-side, we demonstrate how quickly atmospheric NO₂ responds to reductions in emissions and human behavior.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Air Quality and COVID-19"
    ]
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html",
    "href": "notebooks/templates/template-using-the-raster-api.html",
    "title": "VEDA Documentation",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that use the raster API. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#run-this-notebook",
    "href": "notebooks/templates/template-using-the-raster-api.html#run-this-notebook",
    "title": "VEDA Documentation",
    "section": "Run this notebook",
    "text": "Run this notebook\nYou can launch this notbook using mybinder, by clicking the button below.\n  \n\nFill in the text in italics in the following cells"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#approach",
    "href": "notebooks/templates/template-using-the-raster-api.html#approach",
    "title": "VEDA Documentation",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "href": "notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "title": "VEDA Documentation",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "href": "notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "title": "VEDA Documentation",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. In some notebooks we get the collection and use all the items, and in others we search for specific items."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "href": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "title": "VEDA Documentation",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_id}\").json()\ncollection"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "href": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "title": "VEDA Documentation",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "href": "notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "title": "VEDA Documentation",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm\n\n\nWith the STAC object and optionally the AOI in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is one idea though :)"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#use-stactilejson.json-to-get-tiles",
    "href": "notebooks/templates/template-using-the-raster-api.html#use-stactilejson.json-to-get-tiles",
    "title": "VEDA Documentation",
    "section": "Use /stac/tilejson.json to get tiles",
    "text": "Use /stac/tilejson.json to get tiles\nWe pass the item_id, collection id, and the rescale_values in to the RASTER API endpoint and get back a tile.\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={collection_id}&item={item['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\ntiles\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    attr=\"VEDA\",\n)"
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html",
    "href": "notebooks/veda-operations/stac-collection-creation.html",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC Collection Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "href": "notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC Collection Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "href": "notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "title": "STAC Collection Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U pystac nbss-upload --quiet\n\n\nfrom datetime import datetime, timezone\nimport pystac",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC Collection Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "href": "notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "title": "STAC Collection Creation",
    "section": "Create pystac.Collection",
    "text": "Create pystac.Collection\nIn this section we will be creating a pystac.Collection object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string and boolean fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nTITLE = \"NO₂ (Diff)\"\nDESCRIPTION = (\n    \"This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors \"\n    \"indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. \"\n    \"Missing pixels indicate areas of no data most likely associated with \"\n    \"cloud cover or snow.\"\n)\nDASHBOARD__IS_PERIODIC = True\nDASHBOARD__TIME_DENSITY = \"month\"\nLICENSE = \"CC0-1.0\"\n\n\n\nExtents\nThe extents indicate the start (and potentially end) times of the data as well as the footprint of the data.\n\n# Time must be in UTC\ndemo_time = datetime.now(tz=timezone.utc)\n\nextent = pystac.Extent(\n    pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]]),\n    pystac.TemporalExtent([[demo_time, None]]),\n)\n\n\n\nProviders\nWe know that the data host, processor, and producter is “VEDA”, but you can include other providers that fill other roles in the data creation pipeline.\n\nproviders = [\n    pystac.Provider(\n        name=\"VEDA\",\n        roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST],\n        url=\"https://github.com/nasa-impact/veda-data-pipelines\",\n    )\n]\n\n\n\nPut it together\nNow take your constants and the extents and providers and create a pystac.Collection\n\ncollection = pystac.Collection(\n    id=COLLECTION_ID,\n    title=TITLE,\n    description=DESCRIPTION,\n    extra_fields={\n        \"dashboard:is_periodic\": DASHBOARD__IS_PERIODIC,\n        \"dashboard:time_density\": DASHBOARD__TIME_DENSITY,\n    },\n    license=LICENSE,\n    extent=extent,\n    providers=providers,\n)\n\n\n\nTry it out!\nNow that you have a collection you can try it out and make sure that it looks how you expect and that it passes validation checks.\n\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.0.0/collection-spec/json-schema/collection.json']\n\n\n\ncollection.to_dict()\n\n{'type': 'Collection',\n 'id': 'no2-monthly-diff',\n 'stac_version': '1.0.0',\n 'description': 'This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'links': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month',\n 'title': 'NO₂ (Diff)',\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2023-06-12T17:36:30.161697Z', None]]}},\n 'license': 'CC0-1.0',\n 'providers': [{'name': 'VEDA',\n   'roles': [&lt;ProviderRole.PRODUCER: 'producer'&gt;,\n    &lt;ProviderRole.PROCESSOR: 'processor'&gt;,\n    &lt;ProviderRole.HOST: 'host'&gt;],\n   'url': 'https://github.com/nasa-impact/veda-data-pipelines'}]}",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC Collection Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "href": "notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "title": "STAC Collection Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-collection.ipynb",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC Collection Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#run-this-notebook",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#run-this-notebook",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#approach",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#approach",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Approach",
    "text": "Approach\nThis notebook creates STAC collection metadata for a CMIP6 Kerchunk Reference File which has already been generated and stored in S3.\nThis notebook serves as documentation for the publication of the CMIP6 kerchunk reference. It is not expected to generalize for arbitrary Zarr datasets but may be a helpful example. It was run on the VEDA JupyterHub and since veda-data-store-staging is a protected bucket it is not expected to work in an environment without access to that bucket.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-1-install-and-import-necessary-libraries",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-1-install-and-import-necessary-libraries",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 1: Install and import necessary libraries",
    "text": "Step 1: Install and import necessary libraries\n\n#!pip install xstac\nimport pystac\nimport requests\nimport s3fs\nimport xstac\nimport fsspec\nimport xarray as xr",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-2-open-the-dataset-with-xarray",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-2-open-the-dataset-with-xarray",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 2: Open the dataset with xarray",
    "text": "Step 2: Open the dataset with xarray\n\ndataset_url = 's3://veda-data-store-staging/cmip6-GISS-E2-1-G-tas-kerchunk/combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json'\n\nxr_open_args = {\n    \"engine\": \"zarr\",\n    \"decode_coords\": \"all\",\n    \"consolidated\": False\n}\n\nfs = fsspec.filesystem(\n    \"reference\",\n    fo=dataset_url,\n    remote_options={\"anon\": True},\n)\nsrc_path = fs.get_mapper(\"\")\n\nds = xr.open_dataset(src_path, **xr_open_args)\n\n/tmp/ipykernel_5419/732403854.py:16: UserWarning: Variable(s) referenced in cell_measures not in variables: ['areacella']\n  ds = xr.open_dataset(src_path, **xr_open_args)",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-3-generate-stac-metadata",
    "href": "notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-3-generate-stac-metadata",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 3: Generate STAC metadata",
    "text": "Step 3: Generate STAC metadata\nThe spatial extent is taken from the xarray metadata. The temporal extent will be added by the xstac library.\n\nspatial_extent_values = [ds.lon[0].values, ds.lat[0].values, ds.lon[-1].values, ds.lat[-1].values]\nspatial_extent = list(map(int, spatial_extent_values))\n_id = 'combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk_TEST'\nzarr_asset = pystac.Asset(\n    title='zarr',\n    href=dataset_url,\n    media_type='application/vnd+zarr',\n    roles=['data'],\n)\nextent = pystac.Extent(\n    spatial=pystac.SpatialExtent(bboxes=[spatial_extent]),\n    temporal=pystac.TemporalExtent([[None, None]])\n)\n\nAdd the VEDA provider.\n\nproviders = [\n    pystac.Provider(\n        name=\"VEDA\",\n        roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST],\n        url=\"https://github.com/nasa-impact/veda-data-pipelines\",\n    )\n]\n\nPut it all together to intialize a pystac.Collection instance.\n\ncollection = pystac.Collection(\n    id=_id,\n    extent=extent,\n    assets = {'zarr': zarr_asset},\n    description='for zarr testing',\n    providers=providers,\n    stac_extensions=['https://stac-extensions.github.io/datacube/v2.0.0/schema.json'],\n    license=\"CC0-1.0\"\n)\n\nThat collection instance is used by xstac to generate additional metadata, such as the temporal extent and the datacube extension information.\n\ncollection_template = collection.to_dict()\ncollection = xstac.xarray_to_stac(\n    ds,\n    collection_template,\n    temporal_dimension=\"time\",\n    x_dimension=\"lon\",\n    y_dimension=\"lat\",\n    # TODO: get this from attributes if possible\n    reference_system=\"4326\",\n    validate=False\n)\n# It should validate, yay!\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.0.0/collection-spec/json-schema/collection.json',\n 'https://stac-extensions.github.io/datacube/v2.0.0/schema.json']",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "Publishing a CMIP6 Kerchunk Reference to STAC"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html",
    "href": "notebooks/tutorials/mapping-fires.html",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#run-this-notebook",
    "href": "notebooks/tutorials/mapping-fires.html#run-this-notebook",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#approach",
    "href": "notebooks/tutorials/mapping-fires.html#approach",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Approach",
    "text": "Approach\n\nUse OWSLib to determine what data is available and inspect the metadata\nUse OWSLib to filter and read the data\nUse geopandas and folium to analyze and plot the data\n\nNote that the default examples environment is missing one requirement: oswlib. We can pip install that before we move on.\n\n!pip install OWSLib==0.28.1 --quiet\n\n\nfrom owslib.ogcapi.features import Features\nimport geopandas as gpd\nimport datetime as dt\nfrom datetime import datetime, timedelta",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#about-the-data",
    "href": "notebooks/tutorials/mapping-fires.html#about-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "About the Data",
    "text": "About the Data\nThe fire data shown is generated by the FEDs algorithm. The FEDs algorithm tracks fire movement and severity by ingesting observations from the VIIRS thermal sensors on the Suomi NPP and NOAA-20 satellites. This algorithm uses raw VIIRS observations to generate a polygon of the fire, locations of the active fire line, and estimates of fire mean Fire Radiative Power (FRP). The VIIRS sensors overpass at ~1:30 AM and PM local time, and provide estimates of fire evolution ~ every 12 hours. The data produced by this algorithm describe where fires are in space and how fires evolve through time. This CONUS-wide implementation of the FEDs algorithm is based on Chen et al 2020’s algorithm for California.\nThe data produced by this algorithm is considered experimental.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "href": "notebooks/tutorials/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Look at the data that is availible through the OGC API",
    "text": "Look at the data that is availible through the OGC API\nThe datasets that are distributed throught the OGC API are organized into collections. We can display the collections with the command:\n\nOGC_URL = \"https://firenrt.delta-backend.com\"\n\nw = Features(url=OGC_URL)\nw.feature_collections()\n\n['public.eis_fire_snapshot_fireline_nrt',\n 'public.eis_fire_lf_fireline_archive',\n 'public.eis_fire_lf_perimeter_archive',\n 'public.eis_fire_snapshot_perimeter_nrt',\n 'public.eis_fire_lf_newfirepix_nrt',\n 'public.eis_fire_lf_nfplist_nrt',\n 'public.eis_fire_lf_perimeter_nrt',\n 'public.eis_fire_lf_nfplist_archive',\n 'public.eis_fire_lf_newfirepix_archive',\n 'public.eis_fire_snapshot_newfirepix_nrt',\n 'public.eis_fire_lf_fireline_nrt',\n 'public.st_squaregrid',\n 'public.st_hexagongrid',\n 'public.st_subdivide']\n\n\nWe will focus on the public.eis_fire_snapshot_fireline_nrt collection, the public.eis_fire_snapshot_perimeter_nrt collection, and the public.eis_fire_lf_perimeter_archive collection here.\n\nInspect the metatdata for public.eis_fire_snapshot_perimeter_nrt collection\nWe can access information that describes the public.eis_fire_snapshot_perimeter_nrt table.\n\nperm = w.collection(\"public.eis_fire_snapshot_perimeter_nrt\")\n\nWe are particularly interested in the spatial and temporal extents of the data.\n\nperm[\"extent\"]\n\n{'spatial': {'bbox': [[-124.63103485107422,\n    24.069578170776367,\n    -62.97413635253906,\n    49.40156555175781]],\n  'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84'},\n 'temporal': {'interval': [['2023-06-07T00:00:00+00:00',\n    '2023-06-27T00:00:00+00:00']],\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian'}}\n\n\nIn addition to getting metadata about the data we can access the queryable fields. Each of these fields will represent a column in our dataframe.\n\nperm_q = w.collection_queryables(\"public.eis_fire_snapshot_perimeter_nrt\")\nperm_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'duration': {'name': 'duration', 'type': 'number'},\n 'farea': {'name': 'farea', 'type': 'number'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'flinelen': {'name': 'flinelen', 'type': 'number'},\n 'fperim': {'name': 'fperim', 'type': 'number'},\n 'isactive': {'name': 'isactive', 'type': 'number'},\n 'meanfrp': {'name': 'meanfrp', 'type': 'number'},\n 'n_newpixels': {'name': 'n_newpixels', 'type': 'number'},\n 'n_pixels': {'name': 'n_pixels', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 'pixden': {'name': 'pixden', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#filter-the-data",
    "href": "notebooks/tutorials/mapping-fires.html#filter-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Filter the data",
    "text": "Filter the data\nIt is always a good idea to do any data filtering as early as possible. In this example we know that we want the data for particular spatial and temporal extents. We can apply those and other filters using the OWSLib package.\nIn the below example we are:\n\nchoosing the public.eis_fire_snapshot_perimeter_nrt collection\nsubsetting it by space using the bbox parameter\nsubsetting it by time using the datetime parameter\nfiltering for fires over 5km^2 and over 2 days long using the filter parameter. The filter parameter lets us filter by the columns in ‘public.eis_fire_snapshot_perimeter_nrt’ using SQL-style queries.\n\nNOTE: The limit parameter desginates the maximum number of objects the query will return. The default limit is 10, so if we want to all of the fire perimeters within certain conditions, we need to make sure that the limit is large.\n\n## Get the most recent fire perimeters, and 7 days before most recent fire perimeter\nmost_recent_time = max(*perm[\"extent\"][\"temporal\"][\"interval\"])\nnow = dt.datetime.strptime(most_recent_time, \"%Y-%m-%dT%H:%M:%S+00:00\")\nlast_week = now - dt.timedelta(weeks=1)\nlast_week = dt.datetime.strftime(last_week, \"%Y-%m-%dT%H:%M:%S+00:00\")\nprint(\"Most Recent Time =\", most_recent_time)\nprint(\"Last week =\", last_week)\n\nMost Recent Time = 2023-06-27T00:00:00+00:00\nLast week = 2023-06-20T00:00:00+00:00\n\n\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",  # name of the dataset we want\n    bbox=[\"-106.8\", \"24.5\", \"-72.9\", \"37.3\"],  # coodrinates of bounding box,\n    datetime=[last_week + \"/\" + most_recent_time],  # date range\n    limit=1000,  # max number of items returned\n    filter=\"farea&gt;5 AND duration&gt;2\",  # additional filters based on queryable fields\n)\n\nThe result is a dictionary containing all of the data and some summary fields. We can look at the keys to see what all is in there.\n\nperm_results.keys()\n\ndict_keys(['type', 'id', 'title', 'description', 'numberMatched', 'numberReturned', 'links', 'features'])\n\n\nFor instance you can check the total number of matched items and make sure that it is equal to the number of returned items. This is how you know that the limit you defined above is high enough.\n\nperm_results[\"numberMatched\"] == perm_results[\"numberReturned\"]\n\nTrue\n\n\nYou can also access the data directly in the browser or in an HTTP GET call using the constructed link.\n\nperm_results[\"links\"][1][\"href\"]\n\n'https://firenrt.delta-backend.com/collections/public.eis_fire_snapshot_perimeter_nrt/items?bbox=-106.8%2C24.5%2C-72.9%2C37.3&datetime=2023-06-20T00%3A00%3A00%2B00%3A00%2F2023-06-27T00%3A00%3A00%2B00%3A00&limit=1000&filter=farea%3E5+AND+duration%3E2'",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#read-data",
    "href": "notebooks/tutorials/mapping-fires.html#read-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Read data",
    "text": "Read data\nIn addition to all the summary fields, the perm_results dict contains all the data. We can pass the data into geopandas to make it easier to interact with.\n\ndf = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\ndf\n\n\n\n\n\n\n\n\ngeometry\nduration\nfarea\nfireid\nflinelen\nfperim\nisactive\nmeanfrp\nn_newpixels\nn_pixels\nogc_fid\npixden\nt\n\n\n\n\n0\nPOLYGON ((-95.88101 30.22697, -95.88102 30.227...\n45.0\n9.811672\n63986\n0.0\n12.124017\n1\n0.0\n0\n104\n430\n10.599620\n2023-06-26T12:00:00\n\n\n1\nPOLYGON ((-92.83274 29.81987, -92.83264 29.820...\n4.0\n8.958987\n80714\n0.0\n12.583987\n0\n0.0\n0\n63\n538\n7.032045\n2023-06-20T12:00:00\n\n\n2\nPOLYGON ((-89.29801 36.86458, -89.29801 36.864...\n5.0\n7.138301\n80749\n0.0\n13.474343\n0\n0.0\n0\n25\n1902\n3.502234\n2023-06-21T12:00:00\n\n\n3\nPOLYGON ((-106.33318 36.38887, -106.32771 36.3...\n5.5\n11.325657\n82393\n0.0\n18.949202\n1\n0.0\n0\n157\n5271\n13.862330\n2023-06-26T00:00:00\n\n\n4\nPOLYGON ((-106.00861 36.60666, -106.00859 36.6...\n11.0\n12.858160\n79918\n0.0\n20.962730\n1\n0.0\n0\n85\n5273\n6.610588\n2023-06-25T12:00:00\n\n\n5\nMULTIPOLYGON (((-101.28638 32.47797, -101.2863...\n14.0\n9.415797\n78723\n0.0\n36.674171\n1\n0.0\n0\n114\n5647\n12.107313\n2023-06-26T00:00:00\n\n\n6\nMULTIPOLYGON (((-102.69888 31.72183, -102.6987...\n27.0\n10.484613\n71936\n0.0\n17.485984\n1\n0.0\n0\n123\n5736\n11.731477\n2023-06-25T12:00:00\n\n\n7\nPOLYGON ((-103.44609 31.79658, -103.44608 31.7...\n17.0\n23.515158\n76919\n0.0\n22.167102\n1\n0.0\n0\n10\n5774\n0.425258\n2023-06-25T00:00:00\n\n\n8\nPOLYGON ((-103.89600 32.08170, -103.89603 32.0...\n7.0\n11.917955\n79765\n0.0\n21.145696\n0\n0.0\n0\n7\n5806\n0.587349\n2023-06-21T00:00:00\n\n\n9\nPOLYGON ((-106.63614 30.07301, -106.63617 30.0...\n3.5\n76.317924\n83254\n0.0\n51.647117\n1\n0.0\n0\n341\n5881\n4.468151\n2023-06-26T00:00:00\n\n\n10\nPOLYGON ((-106.78326 30.05115, -106.78330 30.0...\n2.5\n10.102581\n83084\n0.0\n13.674702\n1\n0.0\n0\n72\n5901\n7.126891\n2023-06-24T12:00:00\n\n\n11\nPOLYGON ((-102.85583 27.81858, -102.85601 27.8...\n8.0\n33.159692\n81431\n0.0\n25.731754\n1\n0.0\n0\n340\n5969\n10.253412\n2023-06-26T00:00:00\n\n\n12\nPOLYGON ((-97.43908 28.43692, -97.43900 28.436...\n19.0\n5.312484\n73340\n0.0\n10.166578\n0\n0.0\n0\n40\n6187\n7.529434\n2023-06-20T12:00:00\n\n\n13\nPOLYGON ((-100.71656 25.53519, -100.71659 25.5...\n6.5\n5.131956\n79873\n0.0\n8.995199\n0\n0.0\n0\n98\n6423\n19.096034\n2023-06-21T00:00:00\n\n\n14\nPOLYGON ((-102.25288 26.97686, -102.25289 26.9...\n6.0\n10.666481\n82098\n0.0\n13.479402\n1\n0.0\n0\n73\n6470\n6.843869\n2023-06-26T00:00:00\n\n\n15\nPOLYGON ((-106.64036 25.02528, -106.64036 25.0...\n14.5\n9.843044\n78547\n0.0\n18.376243\n1\n0.0\n0\n62\n7118\n6.298864\n2023-06-26T00:00:00\n\n\n16\nPOLYGON ((-106.52000 24.91752, -106.52000 24.9...\n12.5\n16.358760\n79217\n0.0\n19.469750\n1\n0.0\n0\n138\n7129\n8.435847\n2023-06-25T12:00:00\n\n\n17\nPOLYGON ((-106.47703 24.93926, -106.47705 24.9...\n7.0\n5.051376\n82013\n0.0\n10.648427\n1\n0.0\n0\n47\n7143\n9.304395\n2023-06-26T12:00:00",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#explore-data",
    "href": "notebooks/tutorials/mapping-fires.html#explore-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Explore data",
    "text": "Explore data\nWe can quickly explore the data by setting the coordinate reference system (crs) and using .explore()\n\ndf = df.set_crs(\"EPSG:4326\")\ndf.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "href": "notebooks/tutorials/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize Most Recent Fire Perimeters with Firelines",
    "text": "Visualize Most Recent Fire Perimeters with Firelines\nIf we wanted to combine collections to make more informative analyses, we can use some of the same principles.\nFirst we’ll get the queryable fields, and the extents:\n\nfline_q = w.collection_queryables(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_collection = w.collection(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'mergeid': {'name': 'mergeid', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}\n\n\n\nRead\nThen we’ll use those fields to get most recent fire perimeters and fire lines.\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",\n    datetime=most_recent_time,\n    limit=1000,\n)\nperimeters = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\n\n## Get the most recent fire lines\nperimeter_ids = perimeters.fireid.unique()\nperimeter_ids = \",\".join(map(str, perimeter_ids))\n\nfline_results = w.collection_items(\n    \"public.eis_fire_snapshot_fireline_nrt\",\n    limit=1000,\n    filter=\"fireid IN (\"\n    + perimeter_ids\n    + \")\",  # only the fires from the fire perimeter query above\n)\nfline = gpd.GeoDataFrame.from_features(fline_results[\"features\"])\n\n\n\nVisualize\n\nperimeters = perimeters.set_crs(\"epsg:4326\")\nfline = fline.set_crs(\"epsg:4326\")\n\nm = perimeters.explore()\nm = fline.explore(m=m, color=\"orange\")\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "href": "notebooks/tutorials/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize the Growth of the Camp Fire",
    "text": "Visualize the Growth of the Camp Fire\nWe may be interested in understanding how a fire evolved through time. To do this, we can work with the “Large fire” or “lf” perimeter collections. The public.eis_fire_lf_perimeter_nrt collection has the full spread history of fires from this year. public.eis_fire_lf_perimeter_archive has the full spread history of fires from 2018-2021 that were in the Western United States. The Camp Fire was in 2018, so we will work with the public.eis_fire_lf_perimeter_archive collection.\nWe can start by querying with information specific to the Camp Fire, like it’s genreal region (Northern California), and when it was active (November 2018). With that information, we can get the fireID associated with the Camp Fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",  \n    bbox=[\"-124.52\", \"39.2\", \"-120\", \"42\"],  # North California bounding box,\n    datetime=[\"2018-11-01T00:00:00+00:00/2018-11-30T12:00:00+00:00\"], \n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by = \"t\", ascending = False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nprint(perimeters.fireid.unique())\nm = perimeters.explore(style_kwds = {'fillOpacity':0})\nm\n\n['F17028' 'F18493']\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nBased on the map, we know that the fireID for the Camp Fire is “F17028”. We can use that to directly query for that particular fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",\n    filter=\"fireid = 'F17028'\",\n    datetime=[\"2018-01-01T00:00:00+00:00/2018-12-31T12:00:00+00:00\"],\n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by = \"t\", ascending = False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nm = perimeters.explore(style_kwds = {'fillOpacity':0})\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#download-data",
    "href": "notebooks/tutorials/mapping-fires.html#download-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Download Data",
    "text": "Download Data\nDownloading pre-filtered data may be useful for working locally, or for working with the data in GIS software.\nWe can download the dataframe we made by writing it out into a shapefile or into a GeoJSON file.\nperimeters.to_file('perimeters.shp') \nperimeters.to_file('perimeters.geojson', driver='GeoJSON')",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/mapping-fires.html#collection-information",
    "href": "notebooks/tutorials/mapping-fires.html#collection-information",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Collection Information",
    "text": "Collection Information\nThe API hosts 9 different collections. There are four different types of data, and three different time-scales availible for querying through the API. “*snapshot*” collections are useful for visualizing the most recent data. It contains the most recent fires perimeters, active firelines, or VIIRS observations within the last 20 days. “*lf*” collections (short for Large Fire), show every fire perimeter, active fire line, or VIIRS observations for fires over 5 km^2. Collections that end in *archive are for year 2018 - 2021 across the Western United States. Collections with the *nrt ending are for CONUS from this most recent year. FireIDs are consistent only between layers with the same timescale (snapshot, lf_*nrt, and lf_archive*).\npublic.eis_fire_snapshot_perimeter_nrt\nPerimeter of cumulative fire-area. Most recent perimeter from the last 20 days.\npublic.eis_fire_lf_perimeter_nrt\nPerimeter of cumulative fire-area, from fires over 5 km^2. Every fire perimeter from current year to date.\npublic.eis_fire_lf_perimeter_archive\nPerimeter of cumulative fire-area, from fires over 5 km^2 in the Western United States. Every fire perimeter from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nmeanfrp\nMean fire radiative power. The weighted sum of the fire radiative power detected at each new pixel, divided by the number of pixels. If no new pixels are detected, meanfrp is set to zero.\nMW/(pixel area)\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\npixden\nNumber of pixels divided by area of perimeter.\npixels/Km^2\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nKm\n\n\nfperim\nLength of fire perimeter.\nKm\n\n\nfarea\nArea within fire perimeter.\nKm^2\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\nn_pixels\nNumber of pixel-detections in history of fire.\npixels\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\ngeometry\nThe shape of the perimeter.\nGeometry\n\n\n\npublic.eis_fire_snapshot_fireline_nrt\nActive fire line as estimated by new VIIRS detections. Most fire line from the last 20 days.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2. Every fire line from current year to date.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2 in the Western United States. Every fire line from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID\n\n\n\npublic.eis_fire_snapshot_newfirepix_nrt\nNew pixel detections that inform the most recent time-step’s perimeter and fireline calculation from the last 20 days.\npublic.eis_fire_lf_newfirepix_nrt\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible from start of current year to date.\npublic.eis_fire_lf_newfirepix_archive\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible for Western United States from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html",
    "href": "notebooks/tutorials/stac_ipyleaflet.html",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "It is recommended to run through this tutorial using VEDA’s Pangeo Notebook image, which already includes the stac_ipyleaflet library. When working within the notebook, be sure to select the nasa-veda-singleuser kernel.\nTo access VEDA’s JupyterHub Environment, please refer to the “Getting access…” section of our documentation.\nFor reference: stac_ipyleafet repository\n\n\n\n\nimport stac_ipyleaflet\nm = stac_ipyleaflet.StacIpyleaflet()\nm\n\nThe stac ipyleaflet notebook’s user interface consists of a map and a custom set of tools to aid in the discovery and visualization of STAC datasets and pre-determined Basemaps.\n\n\n\n\n\n\n\npress and hold a mouse-click, then drag the map\n\n\n\n\n\nclick the Zoom In / Out buttons in the top left-corner (this will maintain the center)\nuse your mouse’s scroll-wheel - hovering over an area of interest\ndouble-click within the map on an area of interest\nwhile pressing the shift key on your keyboard, press and hold a mouse-click, then drag to draw a rectangle around the area of interest\n\n\n\n\n\n\nPressing the Layers button at the top opens the Layers widget that consists of 2 tabs. This tool currently allows users to: - View Pre-defined Layers at the same time to see different combinations (currently, there are none for VEDA). - Choose between common Basemap Layers that are known favorites. - Have full control over the opacity of any layer or basemap for fine-tuning how the map looks.\n\n\n\nToggle each layer’s visibility by using its checkbox\nAdjust each layer’s opacity by moving its slider\n\n\n\n\n\nSelect a basemap from the dropdown\nAdjust the basemap’s opacity by moving its slider\n\n\n\n\n\n\nPressing the STAC Data button at the top opens the STAC widget that consists of 2 tabs. This tool currently allows users to: - Connect to the VEDA STAC to access collections of mission data. - Discover items per the selected collection, including description, available dates, & direct URL. - Identify valid COG datasets. - Add COG tiles dynamically to the map. - Customize the tiles by changing the selected color palette for the selected item.\n\n\n\nSelect a Collection within the default STAC library.\nBrowse through the Collection’s details.\nSelect an item from the collection to check if it is a valid COG. If it is, the Display button will become active (available) to add the selected item to the map. The displayed STAC layer’s opacity can be adjusted by moving its slider.\n\n\n\n\n\nSelect a category from the dropdown.\nSelect an item from the corresponding color palettes.\nPress the Display button to update the data on the map.\n\n\n\n\n\n\n\nActivate the Interact Tools (click on the top Interact button)\nFrom within the Point tab: Use your mouse to activate the Point tool; then click on the map at a location of interest\n\nCoordinates will be printed within the open tab\nRaster cell values will be identified and printed, if raster layers are on\n\nFrom within the Area tab: Use your mouse to activate the Polygon tool; then click, hold and draw a polygon over the map - releasing to finish\n\nThe area of interest’s Coordinates & BBox within the open tab\nAlternatively Print the area of interest’s bbox from within a cell:\n\n\nClear the point or polygon graphics as needed",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#run-this-notebook",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#run-this-notebook",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "It is recommended to run through this tutorial using VEDA’s Pangeo Notebook image, which already includes the stac_ipyleaflet library. When working within the notebook, be sure to select the nasa-veda-singleuser kernel.\nTo access VEDA’s JupyterHub Environment, please refer to the “Getting access…” section of our documentation.\nFor reference: stac_ipyleafet repository",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#import-the-library-use-the-map",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#import-the-library-use-the-map",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "import stac_ipyleaflet\nm = stac_ipyleaflet.StacIpyleaflet()\nm\n\nThe stac ipyleaflet notebook’s user interface consists of a map and a custom set of tools to aid in the discovery and visualization of STAC datasets and pre-determined Basemaps.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#map-navigation",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#map-navigation",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "press and hold a mouse-click, then drag the map\n\n\n\n\n\nclick the Zoom In / Out buttons in the top left-corner (this will maintain the center)\nuse your mouse’s scroll-wheel - hovering over an area of interest\ndouble-click within the map on an area of interest\nwhile pressing the shift key on your keyboard, press and hold a mouse-click, then drag to draw a rectangle around the area of interest",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#layers-tool",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#layers-tool",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Pressing the Layers button at the top opens the Layers widget that consists of 2 tabs. This tool currently allows users to: - View Pre-defined Layers at the same time to see different combinations (currently, there are none for VEDA). - Choose between common Basemap Layers that are known favorites. - Have full control over the opacity of any layer or basemap for fine-tuning how the map looks.\n\n\n\nToggle each layer’s visibility by using its checkbox\nAdjust each layer’s opacity by moving its slider\n\n\n\n\n\nSelect a basemap from the dropdown\nAdjust the basemap’s opacity by moving its slider",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#stac-discovery-tool",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#stac-discovery-tool",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Pressing the STAC Data button at the top opens the STAC widget that consists of 2 tabs. This tool currently allows users to: - Connect to the VEDA STAC to access collections of mission data. - Discover items per the selected collection, including description, available dates, & direct URL. - Identify valid COG datasets. - Add COG tiles dynamically to the map. - Customize the tiles by changing the selected color palette for the selected item.\n\n\n\nSelect a Collection within the default STAC library.\nBrowse through the Collection’s details.\nSelect an item from the collection to check if it is a valid COG. If it is, the Display button will become active (available) to add the selected item to the map. The displayed STAC layer’s opacity can be adjusted by moving its slider.\n\n\n\n\n\nSelect a category from the dropdown.\nSelect an item from the corresponding color palettes.\nPress the Display button to update the data on the map.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/stac_ipyleaflet.html#interact-with-the-map",
    "href": "notebooks/tutorials/stac_ipyleaflet.html#interact-with-the-map",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Activate the Interact Tools (click on the top Interact button)\nFrom within the Point tab: Use your mouse to activate the Point tool; then click on the map at a location of interest\n\nCoordinates will be printed within the open tab\nRaster cell values will be identified and printed, if raster layers are on\n\nFrom within the Area tab: Use your mouse to activate the Polygon tool; then click, hold and draw a polygon over the map - releasing to finish\n\nThe area of interest’s Coordinates & BBox within the open tab\nAlternatively Print the area of interest’s bbox from within a cell:\n\n\nClear the point or polygon graphics as needed",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Using the stac_ipyleaflet mapping library"
    ]
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Converting NetCDF to COG (CMIP6)"
    ]
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Approach",
    "text": "Approach\nCloud-optimized GeoTIFF (COG) is a geospatial raster (image) data format optimized for on-the-fly analytics and visualization of raster data in cloud applications.\nConverting NetCDF (climate data) to COG can be relevant when the data should be included in GIS or web map applications.\nThis tutorial shows how this conversion can be done using Xarray and rioxarray, in-memory, avoiding temporary files on-disk.\n\nStep-by-step guide to conversion from NetCDF to Cloud-Optimized GeoTIFF\nCombined workflow including upload to S3",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Converting NetCDF to COG (CMIP6)"
    ]
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Step by step",
    "text": "Step by step\n\nStep 0 - Installs and imports\n\n!pip install s3fs h5netcdf --quiet\n\n\nimport boto3\nimport s3fs\nimport rioxarray\nimport rasterio\nimport rio_cogeo.cogeo\nimport xarray as xr\nfrom rasterio.io import MemoryFile\n\n\n\nStep 1 - Inspect source NetCDF\n\nSOURCE_URI = 'cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc'\n\n\nfs = s3fs.S3FileSystem()\nfileobj = fs.open(SOURCE_URI)\nds = xr.open_dataset(fileobj, engine=\"h5netcdf\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 600, lon: 1440)\nCoordinates:\n  * lat      (lat) float64 -59.88 -59.62 -59.38 -59.12 ... 89.38 89.62 89.88\n  * lon      (lon) float64 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\nData variables: (12/13)\n    FFMC     (lat, lon) float32 ...\n    DMC      (lat, lon) float32 ...\n    DC       (lat, lon) float32 ...\n    ISI      (lat, lon) float32 ...\n    BUI      (lat, lon) float32 ...\n    FWI      (lat, lon) float32 ...\n    ...       ...\n    FWI_N30  (lat, lon) uint16 ...\n    FWI_N45  (lat, lon) uint16 ...\n    FWI_P25  (lat, lon) float32 ...\n    FWI_P50  (lat, lon) float32 ...\n    FWI_P75  (lat, lon) float32 ...\n    FWI_P95  (lat, lon) float32 ...xarray.DatasetDimensions:lat: 600lon: 1440Coordinates: (2)lat(lat)float64-59.88 -59.62 ... 89.62 89.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([-59.875, -59.625, -59.375, ...,  89.375,  89.625,  89.875])lon(lon)float640.125 0.375 0.625 ... 359.6 359.9units :degrees_eaststandard_name :longitudelong_name :longitudeaxis :Xarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02])Data variables: (13)FFMC(lat, lon)float32...[864000 values with dtype=float32]DMC(lat, lon)float32...[864000 values with dtype=float32]DC(lat, lon)float32...[864000 values with dtype=float32]ISI(lat, lon)float32...[864000 values with dtype=float32]BUI(lat, lon)float32...[864000 values with dtype=float32]FWI(lat, lon)float32...[864000 values with dtype=float32]FWI_N15(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N30(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N45(lat, lon)uint16...[864000 values with dtype=uint16]FWI_P25(lat, lon)float32...[864000 values with dtype=float32]FWI_P50(lat, lon)float32...[864000 values with dtype=float32]FWI_P75(lat, lon)float32...[864000 values with dtype=float32]FWI_P95(lat, lon)float32...[864000 values with dtype=float32]Indexes: (2)latPandasIndexPandasIndex(Float64Index([-59.875, -59.625, -59.375, -59.125, -58.875, -58.625, -58.375,\n              -58.125, -57.875, -57.625,\n              ...\n               87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,\n               89.375,  89.625,  89.875],\n             dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Float64Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,\n                1.875,   2.125,   2.375,\n              ...\n              357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125,\n              359.375, 359.625, 359.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\n\n\nStep 2 - Select data variable\nThe NetCDF contains several data variables. We pick the first one for demo.\n\nVARIABLE_NAME = \"FFMC\"\n\n\nda = ds[VARIABLE_NAME]\n\n\nda.plot();\n\n\n\n\n\n\n\n\n\nda.encoding\n\n{'chunksizes': None,\n 'fletcher32': False,\n 'shuffle': False,\n 'source': '&lt;File-like object S3FileSystem, cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc&gt;',\n 'original_shape': (600, 1440),\n 'dtype': dtype('&lt;f4'),\n '_FillValue': nan}\n\n\n\n\nStep 3. Conform to raster data conventions\nCommon practice in NetCDF lat/lon data the first grid cell is the south-west corner, i.e. latitude and longitude axes increase along the array dimensions.\nCommon practice in raster formats like GeoTIFF is that the y-axis (latitude in this case) decreases from origin, i.e. the first pixel is the north-west corner.\nWe can reverse the latitude dimension like this:\n\nda = da.isel(lat=slice(None, None, -1))\nda.lat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lat' (lat: 600)&gt;\narray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])\nCoordinates:\n  * lat      (lat) float64 89.88 89.62 89.38 89.12 ... -59.38 -59.62 -59.88\nAttributes:\n    units:          degrees_north\n    standard_name:  latitude\n    long_name:      latitude\n    axis:           Yxarray.DataArray'lat'lat: 60089.88 89.62 89.38 89.12 88.88 ... -58.88 -59.12 -59.38 -59.62 -59.88array([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Coordinates: (1)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Indexes: (1)latPandasIndexPandasIndex(Float64Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,\n               88.125,  87.875,  87.625,\n              ...\n              -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125,\n              -59.375, -59.625, -59.875],\n             dtype='float64', name='lat', length=600))Attributes: (4)units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Y\n\n\nWe would also like the longitude axis to range from -180 to 180 degrees east, instead of 0 to 360 degrees east (matter of taste, not convention).\n\nda = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\nda.lon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lon' (lon: 1440)&gt;\narray([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])\nCoordinates:\n  * lon      (lon) float64 -179.9 -179.6 -179.4 -179.1 ... 179.4 179.6 179.9xarray.DataArray'lon'lon: 1440-179.9 -179.6 -179.4 -179.1 -178.9 ... 178.9 179.1 179.4 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Coordinates: (1)lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Indexes: (1)lonPandasIndexPandasIndex(Float64Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625,\n              -178.375, -178.125, -177.875, -177.625,\n              ...\n               177.625,  177.875,  178.125,  178.375,  178.625,  178.875,\n               179.125,  179.375,  179.625,  179.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck that the data still looks right, just rotated along x-axis:\n\nda.plot();\n\n\n\n\n\n\n\n\nNow we need to set raster data attributes which are missing from the NetCDF, to help rio-xarray infer the raster information\n\nda.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\nda.rio.write_crs(\"epsg:4326\", inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'FFMC' (lat: 600, lon: 1440)&gt;\n[864000 values with dtype=float32]\nCoordinates:\n  * lat          (lat) float64 89.88 89.62 89.38 89.12 ... -59.38 -59.62 -59.88\n  * lon          (lon) float64 -179.9 -179.6 -179.4 -179.1 ... 179.4 179.6 179.9\n    spatial_ref  int64 0xarray.DataArray'FFMC'lat: 600lon: 1440...[864000 values with dtype=float32]Coordinates: (3)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Indexes: (2)latPandasIndexPandasIndex(Float64Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,\n               88.125,  87.875,  87.625,\n              ...\n              -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125,\n              -59.375, -59.625, -59.875],\n             dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Float64Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625,\n              -178.375, -178.125, -177.875, -177.625,\n              ...\n               177.625,  177.875,  178.125,  178.375,  178.625,  178.875,\n               179.125,  179.375,  179.625,  179.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck CRS\n\nda.rio.crs\n\nCRS.from_epsg(4326)\n\n\nCheck affine image transform:\na = width of a pixel\nb = row rotation (typically zero)\nc = x-coordinate of the upper-left corner of the upper-left pixel\nd = column rotation (typically zero)\ne = height of a pixel (typically negative)\nf = y-coordinate of the of the upper-left corner of the upper-left pixel\n\nda.rio.transform()\n\nAffine(0.25, 0.0, -180.0,\n       0.0, -0.25, 90.0)\n\n\n\n\nStep 4 - Write to COG and validate in-memory\nFor the demonstration here, we do not write the file to disk but to a memory file which can be validated and uploaded in-memory\nGeoTIFFs / COGs can be tuned for performance. Here are some defaults we found to work well (check out this blog post for detail).\n\nCOG_PROFILE = {\n    \"driver\": \"COG\",\n    \"compress\": \"DEFLATE\",\n    \"predictor\": 2\n}\n\n\nwith MemoryFile() as memfile:\n    da.rio.to_raster(memfile.name, **COG_PROFILE)\n    \n    cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n\ncog_valid\n\nTrue",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Converting NetCDF to COG (CMIP6)"
    ]
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Combined workflow from conversion to upload",
    "text": "Combined workflow from conversion to upload\n\nSOURCE_URI = 'cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc'\n\n\nVARIABLE_NAME = \"FFMC\"\n\n\nCOG_PROFILE = {\n    \"driver\": \"COG\",\n    \"compress\": \"DEFLATE\",\n    \"predictor\": 2\n}\n\n\nDESTINATION_BUCKET = None\nDESTINATION_KEY = None\n\n\nfs = s3fs.S3FileSystem()\nwith fs.open(SOURCE_URI) as fileobj:\n    with xr.open_dataset(fileobj, engine=\"h5netcdf\") as ds:\n\n        # Read individual metric into data array (only one time in yearly NetCDFs)\n        da = ds[VARIABLE_NAME]\n\n        # Realign the x dimension to -180 origin for dataset\n        da = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\n\n        # Reverse the DataArray's y dimension to comply with raster common practice\n        if da.lat.values[-1] &gt; da.lat.values[0]:\n            da = da.isel(lat=slice(None, None, -1))\n\n        # Set raster attributes\n        da.rio.set_spatial_dims(\"lon\", \"lat\")\n        da.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        with MemoryFile() as memfile:\n            da.rio.to_raster(memfile.name, **COG_PROFILE)\n\n            # Validate COG in-memory\n            cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n            if not cog_valid:\n                raise RuntimeError(\"COG validation failed.\")\n            \n            # Upload to S3\n            if DESTINATION_BUCKET is not None:\n                client = boto3.client(\"s3\")\n                r = client.put_object(\n                    Body=memfile,\n                    Bucket=DESTINATION_BUCKET,\n                    Key=DESTINATION_KEY,\n                )\n                if r[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n                    raise RuntimeError(\"Upload failed.\")",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Converting NetCDF to COG (CMIP6)"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html",
    "href": "notebooks/quickstarts/visualize-zarr.html",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "href": "notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#approach",
    "href": "notebooks/quickstarts/visualize-zarr.html#approach",
    "title": "Visualize zarr",
    "section": "Approach",
    "text": "Approach\n\nUse intake to open a STAC collection using with xarray and dask\nPlot the data using hvplot",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "href": "notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "title": "Visualize zarr",
    "section": "About the data",
    "text": "About the data\nThis is the Gridded Daily OCO-2 Carbon Dioxide assimilated dataset. More information can be found at: OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2 V10r (OCO2_GEOS_L3CO2_DAY)\nThe data has been converted to zarr format and published to the development version of the VEDA STAC Catalog.\n\nimport intake\nimport hvplot.xarray  # noqa",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "title": "Visualize zarr",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection_id = \"oco2-geos-l3-daily\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "href": "notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "title": "Visualize zarr",
    "section": "Get STAC collection",
    "text": "Get STAC collection\nUse intake to get the entire STAC collection.\n\ncollection = intake.open_stac_collection(f\"{STAC_API_URL}/collections/{collection_id}\")\ncollection\n\noco2-geos-l3-daily:\n  args:\n    stac_obj: https://staging-stac.delta-backend.com//collections/oco2-geos-l3-daily\n  description: ''\n  driver: intake_stac.catalog.StacCollection\n  metadata:\n    assets:\n      zarr:\n        href: s3://veda-data-store-staging/EIS/zarr/OCO2_GEOS_L3CO2_day.zarr\n        roles:\n        - data\n        title: zarr\n        type: application/vnd+zarr\n    cube:dimensions:\n      lat:\n        axis: y\n        description: latitude\n        extent:\n        - -90.0\n        - 90.0\n        reference_system: 4326\n        type: spatial\n      lon:\n        axis: x\n        description: longitude\n        extent:\n        - -180.0\n        - 179.375\n        reference_system: 4326\n        type: spatial\n      time:\n        description: time\n        extent:\n        - '2015-01-01T12:00:00Z'\n        - '2021-11-04T12:00:00Z'\n        step: P1DT0H0M0S\n        type: temporal\n    cube:variables:\n      XCO2:\n        attrs:\n          long_name: Assimilated dry-air column average CO2 daily mean\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Assimilated dry-air column average CO2 daily mean\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n      XCO2PREC:\n        attrs:\n          long_name: Precision of dry-air column average CO2 daily mean from Desroziers\n            et al. (2005) diagnostic\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Precision of dry-air column average CO2 daily mean from Desroziers\n          et al. (2005) diagnostic\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n    dashboard:is_periodic: true\n    dashboard:time_density: day\n    description: \"The OCO-2 mission provides the highest quality space-based XCO2\\\n      \\ retrievals to date. However, the instrument data are characterized by large\\\n      \\ gaps in coverage due to OCO-2\\u2019s narrow 10-km ground track and an inability\\\n      \\ to see through clouds and thick aerosols. This global gridded dataset is produced\\\n      \\ using a data assimilation technique commonly referred to as state estimation\\\n      \\ within the geophysical literature. Data assimilation synthesizes simulations\\\n      \\ and observations, adjusting the state of atmospheric constituents like CO2\\\n      \\ to reflect observed values, thus gap-filling observations when and where they\\\n      \\ are unavailable based on previous observations and short transport simulations\\\n      \\ by GEOS. Compared to other methods, data assimilation has the advantage that\\\n      \\ it makes estimates based on our collective scientific understanding, notably\\\n      \\ of the Earth's carbon cycle and atmospheric transport. OCO-2 GEOS (Goddard\\\n      \\ Earth Observing System) Level 3 data are produced by ingesting OCO-2 L2 retrievals\\\n      \\ every 6 hours with GEOS CoDAS, a modeling and data assimilation system maintained\\\n      \\ by NASA's Global Modeling and Assimilation Office (GMAO). GEOS CoDAS uses\\\n      \\ a high-performance computing implementation of the Gridpoint Statistical Interpolation\\\n      \\ approach for solving the state estimation problem. GSI finds the analyzed\\\n      \\ state that minimizes the three-dimensional variational (3D-Var) cost function\\\n      \\ formulation of the state estimation problem.\"\n    extent:\n      spatial:\n        bbox:\n        - - -180.0\n          - -90.0\n          - 180.0\n          - 90.0\n      temporal:\n        interval:\n        - - null\n          - null\n    id: oco2-geos-l3-daily\n    item_assets: null\n    license: CC0-1.0\n    stac_extensions:\n    - https://stac-extensions.github.io/datacube/v2.2.0/schema.json\n    stac_version: 1.0.0\n    title: Gridded Daily OCO-2 Carbon Dioxide assimilated dataset\n    type: Collection",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "href": "notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "title": "Visualize zarr",
    "section": "Read from zarr to xarray",
    "text": "Read from zarr to xarray\nIntake lets you go straight from the asset to an xarray dataset backed by a dask array.\n\nsource = collection.get_asset(\"zarr\")\n\nds = source.to_dask()\nds\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_xarray/base.py:21: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  'dims': dict(self._ds.dims),\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:   (time: 2500, lat: 361, lon: 576)\nCoordinates:\n  * lat       (lat) float64 -90.0 -89.5 -89.0 -88.5 ... 88.5 89.0 89.5 90.0\n  * lon       (lon) float64 -180.0 -179.4 -178.8 -178.1 ... 178.1 178.8 179.4\n  * time      (time) datetime64[ns] 2015-01-01T12:00:00 ... 2021-11-04T12:00:00\nData variables:\n    XCO2      (time, lat, lon) float64 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    XCO2PREC  (time, lat, lon) float64 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\nAttributes: (12/25)\n    BuildId:                        B10.2.06\n    Contact:                        Brad Weir (brad.weir@nasa.gov)\n    Conventions:                    CF-1\n    DataResolution:                 0.5x0.625\n    EastBoundingCoordinate:         179.375\n    Format:                         NetCDF-4/HDF-5\n    ...                             ...\n    ShortName:                      OCO2_GEOS_L3CO2_DAY_10r\n    SouthBoundingCoordinate:        -90.0\n    SpatialCoverage:                global\n    Title:                          OCO-2 GEOS Level 3 daily, 0.5x0.625 assim...\n    VersionID:                      V10r\n    WestBoundingCoordinate:         -180.0xarray.DatasetDimensions:time: 2500lat: 361lon: 576Coordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2015-01-01T12:00:00 ... 2021-11-...begin_date :20170801begin_time :120000long_name :timearray(['2015-01-01T12:00:00.000000000', '2015-01-02T12:00:00.000000000',\n       '2015-01-03T12:00:00.000000000', ..., '2021-11-02T12:00:00.000000000',\n       '2021-11-03T12:00:00.000000000', '2021-11-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)XCO2(time, lat, lon)float64dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Assimilated dry-air column average CO2 daily meanunits :mol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nDask graph\n600 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                             576 361 2500\n\n\n\n\n\n\n\n\nXCO2PREC\n\n\n(time, lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nPrecision of dry-air column average CO2 daily mean from Desroziers et al. (2005) diagnostic\n\nunits :\n\nmol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nDask graph\n600 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                             576 361 2500\n\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([-90.0, -89.5, -89.0, -88.5, -88.0, -87.5, -87.0, -86.5, -86.0, -85.5,\n       ...\n        85.5,  86.0,  86.5,  87.0,  87.5,  88.0,  88.5,  89.0,  89.5,  90.0],\n      dtype='float64', name='lat', length=361))lonPandasIndexPandasIndex(Index([  -180.0, -179.375,  -178.75, -178.125,   -177.5, -176.875,  -176.25,\n       -175.625,   -175.0, -174.375,\n       ...\n         173.75,  174.375,    175.0,  175.625,   176.25,  176.875,    177.5,\n        178.125,   178.75,  179.375],\n      dtype='float64', name='lon', length=576))timePandasIndexPandasIndex(DatetimeIndex(['2015-01-01 12:00:00', '2015-01-02 12:00:00',\n               '2015-01-03 12:00:00', '2015-01-04 12:00:00',\n               '2015-01-05 12:00:00', '2015-01-06 12:00:00',\n               '2015-01-07 12:00:00', '2015-01-08 12:00:00',\n               '2015-01-09 12:00:00', '2015-01-10 12:00:00',\n               ...\n               '2021-10-26 12:00:00', '2021-10-27 12:00:00',\n               '2021-10-28 12:00:00', '2021-10-29 12:00:00',\n               '2021-10-30 12:00:00', '2021-10-31 12:00:00',\n               '2021-11-01 12:00:00', '2021-11-02 12:00:00',\n               '2021-11-03 12:00:00', '2021-11-04 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=2500, freq=None))Attributes: (25)BuildId :B10.2.06Contact :Brad Weir (brad.weir@nasa.gov)Conventions :CF-1DataResolution :0.5x0.625EastBoundingCoordinate :179.375Format :NetCDF-4/HDF-5History :Original file generated: Tue Mar 15 12:02:48 2022 GMTIdentifierProductDOI :10.5067/Y9M4NM9MPCGHIdentifierProductDOIAuthority :http://doi.org/Institution :NASA GSFC Global Modeling and Assimilation Office and OCO-2 Project, Jet Propulsion LaboratoryLatitudeResolution :0.5LongName :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2LongitudeResolution :0.625NorthBoundingCoordinate :90.0ProductionDateTime :2022-03-15T12:02:48ZRangeBeginningDate :2017-08-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2017-08-01RangeEndingTime :23:59:99.999999ShortName :OCO2_GEOS_L3CO2_DAY_10rSouthBoundingCoordinate :-90.0SpatialCoverage :globalTitle :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2VersionID :V10rWestBoundingCoordinate :-180.0",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#plot-data",
    "href": "notebooks/quickstarts/visualize-zarr.html#plot-data",
    "title": "Visualize zarr",
    "section": "Plot data",
    "text": "Plot data\nWe can plot the XCO2 variable as an interactive map (with date slider) using hvplot.\n\nds.XCO2.hvplot(\n    x=\"lon\",\n    y=\"lat\",\n    groupby=\"time\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Visualize zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html",
    "href": "notebooks/quickstarts/intake.html",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#run-this-notebook",
    "href": "notebooks/quickstarts/intake.html#run-this-notebook",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#approach",
    "href": "notebooks/quickstarts/intake.html#approach",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve items\nInspect available items using intake\nRead collection using intake\n\n\nimport intake\nfrom pystac_client import Client\n\nimport rasterio as rio\nfrom rasterio.plot import show\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#discover-items-for-time-of-interest",
    "href": "notebooks/quickstarts/intake.html#discover-items-for-time-of-interest",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Discover items for time of interest",
    "text": "Discover items for time of interest\nUse pystac_client to search the STAC for a particular time of interest.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(\n    max_items=100,  # this is max number of records that will be returned\n    limit=25,  # this is the page size of results per page (so based on `max_items` above we will have four pages of results)\n    datetime=\"2001-01-01/2003-01-01\",\n    sortby=\"datetime\",\n)\nitems = list(search.items())\nlen(items)\n\n100\n\n\nintake requires data be casted to pystac.ItemCollection(s) before we can inspect the catalog:\n\ncollection = intake.open_stac_item_collection(search.item_collection())",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#basic-inspection-of-catalogcollections-with-instake-stac",
    "href": "notebooks/quickstarts/intake.html#basic-inspection-of-catalogcollections-with-instake-stac",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Basic Inspection of Catalog/Collection(s) with instake-stac",
    "text": "Basic Inspection of Catalog/Collection(s) with instake-stac\n\n# list the `features` of a STAC `FeatureCollection`\nlist(collection)[:25]\n\n['SSH_2017_minus_1992.cog',\n 'NDVI_2000_2009_cog',\n 'LST_Night_1km_2000_2009_cog',\n 'LST_Day_1km_2000_2009_cog',\n 'houston-lst-diff_2000_2019',\n 'HOUS_AOD_difference_2000_2019',\n 'HOUS_AOD_2000_2009_cog',\n 'TREND_MEAN_TWS.cog',\n 'MODIS_LC_2001_BD_v2.cog',\n 'landcover_Z_f_2001_cog',\n 'HOUS_DFAL_urbanization_2001_2019',\n 'FLDAS_NOAH01_SoilMoi00_10cm_tavg_C_GL_MA_ANOM200101_20010101',\n 'LE07_L2SR_001113_20010104_20200917_02_T2_SR',\n 'LT05_L2SP_160029_20010106_02_T1',\n 'LT05_L2SP_161029_20010113_02_T2',\n 'LE07_L2SR_005113_20010116_20200917_02_T2_SR',\n 'LT05_L2SP_126051_20010124_02_T1',\n 'LT05_L2SP_161028_20010129_02_T1',\n 'LT05_L2SP_161029_20010129_02_T1',\n 'LT05_L2SP_127051_20010131_02_T1',\n 'FLDAS_NOAH01_SoilMoi00_10cm_tavg_C_GL_MA_ANOM200102_20010201',\n 'LT05_L2SP_161028_20010214_02_T1',\n 'LT05_L2SP_161029_20010214_02_T1',\n 'LT05_L2SP_127051_20010216_02_T1',\n 'FLDAS_NOAH01_SoilMoi00_10cm_tavg_C_GL_MA_ANOM200103_20010301']\n\n\n\n# list the `assets` of a particular `FeatureCollection.&lt;feature_id&gt;`\nlist(collection[\"TREND_MEAN_TWS.cog\"])\n\n['cog_default']\n\n\n\n# if the entries count is too high to `list` all of them in a notebook session you can \"search\" children of the `FeatureCollection`\nfor id, entry in collection.search(\"TREND_MEAN_TWS\").items():\n    print(id)\n\nTREND_MEAN_TWS.cog.cog_default\nTREND_MEAN_TWS.cog\n\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=https://landsatlook.usgs.gov/stac-browser/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_SR_B4.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_SR_B5.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_SR_B7.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_ANG.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_MTL.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_MTL.xml&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_MTL.json&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_QA_PIXEL.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_QA_RADSAT.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/001/113/LE07_L2SR_001113_20010104_20200917_02_T2/LE07_L2SR_001113_20010104_20200917_02_T2_thumb_large.jpeg&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=https://landsatlook.usgs.gov/stac-browser/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_SR_B4.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_SR_B5.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_SR_B7.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_ANG.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_MTL.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_MTL.xml&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_MTL.json&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_QA_PIXEL.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_QA_RADSAT.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20010116_20200917_02_T2/LE07_L2SR_005113_20010116_20200917_02_T2_thumb_large.jpeg&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=https://landsatlook.usgs.gov/stac-browser/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_SR_B4.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_SR_B5.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_SR_B7.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_ANG.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_MTL.txt&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_MTL.xml&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_MTL.json&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_QA_PIXEL.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_QA_RADSAT.TIF&gt;\n  warnings.warn(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_stac/catalog.py:512: UserWarning: STAC Asset \"type\" missing, assuming default type=application/rasterio:\n&lt;Asset href=s3://usgs-landsat/collection02/level-2/standard/etm/2001/005/113/LE07_L2SR_005113_20011202_20200917_02_T2/LE07_L2SR_005113_20011202_20200917_02_T2_thumb_large.jpeg&gt;\n  warnings.warn(\n\n\n\n# or inspect the metadata for a `Feature` outright as `yaml`\ncollection[\"TREND_MEAN_TWS.cog\"]\n\ncog_default:\n  args:\n    chunks: {}\n    urlpath: s3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif\n  description: Default COG Layer\n  direct_access: allow\n  driver: intake_xarray.raster.RasterIOSource\n  metadata:\n    description: Cloud optimized default layer to display on map\n    href: s3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif\n    plots:\n      geotiff:\n        cmap: viridis\n        data_aspect: 1\n        dynamic: true\n        frame_width: 500\n        kind: image\n        rasterize: true\n        x: x\n        y: y\n    raster:bands:\n    - data_type: float64\n      histogram:\n        buckets:\n        - 343348.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 93900.0\n        count: 11.0\n        max: 8.917984008789062\n        min: -9999.0\n      nodata: 0.0\n      offset: 0.0\n      sampling: area\n      scale: 1.0\n      statistics:\n        maximum: 8.917984008789062\n        mean: -7851.75869965132\n        minimum: -9999.0\n        stddev: 4105.965757426157\n        valid_percent: 100.0\n    roles:\n    - data\n    - layer\n    title: Default COG Layer\n    type: image/tiff; application=geotiff; profile=cloud-optimized\n\n\n\n# now walk from the `FeatureCollection` to a specific asset URL\ncollection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].metadata[\"href\"]\n\n's3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif'\n\n\n\n# view this single cloud optimized geotiff source\ncog = rio.open(collection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].metadata[\"href\"])\nshow(cog)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#loading-data-items-with-intake-stac",
    "href": "notebooks/quickstarts/intake.html#loading-data-items-with-intake-stac",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Loading data items with intake-stac",
    "text": "Loading data items with intake-stac\nAfter identifying an asset you want you can then load it into an xarray.DataArray using the .to_dask() function\n\nimport hvplot.xarray\n\nda = collection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].to_dask().squeeze()\nda = da.where(da != -9999)\nda.hvplot.image(x=\"x\", y=\"y\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_xarray/raster.py:107: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  'dims': dict(ds2.dims),",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#loading-many-data-items",
    "href": "notebooks/quickstarts/intake.html#loading-many-data-items",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Loading many data items",
    "text": "Loading many data items\nAlternatively we can get all the asset hrefs in one fell swoop and load all.\nWe’ll use a specific collection for this example. In this case the Leaf Area Index in Bangladesh for 2003 and 2020.\n\nsearch = catalog.search(collections=[\"modis-annual-lai-2003-2020\"])\n\n\nsources = [item.assets[\"cog_default\"].href for item in search.items()]\nda_sources = intake.open_rasterio(\n    sources, chunks={}, concat_dim=\"year\", path_as_pattern=\"_LAI_{year}_BD\"\n)\nda = da_sources.to_dask().sel(band=1).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_xarray/raster.py:107: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  'dims': dict(ds2.dims),\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (year: 2, y: 1312, x: 1037)&gt;\ndask.array&lt;getitem, shape=(2, 1312, 1037), dtype=float32, chunksize=(1, 1312, 1037), chunktype=numpy.ndarray&gt;\nCoordinates:\n    band         int64 1\n  * x            (x) float64 88.03 88.03 88.04 88.04 ... 92.67 92.67 92.68 92.68\n  * y            (y) float64 26.63 26.63 26.62 26.62 ... 20.76 20.75 20.75 20.74\n    spatial_ref  int64 0\n  * year         (year) &lt;U4 '2020' '2003'\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     -3.4028235e+38\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayyear: 2y: 1312x: 1037dask.array&lt;chunksize=(1, 1312, 1037), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.38 MiB\n5.19 MiB\n\n\nShape\n(2, 1312, 1037)\n(1, 1312, 1037)\n\n\nDask graph\n2 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           1037 1312 2\n\n\n\n\nCoordinates: (5)band()int641array(1)x(x)float6488.03 88.03 88.04 ... 92.68 92.68array([88.02816 , 88.032652, 88.037144, ..., 92.67245 , 92.676942, 92.681434])y(y)float6426.63 26.63 26.62 ... 20.75 20.74array([26.632802, 26.628311, 26.623819, ..., 20.753329, 20.748837, 20.744346])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :88.02591469087191 0.004491576420597609 0.0 26.63504817414382 0.0 -0.004491576420597609array(0)year(year)&lt;U4'2020' '2003'array(['2020', '2003'], dtype='&lt;U4')Indexes: (3)xPandasIndexPandasIndex(Index([88.02816047908222, 88.03265205550281, 88.03714363192341,\n         88.041635208344, 88.04612678476461,  88.0506183611852,\n        88.0551099376058,  88.0596015140264,   88.064093090447,\n        88.0685846668676,\n       ...\n       92.64100946303596, 92.64550103945656, 92.64999261587715,\n       92.65448419229776, 92.65897576871835, 92.66346734513895,\n       92.66795892155955, 92.67245049798015, 92.67694207440074,\n       92.68143365082133],\n      dtype='float64', name='x', length=1037))yPandasIndexPandasIndex(Index([ 26.63280238593352,  26.62831080951292, 26.623819233092323,\n       26.619327656671725,  26.61483608025113, 26.610344503830532,\n       26.605852927409934, 26.601361350989336, 26.596869774568738,\n        26.59237819814814,\n       ...\n        20.78476988631543, 20.780278309894832, 20.775786733474238,\n        20.77129515705364,  20.76680358063304, 20.762312004212443,\n        20.75782042779185,  20.75332885137125, 20.748837274950652,\n       20.744345698530054],\n      dtype='float64', name='y', length=1312))yearPandasIndexPandasIndex(Index(['2020', '2003'], dtype='object', name='year'))Attributes: (4)AREA_OR_POINT :Area_FillValue :-3.4028235e+38scale_factor :1.0add_offset :0.0\n\n\nTake the difference between the LAI for the two years and plot that.\n\ndiff = (da.sel(year=\"2020\") - da.sel(year=\"2003\")).compute()\n\n# get the 2% and 98% percentiles for color limits in the plot\nvmin, vmax = diff.quantile([0.02, 0.98]).values\n\n# get the full collection name for the title\ncollection_name = catalog.get_collection(\"modis-annual-lai-2003-2020\").title\n\ndiff.hvplot.image(\n    x=\"x\",\n    y=\"y\",\n    tiles=True,\n    alpha=0.8,\n    cmap=\"turbo_r\",\n    clim=(vmin, vmax),\n    frame_height=600,\n    title=f\"Difference between {collection_name}\",\n)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Searching VEDA STAC API and inspecting catalogs with intake"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "title": "Calculate timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items cropped to AOI\nCalculate the mean for each timestep over the AOI\n\n\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport rioxarray  # noqa\nimport hvplot.xarray  # noqa",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection = \"no2-monthly\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nchina_bbox = [\n    73.675,\n    18.198,\n    135.026,\n    53.459,\n]\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(\n    bbox=china_bbox, datetime=datetime, collections=[collection], limit=1000\n)\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 73 items",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "title": "Calculate timeseries from COGs",
    "section": "Read data",
    "text": "Read data\nRead in data using xarray using a combination of xpystac, stackstac, and rasterio.\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)}).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-501e9e9badc173d84c999a59abafa101' (time: 73,\n                                                                y: 1800, x: 3600)&gt;\ndask.array&lt;getitem, shape=(73, 1800, 3600), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/15)\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ... '...\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    proj:shape      object {1800.0, 3600.0}\n    start_datetime  (time) &lt;U19 '2022-01-01T00:00:00' ... '2016-01-01T00:00:00'\n    ...              ...\n    end_datetime    (time) &lt;U19 '2022-01-31T00:00:00' ... '2016-01-31T00:00:00'\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    epsg            int64 4326\n  * time            (time) datetime64[ns] 2022-01-01 2021-12-01 ... 2016-01-01\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    crs:         epsg:4326\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-501e9e9badc173d84c999a59abafa101'time: 73y: 1800x: 3600dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.52 GiB\n8.00 MiB\n\n\nShape\n(73, 1800, 3600)\n(1, 1024, 1024)\n\n\nDask graph\n584 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                     3600 1800 73\n\n\n\n\nCoordinates: (15)id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)start_datetime(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00',\n       '2019-05-01T00:00:00', '2019-04-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:epsg()float644.326e+03array(4326.)end_datetime(time)&lt;U19'2022-01-31T00:00:00' ... '2016-...array(['2022-01-31T00:00:00', '2021-12-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-08-31T00:00:00',\n       '2021-07-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-03-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-01-31T00:00:00', '2020-12-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-08-31T00:00:00',\n       '2020-07-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-03-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-01-31T00:00:00', '2019-12-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-09-30T00:00:00', '2019-08-31T00:00:00',\n       '2019-07-31T00:00:00', '2019-06-30T00:00:00',\n       '2019-05-31T00:00:00', '2019-04-30T00:00:00',\n       '2019-03-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-01-31T00:00:00', '2018-12-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-08-31T00:00:00',\n       '2018-07-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-03-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-01-31T00:00:00', '2017-12-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-08-31T00:00:00',\n       '2017-07-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-03-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-01-31T00:00:00', '2016-12-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-08-31T00:00:00',\n       '2016-07-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-03-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-01-31T00:00:00'], dtype='&lt;U19')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)time(time)datetime64[ns]2022-01-01 ... 2016-01-01array(['2022-01-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)xPandasIndexPandasIndex(Index([            -180.0,             -179.9,             -179.8,\n                   -179.7,             -179.6,             -179.5,\n                   -179.4,             -179.3,             -179.2,\n                   -179.1,\n       ...\n                    179.0, 179.10000000000002, 179.20000000000005,\n                    179.3, 179.40000000000003,              179.5,\n       179.60000000000002, 179.70000000000005,              179.8,\n       179.90000000000003],\n      dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Index([              90.0,               89.9,               89.8,\n                     89.7,               89.6,               89.5,\n                     89.4,               89.3,               89.2,\n                     89.1,\n       ...\n                    -89.0, -89.10000000000002, -89.20000000000002,\n       -89.30000000000001,              -89.4,              -89.5,\n       -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                    -89.9],\n      dtype='float64', name='y', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2022-01-01', '2021-12-01', '2021-11-01', '2021-10-01',\n               '2021-09-01', '2021-08-01', '2021-07-01', '2021-06-01',\n               '2021-05-01', '2021-04-01', '2021-03-01', '2021-02-01',\n               '2021-01-01', '2020-12-01', '2020-11-01', '2020-10-01',\n               '2020-09-01', '2020-08-01', '2020-07-01', '2020-06-01',\n               '2020-05-01', '2020-04-01', '2020-03-01', '2020-02-01',\n               '2020-01-01', '2019-12-01', '2019-11-01', '2019-10-01',\n               '2019-09-01', '2019-08-01', '2019-07-01', '2019-06-01',\n               '2019-05-01', '2019-04-01', '2019-03-01', '2019-02-01',\n               '2019-01-01', '2018-12-01', '2018-11-01', '2018-10-01',\n               '2018-09-01', '2018-08-01', '2018-07-01', '2018-06-01',\n               '2018-05-01', '2018-04-01', '2018-03-01', '2018-02-01',\n               '2018-01-01', '2017-12-01', '2017-11-01', '2017-10-01',\n               '2017-09-01', '2017-08-01', '2017-07-01', '2017-06-01',\n               '2017-05-01', '2017-04-01', '2017-03-01', '2017-02-01',\n               '2017-01-01', '2016-12-01', '2016-11-01', '2016-10-01',\n               '2016-09-01', '2016-08-01', '2016-07-01', '2016-06-01',\n               '2016-05-01', '2016-04-01', '2016-03-01', '2016-02-01',\n               '2016-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "title": "Calculate timeseries from COGs",
    "section": "Clip the data to the bounding box for China",
    "text": "Clip the data to the bounding box for China\n\n# Subset to Bounding Box for China\nsubset = da.rio.clip_box(*china_bbox)\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-501e9e9badc173d84c999a59abafa101' (time: 73,\n                                                                y: 354, x: 614)&gt;\ndask.array&lt;getitem, shape=(73, 354, 614), dtype=float64, chunksize=(1, 354, 535), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ... '...\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 73.7 73.8 73.9 74.0 ... 134.7 134.8 134.9 135.0\n  * y               (y) float64 53.5 53.4 53.3 53.2 53.1 ... 18.5 18.4 18.3 18.2\n    proj:shape      object {1800.0, 3600.0}\n    start_datetime  (time) &lt;U19 '2022-01-01T00:00:00' ... '2016-01-01T00:00:00'\n    ...              ...\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    epsg            int64 4326\n  * time            (time) datetime64[ns] 2022-01-01 2021-12-01 ... 2016-01-01\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    resolution:  0.1xarray.DataArray'stackstac-501e9e9badc173d84c999a59abafa101'time: 73y: 354x: 614dask.array&lt;chunksize=(1, 354, 535), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n121.06 MiB\n1.44 MiB\n\n\nShape\n(73, 354, 614)\n(1, 354, 535)\n\n\nDask graph\n146 chunks in 5 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                               614 354 73\n\n\n\n\nCoordinates: (16)id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float6473.7 73.8 73.9 ... 134.9 135.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 73.7,  73.8,  73.9, ..., 134.8, 134.9, 135. ])y(y)float6453.5 53.4 53.3 ... 18.4 18.3 18.2axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([53.5, 53.4, 53.3, ..., 18.4, 18.3, 18.2])proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)start_datetime(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00',\n       '2019-05-01T00:00:00', '2019-04-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:epsg()float644.326e+03array(4326.)end_datetime(time)&lt;U19'2022-01-31T00:00:00' ... '2016-...array(['2022-01-31T00:00:00', '2021-12-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-08-31T00:00:00',\n       '2021-07-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-03-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-01-31T00:00:00', '2020-12-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-08-31T00:00:00',\n       '2020-07-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-03-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-01-31T00:00:00', '2019-12-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-09-30T00:00:00', '2019-08-31T00:00:00',\n       '2019-07-31T00:00:00', '2019-06-30T00:00:00',\n       '2019-05-31T00:00:00', '2019-04-30T00:00:00',\n       '2019-03-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-01-31T00:00:00', '2018-12-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-08-31T00:00:00',\n       '2018-07-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-03-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-01-31T00:00:00', '2017-12-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-08-31T00:00:00',\n       '2017-07-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-03-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-01-31T00:00:00', '2016-12-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-08-31T00:00:00',\n       '2016-07-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-03-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-01-31T00:00:00'], dtype='&lt;U19')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)time(time)datetime64[ns]2022-01-01 ... 2016-01-01array(['2022-01-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :73.65000000000002 0.09999999999999998 0.0 53.55 0.0 -0.09999999999999999array(0)Indexes: (3)xPandasIndexPandasIndex(Index([ 73.70000000000002,  73.80000000000001,               73.9,\n                     74.0,  74.10000000000002,  74.20000000000002,\n        74.30000000000001,               74.4,               74.5,\n        74.60000000000002,\n       ...\n       134.10000000000002, 134.20000000000005,              134.3,\n       134.40000000000003,              134.5, 134.60000000000002,\n       134.70000000000005,              134.8, 134.90000000000003,\n                    135.0],\n      dtype='float64', name='x', length=614))yPandasIndexPandasIndex(Index([              53.5,               53.4,               53.3,\n       53.199999999999996,               53.1,               53.0,\n                     52.9,               52.8, 52.699999999999996,\n                     52.6,\n       ...\n       19.099999999999994,               19.0,  18.89999999999999,\n       18.799999999999997, 18.700000000000003, 18.599999999999994,\n                     18.5,  18.39999999999999, 18.299999999999997,\n       18.200000000000003],\n      dtype='float64', name='y', length=354))timePandasIndexPandasIndex(DatetimeIndex(['2022-01-01', '2021-12-01', '2021-11-01', '2021-10-01',\n               '2021-09-01', '2021-08-01', '2021-07-01', '2021-06-01',\n               '2021-05-01', '2021-04-01', '2021-03-01', '2021-02-01',\n               '2021-01-01', '2020-12-01', '2020-11-01', '2020-10-01',\n               '2020-09-01', '2020-08-01', '2020-07-01', '2020-06-01',\n               '2020-05-01', '2020-04-01', '2020-03-01', '2020-02-01',\n               '2020-01-01', '2019-12-01', '2019-11-01', '2019-10-01',\n               '2019-09-01', '2019-08-01', '2019-07-01', '2019-06-01',\n               '2019-05-01', '2019-04-01', '2019-03-01', '2019-02-01',\n               '2019-01-01', '2018-12-01', '2018-11-01', '2018-10-01',\n               '2018-09-01', '2018-08-01', '2018-07-01', '2018-06-01',\n               '2018-05-01', '2018-04-01', '2018-03-01', '2018-02-01',\n               '2018-01-01', '2017-12-01', '2017-11-01', '2017-10-01',\n               '2017-09-01', '2017-08-01', '2017-07-01', '2017-06-01',\n               '2017-05-01', '2017-04-01', '2017-03-01', '2017-02-01',\n               '2017-01-01', '2016-12-01', '2016-11-01', '2016-10-01',\n               '2016-09-01', '2016-08-01', '2016-07-01', '2016-06-01',\n               '2016-05-01', '2016-04-01', '2016-03-01', '2016-02-01',\n               '2016-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "title": "Calculate timeseries from COGs",
    "section": "Aggregate the data",
    "text": "Aggregate the data\nCalculate the mean at each time across regional data. Note this is the first time that the data is actually loaded.\n\nmeans = subset.mean(dim=(\"x\", \"y\")).compute()\n\nPlot the mean monthly NO2 using hvplot\n\nmeans.hvplot.line(x=\"time\", ylabel=\"NO2\", title=\"Mean Monthly NO2 in China\")",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Calculate timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html",
    "href": "notebooks/quickstarts/timeseries-stac-api.html",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#approach",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#approach",
    "title": "Get timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUsing a list of STAC items and a bouding box fetch stats from /cog/statistics endpoint\nGenerate a timeseries plot using statistics from each time step\nSpeed up workflow using Dask\n\n\nimport requests\nimport folium\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_name = \"no2-monthly\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2023-09-30 00:00:00+00']]}},\n 'license': 'MIT',\n 'keywords': None,\n 'providers': None,\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2023-09-30T00:00:00Z']},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\nDescribe the periodic nature of the data\nIn the collection above we will pay particular attention to the fields that define the periodicity of the data.\n\ncollection[\"dashboard:is_periodic\"]\n\nTrue\n\n\n\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\ncollection[\"summaries\"]\n\n{'datetime': ['2016-01-01T00:00:00Z', '2023-09-30T00:00:00Z']}",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC items",
    "text": "Fetch STAC items\nGet the list of all the STAC items within this collection.\n\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nlen(items)\n\n93\n\n\nWe can inspect one of these items to get a sense of what metadata is available.\n\nitems[0]\n\n{'id': 'OMI_trno2_0.10x0.10_202309_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202309_Col3_V4.nc'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly/OMI_trno2_0.10x0.10_202309_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 11639639471292416,\n      'min': -4024247454269440.0,\n      'count': 11.0,\n      'buckets': [24.0,\n       1128.0,\n       403168.0,\n       64788.0,\n       5583.0,\n       1096.0,\n       252.0,\n       64.0,\n       19.0,\n       4.0]},\n     'statistics': {'mean': 343251029873510.25,\n      'stddev': 563904505347325.8,\n      'maximum': 11639639471292416,\n      'minimum': -4024247454269440.0,\n      'valid_percent': 90.81382751464844}}]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1800.0, 3600.0],\n  'end_datetime': '2023-09-30T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'start_datetime': '2023-09-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Define an area of interest",
    "text": "Define an area of interest\nWe will be using a bounding box over metropolitan france. We’ll use that bounding box to subset the data when calculating the timeseries.\n\nfrance_bounding_box = {\n    \"type\": \"Feature\",\n    \"properties\": {\"ADMIN\": \"France\", \"ISO_A3\": \"FRA\"},\n    \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n            [\n                [-5.183429, 42.332925],\n                [8.233998, 42.332925],\n                [8.233998, 51.066135],\n                [-5.183429, 51.066135],\n                [-5.183429, 42.332925],\n            ]\n        ],\n    },\n}\n\nLet’s take a look at that box.\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=3,\n)\n\nfolium.GeoJson(france_bounding_box, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "title": "Get timeseries from COGs",
    "section": "Use /cog/statistics to get data for the AOI",
    "text": "Use /cog/statistics to get data for the AOI\nFirst, we create a generate_stats function and then we call it with the bounding box defined for France.\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][\"cog_default\"][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\nGenerate stats\nThis may take some time due to the complexity of the shape we’re requesting. See the end of this notebook for tips on how to speed this up.\n\n%%time\nstats = [generate_stats(item, france_bounding_box) for item in items]\n\nCPU times: user 1.21 s, sys: 50.7 ms, total: 1.26 s\nWall time: 52.7 s\n\n\n\n\nInspect one result\n\nstats[0]\n\n{'statistics': {'b1': {'min': -387779578036224.0,\n   'max': 5971174383157248.0,\n   'mean': 1633500418513795.8,\n   'count': 11658.0,\n   'sum': 1.904334787903383e+19,\n   'std': 807808763419533.5,\n   'median': 1504613211570176.0,\n   'majority': 1143241374171136.0,\n   'minority': -387779578036224.0,\n   'unique': 11653.0,\n   'histogram': [[118.0,\n     1594.0,\n     4202.0,\n     3374.0,\n     1350.0,\n     613.0,\n     233.0,\n     122.0,\n     40.0,\n     12.0],\n    [-387779578036224.0,\n     248115814727680.0,\n     884011241046016.0,\n     1519906650587136.0,\n     2155802060128256.0,\n     2791697335451648.0,\n     3427592744992768.0,\n     4063488154533888.0,\n     4699383564075008.0,\n     5335279242051584.0,\n     5971174383157248.0]],\n   'valid_percent': 100.0,\n   'masked_pixels': 0.0,\n   'valid_pixels': 11658.0,\n   'percentile_2': 395805562618511.4,\n   'percentile_98': 3845241758988372.5}},\n 'ADMIN': 'France',\n 'ISO_A3': 'FRA',\n 'start_datetime': '2023-09-01T00:00:00'}",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "title": "Get timeseries from COGs",
    "section": "Plot timeseries",
    "text": "Plot timeseries\nIt is easier to interact with these results as a pandas dataframe. The following function takes the json, passes it to pandas, cleans up the column names and parses the date column.\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\n\n\nConstruct the plot\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly NO2 values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NO2 values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NO2 values\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\n\n\n\n\nIn this graph we can see the yearly cycles in NO2 values due to seasonal variations, as well as a slight downward slope in maximum NO2 values",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "title": "Get timeseries from COGs",
    "section": "Complex AOI",
    "text": "Complex AOI\nThe values plotted above don’t correspond exactly to Fance, since the bounding box excludes Corsica and overseas territories such as Mayotte and French Polynesia, and covers parts of neighboring countries including Spain, Italy, Germany and the entirety of Luxembourg. We can fetch GeoJSON from an authoritative online source (https://gadm.org/download_country.html).\nWhile the NO2 values above correspond more or less to those of in France, we can be much more precise by using a complex geojson that represents the boundaries of France exactly, including overseas territories in the Carribean and Indian Oceans, and South America.\nNote: In this notebook we write out the whole perimeter as a MultiPolygon in geojson. In practice you will often be reading this kind of shape from a file (usually with the help of geopandas).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=3,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe can now request the NO2 values for this complex AOI the same way as for the bounding box.\nNotice, however, that due to the complexity of the shape, it takes much longer to gather the requested data about 4 times as long as for the bounding box example above.\n\n%%time\naoi_df = clean_stats([generate_stats(item, france_aoi) for item in items])\n\nCPU times: user 4.08 s, sys: 97.2 ms, total: 4.17 s\nWall time: 1min 36s\n\n\nWe can compare the mean monthly NO2 values calculated when using the bounding box and when using the country’s exact borders\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"],\n    df[\"mean\"],\n    color=\"blue\",\n    label=\"Mean monthly NO2 values using bounding box\",\n)\nplt.plot(\n    aoi_df[\"date\"],\n    aoi_df[\"mean\"],\n    color=\"red\",\n    label=\"Mean monthly NO2 values using complex AOI\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\n\n\n\n\nWhile the difference is small, it is very interesting to note that the NO2 values calculated using the exact borders are systematically less than when using the bounding box. This may be due to the fact that the bounding box includes parts of western Germany and northern Italy that have a lot industrial activity, whereas the areas included when using the exact borders that are not included in the bounding box case, are overseas territories much less industrial activity.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "title": "Get timeseries from COGs",
    "section": "Speed things up: parallelize computation with Dask",
    "text": "Speed things up: parallelize computation with Dask\nWe can drastically reduce the time it takes to generate the timeseries, even with the complex AOI above, by parallelizing our code. The cogs/statistics API is powered by AWS Lambda which executes each request in a separate instance. This means the requests are highly scalable. Since each statistics request is for a single timestamp, we can request statistics for multiple timesteps concurrently, and greatly reduce the amount of time needed. We will demonstrate this by using the Dask.\n\nCreate a Dask client\nFirst we will create a Dask client. In this case we will use the threads on the same server that is running this jupyter notebook.\n\nimport dask.distributed\n\nclient = dask.distributed.Client()\n\n\n\nSubmit work\nWe will submit the generate_stats function for each item in our list and collect a list of futures. This will immediately kick off work in dask. We can then gather all the results.\n\n%%time\nfutures = [client.submit(generate_stats, item, france_aoi) for item in items]\nstats = client.gather(futures)\n\nCPU times: user 19.4 s, sys: 338 ms, total: 19.7 s\nWall time: 34 s\n\n\n\n\nClose the Dask client\nIt is good practice to close the client when you are done.\nclient.shutdown()\n\n\nAlternate approach\nIf you are familiar with the concurrent.futures library you can use that instead of Dask.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get timeseries from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html",
    "href": "notebooks/quickstarts/hls-visualization.html",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "href": "notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#approach",
    "href": "notebooks/quickstarts/hls-visualization.html#approach",
    "title": "Get tiles from COGs",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates within a bounding box, which is also an area of interest (AOI) in this example, for a given collection\nRegister a dynamic tiler search for an AOI and specific date range for a given collection\nExplore different options for displaying multi-band Harmonized Landsat and Sentinel (HLS) assets with the Raster API.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#about-the-data",
    "href": "notebooks/quickstarts/hls-visualization.html#about-the-data",
    "title": "Get tiles from COGs",
    "section": "About the Data",
    "text": "About the Data\nA small subset of HLS data has been ingested to the VEDA datastore to visually explore data using the Raster API, which is a VEDA instance of (pgstac-titiler). This limited subset includes a two granules for dates before and after Hurricane Maria in 2017 and Hurricane Ida in 2021.\nNote about HLS datasets: The Sentinel and Landsat assets have been “harmonized” in the sense that these products have been generated to use the same spatial resolution and grid system. Thus these 2 HLS S30 and L30 productscan be used interchangeably in algorithms. However, the individual band assets are specific to each provider. This notebook focuses on displaying HLS data with a dynamic tiler so separate examples are provided for rendering the unique band assets of each collection.\nAdditional Resources\n\nHLSL30 Dataset Landing Page\nLandsat 8 Bands and Combinations Blog\nHLSS30 Dataset Landing Page\nSentinel 2 Bands and Combinations Blog\nCQL2 STAC-API Examples\n\n\nimport json\nimport requests\n\nfrom folium import Map, TileLayer",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "href": "notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "title": "Get tiles from COGs",
    "section": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations",
    "text": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations\nIn this notebook we will focus on HLS S30 data for Hurricane Ida, but Hurricane Maria and L30 parameters are provided below for further exploration.\n\n# Endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Harmonized Sentinel collection id and configuration info\ns30_collection_id = \"hls-s30-002-ej-reprocessed\"\ns30_swir_assets = [\"B12\", \"B8A\", \"B04\"]\ns30_vegetation_index_assets = [\"B08\", \"B04\"]\ns30_vegetation_index_expression = \"(B08_b1-B04_b1)/(B08_b1+B04_b1)\"\ns30_vegetation_index_rescaling = \"0,1\"\ns30_vegetation_index_colormap = \"rdylgn\"\n\n# Harmonized Landsat collection id and map configuration info\nl30_collection_id = \"hls-l30-002-ej-reprocessed\"\nl30_swir_assets = [\"B07\", \"B05\", \"B04\"]\nl30_ndwi_expression = \"(B03_b1-B05_b1)/(B03_b1+B05_b1)\"\nl30_ndwi_assets = [\"B03\", \"B05\"]\nl30_ndwi_rescaling = \"0,1\"\nl30_ndwi_colormap = \"spectral\"\n\n# Search criteria for events in both HLS Events collections\nmaria_bbox = [-66.167596, 17.961538, -65.110098, 18.96772]\nmaria_temporal_range = [\"2017-06-06T00:00:00Z\", \"2017-11-30T00:00:00Z\"]\n\nida_bbox = [-90.932637, 29.705366, -89.766437, 30.71627]\nida_temporal_range = [\"2021-07-01T00:00:00Z\", \"2021-10-28T00:00:00Z\"]\n\n\nFirst, search the STAC API to find the specific dates available within timeframe of interest (Hurricane Ida)\nTo focus on a specific point in time, we will restrict the temporal range when defining the item search in the example below.\n\ncollections_filter = {\n    \"op\": \"=\",\n    \"args\": [{\"property\": \"collection\"}, s30_collection_id],\n}\n\nspatial_filter = {\"op\": \"s_intersects\", \"args\": [{\"property\": \"bbox\"}, ida_bbox]}\n\ntemporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [{\"property\": \"datetime\"}, {\"interval\": ida_temporal_range}],\n}\n\n# Additional filters can be applied for other search criteria like &lt;= maximum eo:cloud_cover in item properties\ncloud_filter = {\"op\": \"&lt;=\", \"args\": [{\"property\": \"eo:cloud_cover\"}, 80]}\n\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"limit\": 100,\n    \"sortby\": [{\"direction\": \"asc\", \"field\": \"properties.datetime\"}],\n    \"context\": \"on\",  # add context for a summary of matched results\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, temporal_filter, cloud_filter],\n    },\n}\n\n# Note this search body can also be used for a stac item search\nstac_items_response = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json=search_body,\n).json()\n\n# Check how many items were matched in search\nprint(\"search context:\", stac_items_response[\"context\"])\n\n# Iterate over search results to get an array of item datetimes\n[item[\"properties\"][\"datetime\"] for item in stac_items_response[\"features\"]]\n\nsearch context: {'limit': 100, 'matched': 14, 'returned': 14}\n\n\n['2021-07-14T16:55:15.122720Z',\n '2021-07-24T16:55:15.112940Z',\n '2021-07-29T16:55:16.405890Z',\n '2021-08-08T16:55:15.798510Z',\n '2021-08-13T16:55:13.394950Z',\n '2021-08-23T16:55:11.785040Z',\n '2021-09-02T16:55:09.568600Z',\n '2021-09-07T16:55:13.430530Z',\n '2021-09-22T16:55:10.763010Z',\n '2021-09-27T16:55:17.027350Z',\n '2021-10-07T16:55:18.213640Z',\n '2021-10-12T16:55:14.209080Z',\n '2021-10-17T16:55:18.517600Z',\n '2021-10-22T16:55:14.670710Z']",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "href": "notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "title": "Get tiles from COGs",
    "section": "Visualizing the data on a map",
    "text": "Visualizing the data on a map\nThe VEDA backend is based on eoAPI, an application for searching and tiling earth observation STAC records. The application uses titiler-pgstac for dynamically mosaicing cloud optimized data from a registerd STAC API search.\nTo use the dynamic tiler, register a STAC item search and then use the registered search ID to dynamically mosaic the search results on the map.\n\nUpdate the temporal range in search body and register that search with the Raster API\nThe registered search id can be reused for alternate map layer visualizations.\n\n# Restricted date range\nrestricted_temporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        {\"property\": \"datetime\"},\n        {\"interval\": [\"2021-10-16T00:00:00Z\", \"2021-10-18T00:00:00Z\"]},\n    ],\n}\n\n# Specify cql2-json filter language in search body\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, restricted_temporal_filter],\n    },\n}\n\nmosaic_response = requests.post(\n    f\"{RASTER_API_URL}/mosaic/register\",\n    json=search_body,\n).json()\nprint(json.dumps(mosaic_response, indent=2))\n\n{\n  \"searchid\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"links\": [\n    {\n      \"rel\": \"metadata\",\n      \"title\": \"Mosaic metadata\",\n      \"type\": \"application/json\",\n      \"href\": \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/info\"\n    },\n    {\n      \"rel\": \"tilejson\",\n      \"title\": \"Link for TileJSON\",\n      \"type\": \"application/json\",\n      \"href\": \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/tilejson.json\"\n    },\n    {\n      \"rel\": \"wmts\",\n      \"title\": \"Link for WMTS\",\n      \"type\": \"application/json\",\n      \"href\": \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/WMTSCapabilities.xml\"\n    }\n  ]\n}\n\n\n\n# Get base url for tiler from the register mosaic request\ntiles_href = next(\n    link[\"href\"] for link in mosaic_response[\"links\"] if link[\"rel\"] == \"tilejson\"\n)\n\n\n\nConfigure map formatting parameters\nSee the raster-api/docs for more formatting options",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "href": "notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "title": "Get tiles from COGs",
    "section": "Use the built-in SWIR post processing algorithm",
    "text": "Use the built-in SWIR post processing algorithm\nNote in the example below the band assets for HLS S30 are selected. The equivalent SWIR band assets for L30 are provided at the top of this notebook.\n\n# Add additional map formatting parameters to tiles url\ntilejson_response = requests.get(\n    tiles_href,\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"algorith\": \"swir\",\n        \"assets\": s30_swir_assets,\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/tiles/WebMercatorQuad/{z}/{x}/{y}?algorith=swir&assets=B12&assets=B8A&assets=B04\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.00000000000009,\n    85.0511287798066\n  ],\n  \"center\": [\n    4.263256414560601e-14,\n    0.0,\n    6\n  ]\n}\n\n\n\nDisplay the data on a map\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFormat and render tiles using custom formatting\nThe titiler/raster-api supports user defined band combinations, band math expressions, rescaling, band index, resampling and more.\n\n# Add additional map formatting parameters to tiles url\ntilejson_response = requests.get(\n    tiles_href,\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"assets\": s30_vegetation_index_assets,\n        \"expression\": s30_vegetation_index_expression,\n        \"rescale\": s30_vegetation_index_rescaling,\n        \"colormap_name\": s30_vegetation_index_colormap,\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/tiles/WebMercatorQuad/{z}/{x}/{y}?assets=B08&assets=B04&expression=%28B08_b1-B04_b1%29%2F%28B08_b1%2BB04_b1%29&rescale=0%2C1&colormap_name=rdylgn\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.00000000000009,\n    85.0511287798066\n  ],\n  \"center\": [\n    4.263256414560601e-14,\n    0.0,\n    6\n  ]\n}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#l30-hurricane-maria-example",
    "href": "notebooks/quickstarts/hls-visualization.html#l30-hurricane-maria-example",
    "title": "Get tiles from COGs",
    "section": "L30 Hurricane Maria Example",
    "text": "L30 Hurricane Maria Example\n\ncollections_filter = {\n    \"op\": \"=\", \n    \"args\" : [{ \"property\": \"collection\" }, l30_collection_id]\n}\n\nspatial_filter = {\n    \"op\": \"s_intersects\",\n    \"args\": [\n        {\"property\": \"bbox\"}, maria_bbox\n    ]\n}\n\ntemporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        { \"property\": \"datetime\" },\n        { \"interval\" : maria_temporal_range }\n    ]\n}\n\n# Additional filters can be applied for other search criteria like &lt;= maximum eo:cloud_cover in item properties\ncloud_filter = {\n    \"op\": \"&lt;=\",\n    \"args\": [\n        {\"property\": \"eo:cloud_cover\"},\n        80\n    ]\n}\n\n# Specify cql2-json filter language in search body and add context for a summary of matched results\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"context\": \"on\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            collections_filter,\n            temporal_filter,\n            cloud_filter\n        ]\n    }\n}\n\n# Note this search body can also be used for a stac item search \nstac_items_response = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json=search_body,\n).json()\n\n# Check how many items were matched in searc\nprint(\"search context:\", stac_items_response[\"context\"])\n\n# Iterate over search results to get an array of unique item datetimes\ndatetimes = []\nfeatures = stac_items_response[\"features\"]\ndatetimes += [item[\"properties\"][\"datetime\"] for item in features]\nnext_link = next((link for link in stac_items_response[\"links\"] if link[\"rel\"] == \"next\"), None)\nwhile next_link:\n    stac_items_response = requests.post(\n        f\"{STAC_API_URL}/search\",\n        json=next_link[\"body\"],\n    ).json()\n    features = stac_items_response[\"features\"]\n    datetimes += [item[\"properties\"][\"datetime\"] for item in features]\n    next_link = next((link for link in stac_items_response[\"links\"] if link[\"rel\"] == \"next\"), False)\n\nsorted(datetimes)\n\nsearch context: {'limit': 10, 'matched': 9, 'returned': 9}\n\n\n['2017-06-06T14:43:41.335694Z',\n '2017-06-22T14:43:47.156698Z',\n '2017-07-24T14:43:56.898518Z',\n '2017-08-09T14:44:03.584741Z',\n '2017-08-25T14:44:07.854507Z',\n '2017-09-26T14:44:14.813967Z',\n '2017-10-12T14:44:19.576858Z',\n '2017-11-13T14:44:17.834919Z',\n '2017-11-29T14:44:11.126689Z']\n\n\n\n# Restricted date range \nrestricted_temporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        { \"property\": \"datetime\" },\n        { \"interval\" : [ \"2017-10-11T00:00:00Z\", \"2017-10-13T00:00:00Z\"] }\n    ]\n}\n\n# Specify cql2-json filter language in search body\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            collections_filter,\n            spatial_filter,\n            restricted_temporal_filter\n        ]\n    }\n}\n\nmosaic_response = requests.post(\n    f\"{RASTER_API_URL}/mosaic/register\",\n    json=search_body,\n).json()\n\n# Set up format for Map API url\n# Get base url for tiler from the register mosaic request\ntiles_href = next(link[\"href\"] for link in mosaic_response[\"links\"] if link[\"rel\"]==\"tilejson\")\n\n# Add additional map formatting parameters to tiles url\ntilejson_response = requests.get(\n    tiles_href,\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"assets\": l30_ndwi_assets,\n        \"expression\": l30_ndwi_expression,\n        \"rescale\": l30_ndwi_rescaling,\n        \"colormap_name\": \"viridis\"\n    }\n).json()\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((maria_bbox[1] + maria_bbox[3]) / 2,(maria_bbox[0] + maria_bbox[2]) / 2),\n    zoom_start=zoom_start\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",  \n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get tiles from COGs"
    ]
  },
  {
    "objectID": "contributing/docs-and-notebooks.html",
    "href": "contributing/docs-and-notebooks.html",
    "title": "Usage Example Notebook Submission",
    "section": "",
    "text": "Contribution to VEDA’s documentation is always welcome - just open a Pull Request on the veda-docs repository.\nYou can submit a PR by forking the repository and submitting a PR with your fork. However, PR previews will not work for PRs from forks. You can push directly to this repository by becoming a collaborator. If you are not already a collaborator of the veda-docs repository, please email your github handle and veda@uah.edu along with a message like “please add me as a collaborator to the veda-docs repository so I can push a branch”. If you are not someone already familiar with the VEDA team, please add some additional information about your interest in contributing to the documentation.\nOnce you are a collaborator, you will be able to submit a PR from a branch of this repository (that is, not a branch from a fork) and PR previews will help in the review process.\nPlease note that this documentation site is rendered using Quarto, which adds a small set of configuration options on top of vanilla Markdown and Jupyter Notebooks.",
    "crumbs": [
      "Contributing",
      "Usage Example Notebook Submission"
    ]
  },
  {
    "objectID": "contributing/docs-and-notebooks.html#notebook-author-guidelines",
    "href": "contributing/docs-and-notebooks.html#notebook-author-guidelines",
    "title": "Usage Example Notebook Submission",
    "section": "Notebook Author Guidelines",
    "text": "Notebook Author Guidelines\nThere are two template notebooks in this directory titled: template-using-the-raster-api.ipynb and template-accessing-the-data-directly.ipynb that you can use as a starting place. Alternatively you can pull specific cells from that notebook into your own.\n\nStyle\n\nEach code cell should come after a markdown cell with some explanatory text. This is preferred over comments in the code cells.\nThe max header should be ##.\nOnly include imports that are needed for the notebook to run.\nWe don’t enforce any formatting, but periodically run black on all the notebooks. If you would like to run black yourself do pip install black[jupyter] and then black.\n\n\n\nRendering information\nThe first cell in every notebook is a raw cell that contains the following metadata for rendering with our site builder Quarto.\n---\ntitle: Short title\ndescription: One sentence description\nauthor: Author Name\ndate: May 2, 2023\nexecute:\n  freeze: true\n---\n\n\nRunning notebooks\nWe store evaluated notebooks in this repository. So before you commit your notebook, you should restart your kernel and run all cells in order.\nNormally we run the notebooks on VEDA JupyterHub.\nTo run the notebooks with a new image, use the JupyterHub image selection interface and under “Custom Image” type in the address to the public ecr image with the full tag sha.\nSomething like: public.ecr.aws/nasa-veda/pangeo-notebook:60b023fba2ca5f9e19d285c245987e368e27c0ea626b65777b204cec14b697c7\n\n\nStandard sections\nTo give the notebooks a standard look and feel we typically include the following sections:\n\nRun this Notebook: The section explains how to run the notebook locally, on VEDA JupyterHub or on mybinder. There are several examples of what this section can look like in the template notebooks.\nApproach: List a few steps that outline the approach you be taking in this notebook.\nAbout the data: Optional description of the dataset\nDeclare your collection of interest: This section reiterates how you can discover which collections are available. You can copy the example of this section from one of the template notebooks.\n\nFrom then on the standard sections diverge depending on whether the notebook access the data directly or uses the raster API. Check the template notebooks for some ideas of common patterns.\n\n\nUsing complex geometries\nIf you are defining the AOI using a bounding box, you can include it in the text of the notebook, but for more complex geometries we prefer that the notebook access the geometry directly from a canonical source. You can check the template notebooks for examples of this. If the complex geometry is not available online the VEDA team can help get it up in a public s3 bucket.\n\n\nRecommended libraries\n\nMapping + Visualization\n\nfolium: folium adds Leaflet.js support to python projects for visualizing data in a map.\nholoviz: High-level tools that make it easier to apply Python plotting libraries to your data.\nipyleaflet: Interactive maps in the Jupyter notebook. ipyleaflet is built on ipywidgets allowing for bidirectional communication between front- and backends (learn more: Interactive GIS in Jupyter with ipyleaflet).\n\n\n\nUsing STAC for cataloging data\nTo present consistent best practices, we always access data via the STAC API.\n\npystac: PySTAC is a library for creating SpatioTemporal Asset Catalogs (STAC) in Python 3.\npystac-client: A Python client for working with STAC Catalogs and APIs.\n\n\n\nAnalyzing data\n\nrioxarray: rasterio xarray extension\nstackstac: stackstac.stack turns a STAC collection into a lazy xarray.DataArray, backed by dask.\n\n\n\n\nGenerate “Launch in VEDA JupyterHub” link\nWe use nbgitpuller links to open the VEDA JupyterHub with a particular notebook pulled in. These links have the form: https://hub.openveda.cloud/hub/user-redirect/git-pull?repo=https://github.com/NASA-IMPACT/veda-docs&urlpath=lab/tree/veda-docs/notebooks/quickstarts/open-and-plot.ipynb&branch=main\nIf you are writing a notebook and want to share it with others you can generate your own nbgitpuller link using this link generator.",
    "crumbs": [
      "Contributing",
      "Usage Example Notebook Submission"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/dataset-configuration.html",
    "href": "contributing/dashboard-configuration/dataset-configuration.html",
    "title": "Dataset Configuration",
    "section": "",
    "text": "Once you have ingested a dataset into the VEDA backend (following the steps in the Dataset Ingestion docs), you will need to configure the Dashboard.\nPlease note that the VEDA Dashboard relies on its own set of metadata about datasets. No information from STAC is loaded initially, so all the Dashboard needs to list your datasets needs to be configured in the following steps, which may require copying some of the information from the catalog metadata records, such as title, description, and dataset providers.\nTo configure your dataset, you can use the experimental VEDA Configuration UI, more closely described in its documentation on Github.\nAlternatively, you can directly submit configuration files and open a pull request in the content repository for the VEDA Dashboard, veda-config.\nIf you have any questions along the way, we prefer that you open tickets in veda-config. Alternatively, you can reach the VEDA team at veda@uah.edu.",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration",
      "Dataset Configuration"
    ]
  },
  {
    "objectID": "contributing/index.html",
    "href": "contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Please see the sections below for documentation on\n\nDataset ingestion into the VEDA data store and STAC\nConfiguration of Dataset information pages and Discoveries on the VEDA Dashboard\nUsage example notebooks that illustrate the use of datasets or compute methods\n\nNote: If you move a page or notebook, make sure to add a redirect link according to the quarto docs",
    "crumbs": [
      "Contributing"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-collection-conventions.html",
    "href": "contributing/dataset-ingestion/stac-collection-conventions.html",
    "title": "STAC collection conventions",
    "section": "",
    "text": "Copied from veda-backend#29\nDashboard-specific notes that supplement the full stac-api collection specification. Note that there is no schema enforcement on the collection table content in pgstac—this provides flexibility but also requires caution when creating and modifying Collections.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC collection conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "href": "contributing/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "title": "STAC collection conventions",
    "section": "Collection field, extension, and naming recommendations",
    "text": "Collection field, extension, and naming recommendations\n\n\n\nField &/or Extension\nRecommendations\n\n\n\n\nid\nIf dataset exists in NASA’s Earthdata or presumably from some other data provider like ESA, use that ID. If appropriate, add a suffix for any additional processing that has been performed, e.g. “OMSO2PCA_cog”. If dataset is not from NASA’s Earthdata, we can use a human readable name with underscores like “facebook_population_density”.\n\n\ndashboard extension\nTo support the delta-ui we have added two new fields in a proposed dashboard extension. For now we are just adding the fields but after testing things out, we can formalize the extension with a hosted json schema. Dashboard extension properties are only required for collections that will be viewed in the delta-ui dashboard.\n\n\ndashboard:is_periodic\nTrue/False This boolean is used when summarizing the collection—if the collection is periodic, the temporal range of the items in the collection and the time density are all the front end needs to generate a time picker. If the items in the collection are not periodic, a complete list of the unique item datetimes is needed.\n\n\ndashboard:time_density\nyear, month, day, hour, minute, or null. These time steps should be treated as enum when the extension is formalized. For collections with a single time snapshot this value is null.\n\n\nitem_assets\nstac-extension/item_assets is used to explain the assets that are provided for each item in the collection. We’re not providing thumbnails yet, but this example below includes a thumbnail asset to illustrate how the extension will be used. The population of this property is not automated, the creator of the collection writes the item assets documentation. Item assets are only required for collections that will be viewed in the delta-ui dashboard.\n\n\nsummaries\nThe implementation of this core stac-spec field is use-case specific. Our implementation is intended to support the dashboard and will supply datetime and raster statistics for the default map layer asset across the entire collection. Currently summaries are manually updated with a delta-ui specific user defined function in pgstac.\n\n\ntitle and description\nUse these properties to provide specific information about the collection to API users and catalog browsers. These properties correspond to dataset name and info in the covid-api but the delta dashboard will use delta-config to set these values in the UI so the information in our stac collections will be for data curators and API users.\n\n\ncollection name style choices\nPrefer lower-case kebab-case collection names. Decision: Should names align with underlying data identifiers or should it be an interpreted name? omi-trno2-dhrm and omi-trno2-dhrm-difference vs no2-monthly and no2-monthly-diff; bmhd-30m-monthly vs nightlights-hd-monthly\n\n\nlicense\nSPDX license id, license is likely available in CMR but we may need to research other sources of data. Default open license: CC0-1.0\n\n\n\nitem_assets example\n\n\"item_assets\": {\n    \"thumbnail\": {\n      \"type\": \"image/jpeg\",\n      \"roles\": [\n        \"thumbnail\"\n      ],\n      \"title\": \"Thumbnail\",\n      \"description\": \"A medium sized thumbnail\"\n    },\n    \"cog_default\": {\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"roles\": [\n        \"data\",\n        \"layer\"\n      ],\n      \"title\": \"Default COG Layer\",\n      \"description\": \"Cloud optimized default layer to display on map\"\n    }\n  }\nsummaries example for periodic collection\n\"summaries\": {\n    \"datetime\": [\"2016-01-01T00:00:00Z\", \"2022-01-01T00:00:00Z\"],\n    \"cog_default\": {\n      \"max\": 50064805976866820,\n      \"min\": -6618294421291008\n    }\n  }\nsummaries example for non-periodic collection\n\"summaries\": {\n    \"datetime\": [\n      \"2020-01-01T00:00:00Z\",\n      \"2020-02-01T00:00:00Z\",\n      \"2020-03-01T00:00:00Z\",\n      \"2020-04-01T00:00:00Z\",\n      \"2020-05-01T00:00:00Z\",\n      \"2020-06-01T00:00:00Z\",\n      \"2020-07-01T00:00:00Z\",\n      \"2020-08-01T00:00:00Z\",\n      \"2020-09-01T00:00:00Z\",\n      \"2020-10-01T00:00:00Z\",\n      \"2020-11-01T00:00:00Z\",\n      \"2020-12-01T00:00:00Z\",\n      \"2021-01-01T00:00:00Z\",\n      \"2021-02-01T00:00:00Z\",\n      \"2021-03-01T00:00:00Z\",\n      \"2021-04-01T00:00:00Z\",\n      \"2021-05-01T00:00:00Z\",\n      \"2021-06-01T00:00:00Z\",\n      \"2021-07-01T00:00:00Z\",\n      \"2021-08-01T00:00:00Z\",\n      \"2021-09-01T00:00:00Z\"\n    ],\n    \"cog_default\": {\n      \"max\": 255,\n      \"min\": 0\n    }\n  }",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Collection Creation",
      "STAC collection conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/index.html",
    "href": "contributing/dataset-ingestion/index.html",
    "title": "Dataset Ingestion",
    "section": "",
    "text": "VEDA uses a centralized Spatio-Temporal Asset Catalog (STAC) for data dissemination and prefers to host datasets in cloud-object storage (AWS S3 in the region us-west-2) in the cloud-optimized file formats Cloud-Optimized GeoTIFF (COG) and Zarr, which enables viewing and efficient access in the cloud directly from the original datafiles without copies or multiple versions.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "href": "contributing/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "title": "Dataset Ingestion",
    "section": "Steps for ingesting a dataset",
    "text": "Steps for ingesting a dataset\nFor dataset ingestion, generally four steps are required. Depending on the capacity of the dataset provider, some of the steps can be completed by the VEDA team on request.\nThe data ingestion process requires Cognito credentials (username and password). In order to retrieve these credentials, you’ll need to contact a member of the VEDA Data Services Team at veda@uah.edu who can set up an account and credentials for you. The first time you log in using the Cognito Client, you will be prompted to set a new password.\nComplete as many steps of the process as you have capacity or authorization to. Please follow the steps and guides outlined below:\n\nOpen a dedicated pull request in the veda-data repository. Please read through these docs fully first as you they will help supply the information required to complete the PR. Use this “new dataset” template to open a new issue and get started.\nTransform datasets to conform with cloud-optimized file formats - see file preparation\nUpload files to storage (may be skipped, if data is cloud-optimized and in us-west-2)\nLoad those records into the VEDA STAC - see catalog ingestion\n\nFor a walk through of the full process outlined above, please refer to this example notebook. This notebook uses the GEOGLAM June 2023 dataset as an example, but please use this as a guide for the ingestion process (and required dataset defintions), replacing the GEOGLAM dataset with your own.\nStuck on how to develop compliant metadata records for your dataset?\nCheckout the following notebooks and resources to help provide you with the STAC metadata required to create the dataset definitions needed for catalog ingestion.\n\nHow to create STAC Collections: see this example notebook and related STAC conventions\nHow to create STAC Items: see this example notebook and conventions.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VEDA User Documentation",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is an innovative platform empowering researchers to explore and analyze Earth science data in the cloud. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.\nThese pages provide documentation for onboarding users to cloud-enabled open science with Earth data, from computing and API usage to publication on the VEDA Dashboard.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "VEDA User Documentation",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is an innovative platform empowering researchers to explore and analyze Earth science data in the cloud. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.\nThese pages provide documentation for onboarding users to cloud-enabled open science with Earth data, from computing and API usage to publication on the VEDA Dashboard.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#data-services",
    "href": "index.html#data-services",
    "title": "VEDA User Documentation",
    "section": "Data Services",
    "text": "Data Services\nIn VEDA’s open-source science environment, datasets can be discovered and accessed via open-standard data services such as a Spatio Temporal Asset Catalog (STAC) and WMTS map tiles APIs. Here is an overview of the API endpoints.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#resources-for-open-source-data-science",
    "href": "index.html#resources-for-open-source-data-science",
    "title": "VEDA User Documentation",
    "section": "Resources for Open-Source Data Science",
    "text": "Resources for Open-Source Data Science\nIf you are just getting started with geospatial data science (in Python) or want to learn more, you may find the Collection of External Resources useful.\nTo learn from examples, see the VEDA Example Notebooks - examples of open-source data science using VEDA services.\nSelected user groups can also get access to a VEDA-provided JupyterHub service.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "VEDA User Documentation",
    "section": "Contributing",
    "text": "Contributing\nContributions of data, stories, and example code are open to all users affiliated with the project. We strive to make the process as easy as possible. Please see the docs on Contributing.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "external-resources.html",
    "href": "external-resources.html",
    "title": "External resources",
    "section": "",
    "text": "This list is intended for scientists who want to get started with cloud-based, collaborative, open geospatial data analysis, or those looking to refresh their knowledge. It is by no means complete, but contains pointers to more elaborate resources. We anticipate this list to evolve as our platform and use cases evolve. Suggestions for additional resources or topics are highly welcome.",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "external-resources.html#master-the-basic-workflow-tools",
    "href": "external-resources.html#master-the-basic-workflow-tools",
    "title": "External resources",
    "section": "Master the basic workflow tools",
    "text": "Master the basic workflow tools\n\nGit - for managing code versions\nConda - for managing dependencies\nJupyter - for running code",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "external-resources.html#take-a-course-or-read-a-book-on-geospatial-data-analysis",
    "href": "external-resources.html#take-a-course-or-read-a-book-on-geospatial-data-analysis",
    "title": "External resources",
    "section": "Take a course or read a book on geospatial data analysis",
    "text": "Take a course or read a book on geospatial data analysis\n\nData Carpentry geospatial course\nUW course on Geospatial Data Analysis with Python\nGeographic data book - emphasis on vector data",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "external-resources.html#look-through-nicely-curated-lists-of-tools-and-resources",
    "href": "external-resources.html#look-through-nicely-curated-lists-of-tools-and-resources",
    "title": "External resources",
    "section": "Look through nicely curated lists of tools and resources",
    "text": "Look through nicely curated lists of tools and resources\n\nCloud-Optimized Geospatial Formats Guide\nNASA Openscapes Earthdata Cloud Cookbook\nCryoCloud JupyterBook\nProject Pythia - Pangeo’s education hub\nPlanetary Computer\nGeospatial Computing Platform library of training resources (by Python package)",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "external-resources.html#find-out-what-a-hack-week-is-and-see-how-they-teach-best-practice",
    "href": "external-resources.html#find-out-what-a-hack-week-is-and-see-how-they-teach-best-practice",
    "title": "External resources",
    "section": "Find out what a Hack Week is and see how they teach best practice",
    "text": "Find out what a Hack Week is and see how they teach best practice\n\nUW GeoHackWeek\nUW OceanHackWeek",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "external-resources.html#stay-up-to-date-through-blogs",
    "href": "external-resources.html#stay-up-to-date-through-blogs",
    "title": "External resources",
    "section": "Stay up-to-date through blogs",
    "text": "Stay up-to-date through blogs\n\nBlog series on latest trends and resources in geospatial - entry-level",
    "crumbs": [
      "External resources"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. For example, current VEDA datasets like no2-mean and no2-diff should be two different collections, because one measures the mean levels of nitrogen dioxide and the other the differences in observed levels. Likewise, datasets like no2-monthly and no2-yearly should be different because the periodicity is different.\nOnce you have logically grouped the datasets into collections, you will need to create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"cogify\": false,\n      \"upload\": false,\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: The steps after this are technical, so at this point open a PR on the veda-data GitHub repository and a member of the VEDA team will handle the publication process.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Catalog Ingestion"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. For example, current VEDA datasets like no2-mean and no2-diff should be two different collections, because one measures the mean levels of nitrogen dioxide and the other the differences in observed levels. Likewise, datasets like no2-monthly and no2-yearly should be different because the periodicity is different.\nOnce you have logically grouped the datasets into collections, you will need to create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"cogify\": false,\n      \"upload\": false,\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: The steps after this are technical, so at this point open a PR on the veda-data GitHub repository and a member of the VEDA team will handle the publication process.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Catalog Ingestion"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "title": "Catalog Ingestion",
    "section": "STEP IV: Publication",
    "text": "STEP IV: Publication\nThe publication process involves 3 steps:\n\n[VEDA] Publishing to the development STAC catalog https://dev.openveda.cloud/api/stac\n[EIS] Reviewing the collection/items published to the dev STAC catalog\n[VEDA] Publishing to the staging STAC catalog https://staging-stac.delta-backend.com\n\nTo use the VEDA Ingestion API to schedule ingestion/publication of the data follow these steps:\n\n1. Obtain credentials from a VEDA team member\nAsk a VEDA team member to create Cognito credentials (username and password) for VEDA authentication.\n\n\n2. Export username and password\nexport username=\"johndoe\"\nexport password=\"xxxx\"\n\n\n3. Get token\n# Required imports\nimport os\nimport requests\n\n# Pull username and password from environment variables\nusername = os.environ.get(\"username\")\npassword = os.environ.get(\"password\")\n\n# base url for the workflows api\n# experimental / subject to change in the future\n# DISCLAIMER: coming soon, not yet available\nbase_url = \"https://dev.openveda.cloud/api/workflows\"\n\n# endpoint to get the token from\ntoken_url = f\"{base_url}/token\"\n\n# authentication credentials to be passed to the token_url\nbody = {\n    \"username\": username,\n    \"password\": password,\n}\n\n# request token\nresponse = requests.post(token_url, data=body)\nif not response.ok:\n    raise Exception(\"Couldn't obtain the token. Make sure the username and password are correct.\")\nelse:\n    # get token from response\n    token = response.json().get(\"AccessToken\")\n    # prepare headers for requests\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n\n\n4. Ingest the dataset\nThen, use the code snippet below to publish the dataset.\n# url for dataset validation / publication\nvalidate_url = f\"{base_url}/dataset/validate\"\n\npublish_url = f\"{base_url}/dataset/publish\"\n\n# prepare the body of the request,\nbody = json.load(open(\"dataset-definition.json\"))\n\n# Validate the data definition using the /validate endpoint\nvalidation_response = requests.post(\n    validate_url,\n    headers=headers,\n    json=body\n)\n\n# look at the response\nvalidation_response.raise_for_status()\n\n# If the validation is successful, publish the dataset using /publish endpoint\npublish_response = requests.post(\n    publish_url,\n    headers=headers,\n    json=body\n)\n\nif publish_response.ok:\n    print(\"Success\")\n\n\nCheck the status of the execution\n# the id of the execution\n# should be available in the response of workflow execution request\nexecution_id = \"xxx\"\n# url for execution status\nexecution_status_url = f\"{workflow_execution_url}/{execution_id}\"\n# make the request\nresponse = requests.get(\n    execution_status_url,\n    headers=headers,\n)\nif response.ok:\n    print(response.json())",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Catalog Ingestion"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html",
    "href": "contributing/dataset-ingestion/file-preparation.html",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The rio-cogeo documentation is a great starting reference point and have a very helpful guide on preparing COGs, too.\n\nTo inspect and validate your raster dataset use the following\nrio cogeo info /path/to/file.tif\nThis will allow you to explore the size in rows and lines, understand the data type (e.g., byte, float, etc.), and verify how NoData is defined. Once you have explored the COG’s definitions you can validate it by using the following commands:\nrio cogeo validate /path/to/file.tif\nCOG validation can be used to identify if any errors are found. The following steps can be used to resolve any errors.\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the most appropriate resampling method for overviews. For example, use average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore).",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "File preparation"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "href": "contributing/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The rio-cogeo documentation is a great starting reference point and have a very helpful guide on preparing COGs, too.\n\nTo inspect and validate your raster dataset use the following\nrio cogeo info /path/to/file.tif\nThis will allow you to explore the size in rows and lines, understand the data type (e.g., byte, float, etc.), and verify how NoData is defined. Once you have explored the COG’s definitions you can validate it by using the following commands:\nrio cogeo validate /path/to/file.tif\nCOG validation can be used to identify if any errors are found. The following steps can be used to resolve any errors.\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the most appropriate resampling method for overviews. For example, use average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore).",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "File preparation"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "href": "contributing/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "title": "File preparation",
    "section": "STEP II: Upload to the VEDA data store",
    "text": "STEP II: Upload to the VEDA data store\nOnce you have the COGs, obtain permissions (Cognito credentials) from the VEDA team to upload them to the veda-data-store-staging bucket.\nUpload the data to a sensible location inside the bucket. Example: s3://veda-data-store-staging/&lt;collection-id&gt;/",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "File preparation"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html",
    "title": "STAC item conventions",
    "section": "",
    "text": "Copied from veda-backend#28\nThis document defines a set of conventions for generating STAC Items consistently for the VEDA Dashboard UI and future API users. Ultimately, these represent the minimum metadata API users can expect from the backend.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC item conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "title": "STAC item conventions",
    "section": "Rio-stac conventions for generating STAC Items",
    "text": "Rio-stac conventions for generating STAC Items\nAll of our current ingestion plans will use rio-stac to generate item metadata for COGs so the notes below are organized around the input parameters of the create_stac_item method.\nexample rio-stac python usage\nitem = rio_stac.stac.create_stac_item(\n  id = item_id,\n  source = f\"s3://{obj.bucket_name}/{obj.key}\", \n  collection = collection_id, \n  input_datetime = &lt;datetime.datetime&gt;,\n  with_proj = True,\n  with_raster = True,\n  asset_name = \"cog_default\",\n  asset_roles = [\"data\", \"layer\"],\n  asset_media_type = \"image/tiff; application=geotiff; profile=cloud-optimized\",\n)\nRio-stac create item parameter recommendations These recommendations are for generating STAC Item metadata for collections intended for the dasboard and may not be applicable to all ARCO collections.\n\n\n\nParameter\nRecommendations\n\n\n\n\nid\n(1) When STAC Item metadata is generated from a COG file, strip the full file extension from the filename for the item id. (2) When ids are not unique across collections, append the collection id to the item id. For example the no2-monthly and no2-monthly-diff COGs are stored with unique bucket prefixes but within the prefix all the filenames are the same, so the collection id is appended: OMI_trno2_0.10x0.10_201604_Col3_V4 → OMI_trno2_0.10x0.10_201604_Col3_V4-no2-monthly).\n\n\nwith_proj\nTrue. Generate projection extension metadata for the item for future ARCO datastore users.\n\n\nwith_raster\nTrue. This will generate gdal statistics for every band in the COG—we use these to get the range of values for the full collection.\n\n\nasset_name\nA meaningful asset name for the default cloud optimized asset to be displayed on a map. cog_default is a placeholder—we need to choose and commit to an asset name for all collections. If not set, will default to asset. * TODO Decision: For items with many assets we should ingest all with appropriate keys and duplicate one preferred display asset as the default cog. We should be considering metadata conventions in pgstac-titiler\n\n\nasset_roles\n[\"data\", \"layer\"] data is an appropriate role, we may also choose to add something like layer to indicate that the asset is optimized to be used as a map layer (stac specification for asset roles).\n\n\nasset_media_type\n\"image/tiff; application=geotiff; profile=cloud-optimized (stac best practices for asset media type).\n\n\nproperties\nCMIP6: TODO, CMR: TODO if we don’t store links to the original data, downstream users are not going to be able to pair STAC records with the versioned parent data in CMR",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC item conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "title": "STAC item conventions",
    "section": "Data provenance convention",
    "text": "Data provenance convention\nWhen adding STAC items that were derived from previously published data (such as CMR records), there are multiple ways to preserve the linkage between the item and the more complete source metadata. We should provide at a minimum metadata assets for any items derived from previously published data. Here are three examples from HLS:\nmetadata are assets The CMR properties question in the table above (how to refer the STAC Item to it’s CMR source metadata) could instead be solved by adding a metadata asset. This does not require creating a new extension for CMR, it just involves creating an asset from the CMR granule metadata which should be in the event context for CMR search driven ingests. The example below is from documentation for using HLS cloud optimized data.\n\"assets\": {\n  \"metadata\": {\n    \"href\": \"https://cmr.earthdata.nasa.gov/search/concepts/G2099379244-LPCLOUD.xml\",\n    \"type\": \"application/xml\"\n    },\n    \"thumbnail\": { ...}\n}\nstac-spec scieintific extension\n\"properties\": {\n   \"sci:doi\": \"10.5067/HLS/HLSS30.002\",\n   ...\n}\nItem links to metadata Use a cite-as Item link to the DOI for the source data.\n\"links\": [\n  {\n    \"rel\": \"cite-as\",\n    \"href\": \"https://doi.org/10.5067/HLS/HLSS30.002\"\n  },\n  ...\n]",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC item conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "title": "STAC item conventions",
    "section": "STAC Item validation convention",
    "text": "STAC Item validation convention\nWe are producing pystac.items with rio-stac’s create_stac_item method and we should validate them before publishing them to s3. Testing found that it is possible to produce structurally sound but invalid STAC Items with create_stac_item.\nThe built in pystac validator on the pystac.item returned by create_stac_item can be used to easily validate the metadata—item.validate() will raise an exception for invalid metadata. Pystac does need to be installed with the appropriate dependencies for validation.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC item conventions"
    ]
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "title": "STAC item conventions",
    "section": "Convention for default map layer assets for spectral data",
    "text": "Convention for default map layer assets for spectral data\nMany of the collections for the dashboard have a clear default map layer asset that we can name cog_default. This convention does not map as well to spectral data with many assets (B01, B02,…). A preferred band asset could be duplicated to define a default map layer asset to be consistent but this needs to be decided.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC item conventions"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/discovery-configuration.html",
    "href": "contributing/dashboard-configuration/discovery-configuration.html",
    "title": "Discovery Configuration",
    "section": "",
    "text": "By this point, you should have a few things:\n🧑‍🏫 We recommend you follow the video walkthrough on how to setup a virtual environment to facilitate discovery creation.",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration",
      "Discovery Configuration"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/discovery-configuration.html#sec-video-walkthrough",
    "href": "contributing/dashboard-configuration/discovery-configuration.html#sec-video-walkthrough",
    "title": "Discovery Configuration",
    "section": "Video Walkthrough",
    "text": "Video Walkthrough\n\nSetting up github codespaces\nCodespaces will allow you to have a development environment in the cloud without the need to setup anything on your local machine. VIDEO\n\n\nCreating a discovery\nWalkthrough of how to use github codespaces to create a discovery. From creating the needed files to the Pull Request that will eventually get the content published. VIDEO",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration",
      "Discovery Configuration"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html",
    "href": "contributing/dashboard-configuration/index.html",
    "title": "Dashboard Configuration",
    "section": "",
    "text": "This guide explains how to publish content in the VEDA Dashboard, the graphical user interface for exploring NASA Earth Data datasets and science stories VEDA UI.\nBy following this document, you should have a good understanding of how to start from having an idea for some content to show on the VEDA Dashboard all the way to having your data and content appear in the production version of the VEDA Dashboard. Detailed technical documentation for each of the steps is available on GitHub and other places, links provided in the Appendix below.\nflowchart LR\n    A(Data & Content Prep) --&gt; B{Is the data already in VEDA?}\n    B --&gt;|No| C[Go to Dataset Ingestion]\n    C --&gt; E\n    B --&gt;|Yes| E{Do you have a story?}\n    E --&gt;|Yes| D[Go to Discovery Configuration]\n    E --&gt;|No| F[Go to Dataset Configuration]\n    click C \"../dataset-ingestion/index.html\" \"Docs on Dataset Ingestion\" _blank\n    click D \"./discovery-configuration.html\" \"Docs on Discovery Configuration\" _blank\n    click F \"./dataset-configuration.html\" \"Docs on Dataset Configuration\" _blank",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html#data-content-preparation",
    "href": "contributing/dashboard-configuration/index.html#data-content-preparation",
    "title": "Dashboard Configuration",
    "section": "Data & Content Preparation",
    "text": "Data & Content Preparation\nThis is an important step before ingesting or configuring anything within VEDA. This will set you up for success in later steps.\n\nKey Steps\n🧑‍💻 Collaborate with partners familiar with the data context, to draft the necessary content.\nFor Discoveries, the required content is:\n\nText for the actual story itself\nAny visuals you would like to include, whether that be images, charts, maps, or other\n\nIf maps, identify which dataset and layer you would like to show and whether that is included in VEDA. (⚠️ If the dataset is not yet included in VEDA you’ll have to provide information about it and configure it as explained below).\nIf charts, gather the relevant data to build the chart. A csv file is the most common, but json is also supported\n\nA cover image for the dataset as it will appear in the Dashboard\nA title and short description/sub-title (5-10 words) for the Discovery\n\nNext step: Discovery Configuration.\nFor Datasets, the required content is:\n\nA descriptive overview of the dataset, how it came to exist, who maintains it, and how it should be used\nShort descriptions for each layer that you will want to reveal within VEDA (an example of this would be “CO2 mean vs CO2 difference”) for users to explore on a map\nA cover image for the dataset as it will appear in the Dashboard\nAny other relevant metadata you might want included https://nasa-impact.github.io/veda-docs/contributing/dashboard-content.html\nFor any datasets that need to be ingested, convert data to Cloud-Optimized GeoTIFFs (COGs) (⚠️ This is currently the only format supported in the VEDA Dashboard. More formats to come in the future)\n\nNext step: If your data is already in VEDA go to Dataset Configuration. Otherwise go to Dataset Ingestion.",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration"
    ]
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html#useful-links",
    "href": "contributing/dashboard-configuration/index.html#useful-links",
    "title": "Dashboard Configuration",
    "section": "Useful Links",
    "text": "Useful Links\n\nContent repository for the VEDA Dashboard - veda-config\nData processing from EIS\nAlexey’s notes on helpful tips",
    "crumbs": [
      "Contributing",
      "Dashboard Configuration"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html",
    "href": "notebooks/quickstarts/download-assets.html",
    "title": "Download STAC assets",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html#run-this-notebook",
    "href": "notebooks/quickstarts/download-assets.html#run-this-notebook",
    "title": "Download STAC assets",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html#approach",
    "href": "notebooks/quickstarts/download-assets.html#approach",
    "title": "Download STAC assets",
    "section": "Approach",
    "text": "Approach\nThis notebook shows how to download data for local use.\nThis is generally not the recommended approach. Whenever possible it is better to not transfer large volumes of data out of the original physical storage location. Instead users should practice data-proximate computing by processing in the same cloud and region. That is why the data for VEDA are hosted in the same region as this VEDA JupyterHub instance.\nHowever, sometimes you do need to download assets. This might be because the assets cannot be accessed directly from remote storage, or you don’t have access to an environment running in the same cloud/region.\nFor these special cases, this is how you go about downloading data:\n\nUse pystac_client to open and search the STAC catalog\nUse stac-asset to download the assets related to that search\nIf you need the file on your local machine, zip and download the output directory\n\nNote that the default examples environment is missing the stac-asset package. We can pip install that before trying to import.\n\n!pip install -q stac-asset\n\n\nimport stac_asset\nfrom pystac_client import Client",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/download-assets.html#declare-your-collection-of-interest",
    "title": "Download STAC assets",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection = \"caldor-fire-burn-severity\"\n\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection])\n\nprint(f\"Found {len(search.item_collection())} items\")\n\nFound 1 items",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html#download-the-assets",
    "href": "notebooks/quickstarts/download-assets.html#download-the-assets",
    "title": "Download STAC assets",
    "section": "Download the assets",
    "text": "Download the assets\nOnce you have identified the items that you are interested in, use stac_asset to download the related assets.\n\nawait stac_asset.download_item_collection(\n    search.item_collection(), \n    directory=\"data\", \n    config=stac_asset.Config(make_directory=True, s3_requester_pays=True)\n)\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"FeatureCollection\"\n        \n    \n                \n            \n                \n                    \n        features[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            type\n            \"Feature\"\n        \n    \n            \n        \n            \n                \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n            \n        \n            \n                \n        \n            id\n            \"bs_to_save\"\n        \n    \n            \n        \n            \n                \n        \n            properties\n            \n        \n            \n                \n        proj:bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            707933.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4271224.262587059\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            767213.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            4309054.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            proj:epsg\n            26910.0\n        \n    \n            \n        \n            \n                \n        proj:shape[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            1261.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            1976.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            end_datetime\n            \"2021-10-21T12:00:00+00:00\"\n        \n    \n            \n        \n            \n                \n        \n            proj:geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            707933.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4271224.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            767213.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4271224.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            767213.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4309054.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            707933.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4309054.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            707933.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            4271224.262587059\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        proj:transform[] 9 items\n        \n            \n        \n            \n                \n        \n            0\n            30.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            707933.4475195253\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            -30.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            4309054.262587059\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            8\n            1.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            start_datetime\n            \"2021-08-15T00:00:00+00:00\"\n        \n    \n            \n        \n            \n                \n        \n            datetime\n            None\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.56515924781838\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.93378864679858\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.549319926107025\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.91919400099995\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.88974438210656\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.60201903270269\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.56515924781838\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        links[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"collection\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://staging-stac.delta-backend.com/collections/caldor-fire-burn-severity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://staging-stac.delta-backend.com/collections/caldor-fire-burn-severity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://staging-stac.delta-backend.com/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"veda-stac\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            assets\n            \n        \n            \n                \n        \n            cog_default\n            \n        \n            \n                \n        \n            href\n            \"/home/jovyan/veda-docs/notebooks/quickstarts/data/bs_to_save/bs_to_save.tif\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Default COG Layer\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Cloud optimized default layer to display on map\"\n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            scale\n            1.0\n        \n    \n            \n        \n            \n                \n        \n            nodata\n            255.0\n        \n    \n            \n        \n            \n                \n        \n            offset\n            0.0\n        \n    \n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float64\"\n        \n    \n            \n        \n            \n                \n        \n            histogram\n            \n        \n            \n                \n        \n            max\n            4.209886074066162\n        \n    \n            \n        \n            \n                \n        \n            min\n            0.8347156643867493\n        \n    \n            \n        \n            \n                \n        \n            count\n            11.0\n        \n    \n            \n        \n            \n                \n        buckets[] 10 items\n        \n            \n        \n            \n                \n        \n            0\n            3869.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            5601.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            13118.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            64022.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            25216.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            23602.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            65527.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            15846.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            8\n            12513.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            9\n            19199.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            statistics\n            \n        \n            \n                \n        \n            mean\n            2.651540032771221\n        \n    \n            \n        \n            \n                \n        \n            stddev\n            0.7138142662108937\n        \n    \n            \n        \n            \n                \n        \n            maximum\n            4.209886074066162\n        \n    \n            \n        \n            \n                \n        \n            minimum\n            0.8347156643867493\n        \n    \n            \n        \n            \n                \n        \n            valid_percent\n            37.10832974961774\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"layer\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.549319926107025\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -119.91919400099995\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        stac_extensions[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/projection/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            collection\n            \"caldor-fire-burn-severity\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNote: For downloading just one item use stac_asset.download_item.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/download-assets.html#download-from-jupyterhub",
    "href": "notebooks/quickstarts/download-assets.html#download-from-jupyterhub",
    "title": "Download STAC assets",
    "section": "Download from JupyterHub",
    "text": "Download from JupyterHub\nIf you want to further download from this JupyterHub to your local machine you can zip the data directory:\n\n!zip -r data.zip data\n\nupdating: data/ (stored 0%)\nupdating: data/item-collection.json (deflated 74%)\nupdating: data/bs_to_save/ (stored 0%)\nupdating: data/bs_to_save/bs_to_save.tif (deflated 16%)\n\n\nThen right click on the the zipped file in the Jupyter file browser and select “Download”\n\n\n\nRight click on zip file to see options that include “Download”",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Download STAC assets"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html",
    "href": "notebooks/quickstarts/no2-map-plot.html",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "href": "notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#approach",
    "href": "notebooks/quickstarts/no2-map-plot.html#approach",
    "title": "Get map from COGs - NO2",
    "section": "Approach",
    "text": "Approach\n\nFetch STAC item for a particular date and collection - NO2\nPass STAC item in to the raster API /stac/tilejson.json endpoint\nVisualize tiles using folium\n\n\nimport requests\nimport folium",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "title": "Get map from COGs - NO2",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_name = \"no2-monthly\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "href": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'assets': None,\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2023-09-30 00:00:00+00']]}},\n 'license': 'MIT',\n 'keywords': None,\n 'providers': None,\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2023-09-30T00:00:00Z']},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': None,\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "href": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_name],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\nlen(items)\n\n1\n\n\nLet’s take a look at that one item.\n\nitem = items[0]\nitem\n\n{'id': 'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly/OMI_trno2_0.10x0.10_202101_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 35781585143857150,\n      'min': -4107596126486528.0,\n      'count': 11.0,\n      'buckets': [7437.0,\n       432387.0,\n       2866.0,\n       699.0,\n       356.0,\n       207.0,\n       76.0,\n       27.0,\n       7.0,\n       1.0]},\n     'statistics': {'mean': 367152773066762.6,\n      'stddev': 961254458662885.4,\n      'maximum': 35781585143857150,\n      'minimum': -4107596126486528.0,\n      'valid_percent': 84.69829559326172}}]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1800.0, 3600.0],\n  'end_datetime': '2021-01-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'start_datetime': '2021-01-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}\n\n\n\nitem_stats = item['assets']['cog_default']['raster:bands'][0]['statistics']\nrescale_values = item_stats['minimum'], item_stats['maximum']",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "href": "notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "title": "Get map from COGs - NO2",
    "section": "Use /stac/tilejson.json to get tiles",
    "text": "Use /stac/tilejson.json to get tiles\nWe pass the, item id, collection name, and the rescale_values in to the RASTER API endpoint and get back a tile.\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={item['collection']}&item={item['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values[0]},{rescale_values[1]}\",\n).json()\ntiles\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202101_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=rdbu_r&rescale=-4107596126486528.0%2C35781585143857150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    min_zoom=3,\n    attr=\"VEDA\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Using the Raster API",
      "Get map from COGs - NO2"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html",
    "href": "notebooks/quickstarts/downsample-zarr.html",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "href": "notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#approach",
    "href": "notebooks/quickstarts/downsample-zarr.html#approach",
    "title": "Downsample zarr",
    "section": "Approach",
    "text": "Approach\nThis notebook demonstrates 2 strategies for resampling data from a Zarr dataset in order to visualize within the memory limits of a notebook.\n\nDownsample the temporal resolution of the data using xarray.DataArray.resample\nCoarsening the spatial resolution of the data using xarray.DataArray.coarsen\n\nA strategy for visualizing any large amount of data is Datashader which bins data into a fixed 2-D array. Using the rasterize argument within hvplot calls ensures the use of the datashader library to bin the data. Optionally an external Dask cluster is used to parallelize and distribute these large downsampling operations across compute nodes.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "title": "Downsample zarr",
    "section": "About the data",
    "text": "About the data\nThe SMAP mission is an orbiting observatory that measures the amount of water in the surface soil everywhere on Earth.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "href": "notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "title": "Downsample zarr",
    "section": "Load libraries",
    "text": "Load libraries\n\nimport xarray as xr\nimport hvplot.xarray\nimport cartopy.crs as ccrs",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "href": "notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "title": "Downsample zarr",
    "section": "Optional: Create and Scale a Dask Cluster",
    "text": "Optional: Create and Scale a Dask Cluster\nWe create a separate Dask cluster to speed up reprojecting the data (and other potential computations which could be required and are parallelizable).\nNote if you skip this cell you will still be using Dask, you’ll just be using the machine where you are running this notebook.\n\nfrom dask_gateway import GatewayCluster, Gateway\n\ngateway = Gateway()\nclusters = gateway.list_clusters()\n\n# connect to an existing cluster - this is useful when the kernel shutdown in the middle of an interactive session\nif clusters:\n    cluster = gateway.connect(clusters[0].name)\nelse:\n    cluster = GatewayCluster(shutdown_on_close=True)\n\ncluster.scale(16)\nclient = cluster.get_client()\nclient\n\n\n     \n    \n        Client\n        Client-b660a68c-0258-11ef-84fd-a249f942cea6\n        \n\n\n\nConnection method: Cluster object\nCluster type: dask_gateway.GatewayCluster\n\n\nDashboard: /services/dask-gateway/clusters/prod.ba9729cf668747e5a3cc07867400551d/status\n\n\n\n\n\n\n        \n            \n                Launch dashboard in JupyterLab\n            \n        \n\n        \n            \n            Cluster Info\n            \n  GatewayCluster\n  \n    Name: prod.ba9729cf668747e5a3cc07867400551d\n    Dashboard: /services/dask-gateway/clusters/prod.ba9729cf668747e5a3cc07867400551d/status",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "href": "notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "title": "Downsample zarr",
    "section": "Open the dataset from S3",
    "text": "Open the dataset from S3\n\nds = xr.open_zarr(\"s3:///veda-data-store-staging/EIS/zarr/SPL3SMP.zarr\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 68GB\nDimensions:                        (northing_m: 406, easting_m: 964,\n                                    datetime: 1679)\nCoordinates:\n  * datetime                       (datetime) datetime64[ns] 13kB 2018-01-01 ...\n  * easting_m                      (easting_m) float64 8kB -1.735e+07 ... 1.7...\n  * northing_m                     (northing_m) float64 3kB 7.297e+06 ... -7....\nData variables: (12/26)\n    albedo                         (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    albedo_pm                      (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density                   (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density_pm                (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction                  (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction_pm               (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    ...                             ...\n    static_water_body_fraction     (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    static_water_body_fraction_pm  (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag                   (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag_pm                (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature            (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature_pm         (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;xarray.DatasetDimensions:northing_m: 406easting_m: 964datetime: 1679Coordinates: (3)datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')easting_m(easting_m)float64-1.735e+07 -1.731e+07 ... 1.735e+07array([-17349514.34, -17313482.12, -17277449.9 , ...,  17277449.08,\n        17313481.3 ,  17349513.52])northing_m(northing_m)float647.297e+06 7.26e+06 ... -7.297e+06array([ 7296524.72,  7260492.5 ,  7224460.28, ..., -7224459.94, -7260492.16,\n       -7296524.38])Data variables: (26)albedo(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Diffuse reflecting power of the Earth&apos;s surface used in DCA within the grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nalbedo_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nDiffuse reflecting power of the Earth&apos;s surface used in DCA retrievals within the grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nbulk_density\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of aggregated bulk_density within the 36 km grid cell.\n\nvalid_max :\n\n2.6500000953674316\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nbulk_density_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of aggregated bulk density within the 36 km grid cell.\n\nvalid_max :\n\n2.6500000953674316\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nclay_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nclay_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nfreeze_thaw_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nFraction of the 36 km grid cell that is denoted as frozen. Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nfreeze_thaw_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nFraction of the 36 km grid cell that is denoted as frozen. Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\ngrid_surface_status\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nIndicates if the grid point lies on land (0) or water (1).\n\nvalid_max :\n\n1\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\ngrid_surface_status_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nIndicates if the grid point lies on land (0) or water (1).\n\nvalid_max :\n\n1\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nradar_water_body_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nradar_water_body_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nretrieval_qual_flag\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflag_masks :\n\n1s, 2s, 4s, 8s\n\nflag_meanings :\n\nRetrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_success\n\nlong_name :\n\nBit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nretrieval_qual_flag_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflag_masks :\n\n1s, 2s, 4s, 8s\n\nflag_meanings :\n\nRetrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_success\n\nlong_name :\n\nBit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nroughness_coefficient\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of bare soil roughness used in DCA within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nroughness_coefficient_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nA unitless value that is indicative of bare soil roughness used in DCA retrievals within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsoil_moisture\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nRepresentative DCA soil moisture measurement for the Earth based grid cell.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.5\n\nvalid_min :\n\n0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsoil_moisture_error\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nNet uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.20000000298023224\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsoil_moisture_error_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nNet uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.20000000298023224\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsoil_moisture_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nRepresentative DCA soil moisture measurement for the Earth based grid cell.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.5\n\nvalid_min :\n\n0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nstatic_water_body_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nstatic_water_body_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsurface_flag\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflag_masks :\n\n1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048s\n\nflag_meanings :\n\n36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radiometer_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_region\n\nlong_name :\n\nBit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsurface_flag_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflag_masks :\n\n1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048s\n\nflag_meanings :\n\n36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radar_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_region\n\nlong_name :\n\nBit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsurface_temperature\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nTemperature at land surface based on GMAO GEOS-5 data.\n\nunits :\n\nKelvins\n\nvalid_max :\n\n350.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\n\n\n\nsurface_temperature_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nTemperature at land surface based on GMAO GEOS-5 data.\n\nunits :\n\nKelvins\n\nvalid_max :\n\n350.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\n\nIndexes: (3)datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n               '2018-01-09', '2018-01-10',\n               ...\n               '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03',\n               '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07',\n               '2022-09-08', '2022-09-09'],\n              dtype='datetime64[ns]', name='datetime', length=1679, freq=None))easting_mPandasIndexPandasIndex(Index([      -17349514.34,       -17313482.12,        -17277449.9,\n             -17241417.68,       -17205385.46,       -17169353.24,\n             -17133321.02,        -17097288.8,       -17061256.58,\n             -17025224.36,\n       ...\n       17025223.540000003,        17061255.76,        17097287.98,\n               17133320.2, 17169352.419999998, 17205384.640000004,\n       17241416.860000003, 17277449.080000002,         17313481.3,\n              17349513.52],\n      dtype='float64', name='easting_m', length=964))northing_mPandasIndexPandasIndex(Index([        7296524.72,          7260492.5,  7224460.279999999,\n               7188428.06,         7152395.84,         7116363.62,\n        7080331.399999999,         7044299.18,         7008266.96,\n               6972234.74,\n       ...\n       -6972234.400000001,        -7008266.62, -7044298.840000001,\n       -7080331.060000001,        -7116363.28, -7152395.500000001,\n       -7188427.720000002,        -7224459.94, -7260492.160000001,\n              -7296524.38],\n      dtype='float64', name='northing_m', length=406))Attributes: (0)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 1: Downsample the temporal resolution of the data",
    "text": "Strategy 1: Downsample the temporal resolution of the data\nTo plot one day from every month, resample the data to 1 observation a month.\n\nsomo_one_month = soil_moisture.resample(datetime=\"1ME\").nearest()\n\n\nQuick plot\nWe can generate a quick plot using hvplot and datashader.\n\n# workaround to avoid warnings that are triggered within Dask.\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"All-NaN slice encountered\", category=RuntimeWarning\n)\n\n\nsomo_one_month.hvplot(\n    x=\"easting_m\",\n    y=\"northing_m\",\n    groupby=\"datetime\",\n    crs=ccrs.epsg(6933),  # this is a workaround for https://github.com/holoviz/hvplot/issues/1329\n    rasterize=True,\n    coastline=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nReproject before plotting\nReproject the data for map visualization.\n\nsomo_one_month = somo_one_month.transpose(\"datetime\", \"northing_m\", \"easting_m\")\nsomo_one_month = somo_one_month.rio.set_spatial_dims(\n    x_dim=\"easting_m\", y_dim=\"northing_m\"\n)\nsomo_one_month = somo_one_month.rio.write_crs(\"EPSG:6933\")\nsomo_reprojected = somo_one_month.rio.reproject(\"EPSG:4326\")\nsomo_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 57, y: 1046, x: 2214)&gt; Size: 528MB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 18kB -179.9 -179.8 -179.6 ... 179.6 179.8 179.9\n  * y            (y) float64 8kB 84.96 84.8 84.64 84.48 ... -84.64 -84.8 -84.96\n  * datetime     (datetime) datetime64[ns] 456B 2018-01-31 ... 2022-09-30\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:  Representative DCA soil moisture measurement for the Earth ba...\n    units:      cm**3/cm**3\n    valid_max:  0.5\n    valid_min:  0.019999999552965164xarray.DataArray'soil_moisture'datetime: 57y: 1046x: 2214nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.9 -179.8 ... 179.8 179.9axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.918425, -179.755825, -179.593224, ...,  179.591393,  179.753993,\n        179.916594])y(y)float6484.96 84.8 84.64 ... -84.8 -84.96axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.963262,  84.800655,  84.638047, ..., -84.636773, -84.79938 ,\n       -84.961988])datetime(datetime)datetime64[ns]2018-01-31 ... 2022-09-30array(['2018-01-31T00:00:00.000000000', '2018-02-28T00:00:00.000000000',\n       '2018-03-31T00:00:00.000000000', '2018-04-30T00:00:00.000000000',\n       '2018-05-31T00:00:00.000000000', '2018-06-30T00:00:00.000000000',\n       '2018-07-31T00:00:00.000000000', '2018-08-31T00:00:00.000000000',\n       '2018-09-30T00:00:00.000000000', '2018-10-31T00:00:00.000000000',\n       '2018-11-30T00:00:00.000000000', '2018-12-31T00:00:00.000000000',\n       '2019-01-31T00:00:00.000000000', '2019-02-28T00:00:00.000000000',\n       '2019-03-31T00:00:00.000000000', '2019-04-30T00:00:00.000000000',\n       '2019-05-31T00:00:00.000000000', '2019-06-30T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-31T00:00:00.000000000',\n       '2019-09-30T00:00:00.000000000', '2019-10-31T00:00:00.000000000',\n       '2019-11-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000',\n       '2020-01-31T00:00:00.000000000', '2020-02-29T00:00:00.000000000',\n       '2020-03-31T00:00:00.000000000', '2020-04-30T00:00:00.000000000',\n       '2020-05-31T00:00:00.000000000', '2020-06-30T00:00:00.000000000',\n       '2020-07-31T00:00:00.000000000', '2020-08-31T00:00:00.000000000',\n       '2020-09-30T00:00:00.000000000', '2020-10-31T00:00:00.000000000',\n       '2020-11-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000',\n       '2021-01-31T00:00:00.000000000', '2021-02-28T00:00:00.000000000',\n       '2021-03-31T00:00:00.000000000', '2021-04-30T00:00:00.000000000',\n       '2021-05-31T00:00:00.000000000', '2021-06-30T00:00:00.000000000',\n       '2021-07-31T00:00:00.000000000', '2021-08-31T00:00:00.000000000',\n       '2021-09-30T00:00:00.000000000', '2021-10-31T00:00:00.000000000',\n       '2021-11-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000',\n       '2022-01-31T00:00:00.000000000', '2022-02-28T00:00:00.000000000',\n       '2022-03-31T00:00:00.000000000', '2022-04-30T00:00:00.000000000',\n       '2022-05-31T00:00:00.000000000', '2022-06-30T00:00:00.000000000',\n       '2022-07-31T00:00:00.000000000', '2022-08-31T00:00:00.000000000',\n       '2022-09-30T00:00:00.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.1626005508861423 0.0 85.04456635019852 0.0 -0.16260789541619725array(0)Indexes: (3)xPandasIndexPandasIndex(Index([-179.91842511650856, -179.75582456562242, -179.59322401473628,\n       -179.43062346385014,   -179.268022912964, -179.10542236207786,\n        -178.9428218111917, -178.78022126030555,  -178.6176207094194,\n       -178.45502015853327,\n       ...\n        178.45318903654908,  178.61578958743524,  178.77839013832136,\n        178.94099068920752,  179.10359124009364,   179.2661917909798,\n        179.42879234186597,  179.59139289275208,  179.75399344363825,\n        179.91659399452436],\n      dtype='float64', name='x', length=2214))yPandasIndexPandasIndex(Index([ 84.96326240249043,  84.80065450707423,  84.63804661165804,\n        84.47543871624184,  84.31283082082564,  84.15022292540944,\n        83.98761502999325,  83.82500713457705,  83.66239923916085,\n        83.49979134374465,\n       ...\n       -83.49851724868991, -83.66112514410612, -83.82373303952231,\n        -83.9863409349385, -84.14894883035471,  -84.3115567257709,\n       -84.47416462118711,  -84.6367725166033, -84.79938041201949,\n        -84.9619883074357],\n      dtype='float64', name='y', length=1046))datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n               '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31',\n               '2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30',\n               '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n               '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31',\n               '2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30',\n               '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n               '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31',\n               '2021-01-31', '2021-02-28', '2021-03-31', '2021-04-30',\n               '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31',\n               '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31',\n               '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30',\n               '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31',\n               '2022-09-30'],\n              dtype='datetime64[ns]', name='datetime', freq=None))Attributes: (4)long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164\n\n\n\n[!NOTE] This is now a fully materialized data array - when we reprojected we triggered an implicit compute.\n\n\nsomo_reprojected.hvplot(\n    x=\"x\",\n    y=\"y\",\n    groupby=\"datetime\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 2: Coarsen spatial resolution of the data",
    "text": "Strategy 2: Coarsen spatial resolution of the data\nBelow, we coarsen the spatial resolution of the data by a factor of 4 in the x and 2 in the y. These values were chosen because they can be used with the exact boundary argument as the dimensions size is a multiple of these values.\nYou can also coarsen by datetime, using the same strategy as below but replacing easting_m and northing_m with datetime. If {datetime: n} is the value given to the dim argument, this would create a mean of the soil moisture average for n days.\nOnce the data has been coarsened, again it is reprojected for map visualization and then visualized.\n\ncoarsened = soil_moisture.coarsen(dim={\"easting_m\": 4, \"northing_m\": 2}).mean()\n\ncoarsened = coarsened.transpose(\"datetime\", \"northing_m\", \"easting_m\")\ncoarsened = coarsened.rio.set_spatial_dims(x_dim=\"easting_m\", y_dim=\"northing_m\")\ncoarsened = coarsened.rio.write_crs(\"epsg:6933\")\ncoarsened_reprojected = coarsened.rio.reproject(\"EPSG:4326\")\ncoarsened_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 1679, y: 315, x: 667)&gt; Size: 1GB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 5kB -179.7 -179.2 -178.7 ... 178.6 179.2 179.7\n  * y            (y) float64 3kB 84.77 84.23 83.7 83.16 ... -83.64 -84.18 -84.72\n  * datetime     (datetime) datetime64[ns] 13kB 2018-01-01 ... 2022-09-09\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:   Representative DCA soil moisture measurement for the Earth b...\n    units:       cm**3/cm**3\n    valid_max:   0.5\n    valid_min:   0.019999999552965164\n    _FillValue:  3.402823466e+38xarray.DataArray'soil_moisture'datetime: 1679y: 315x: 667nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.7 -179.2 ... 179.2 179.7axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.729872, -179.190164, -178.650456, ...,  178.636044,  179.175751,\n        179.715459])y(y)float6484.77 84.23 83.7 ... -84.18 -84.72axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.774672,  84.234883,  83.695095, ..., -83.639381, -84.17917 ,\n       -84.718958])datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.5397077035841558 0.0 85.04456635019838 0.0 -0.5397886314149526array(0)Indexes: (3)xPandasIndexPandasIndex(Index([-179.72987154015956,  -179.1901638365754, -178.65045613299125,\n        -178.1107484294071, -177.57104072582294, -177.03133302223878,\n       -176.49162531865463, -175.95191761507047, -175.41220991148631,\n       -174.87250220790216,\n       ...\n        174.85808971463078,  175.39779741821494,   175.9375051217991,\n        176.47721282538325,   177.0169205289674,  177.55662823255156,\n        178.09633593613572,  178.63604363971987,  179.17575134330403,\n        179.71545904688818],\n      dtype='float64', name='x', length=667))yPandasIndexPandasIndex(Index([  84.7746720344909,  84.23488340307595,    83.695094771661,\n        83.15530614024604,  82.61551750883109,  82.07572887741614,\n        81.53594024600119,  80.99615161458624,  80.45636298317129,\n        79.91657435175634,\n       ...\n       -79.86086054706963, -80.40064917848458, -80.94043780989954,\n       -81.48022644131449, -82.02001507272944, -82.55980370414439,\n       -83.09959233555936, -83.63938096697431, -84.17916959838927,\n       -84.71895822980422],\n      dtype='float64', name='y', length=315))datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n               '2018-01-09', '2018-01-10',\n               ...\n               '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03',\n               '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07',\n               '2022-09-08', '2022-09-09'],\n              dtype='datetime64[ns]', name='datetime', length=1679, freq=None))Attributes: (5)long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164_FillValue :3.402823466e+38\n\n\n\ncoarsened_reprojected.hvplot(\n    x=\"x\",\n    y=\"y\",\n    groupby=\"datetime\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#cleanup",
    "href": "notebooks/quickstarts/downsample-zarr.html#cleanup",
    "title": "Downsample zarr",
    "section": "Cleanup",
    "text": "Cleanup\nWhen using a remote Dask cluster it is recommented to explicitly close the cluster.\nclient.shutdown()",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Downsample zarr"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html",
    "href": "notebooks/quickstarts/open-and-plot.html",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "href": "notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#approach",
    "href": "notebooks/quickstarts/open-and-plot.html#approach",
    "title": "Open and plot COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nOpen the collection with xarray and stackstac\nPlot the data using hvplot",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#about-the-data",
    "href": "notebooks/quickstarts/open-and-plot.html#about-the-data",
    "title": "Open and plot COGs",
    "section": "About the data",
    "text": "About the data\nCDC’s Social Vulnerability Index (SVI) uses 15 variables at the census tract level. The data comes from the U.S. decennial census for the years 2000 & 2010, and the American Community Survey (ACS) for the years 2014, 2016, and 2018. It is a hierarchical additive index (Tate, 2013), with the component elements of CDC’s SVI including the following for 4 themes: Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation.\nSVI indicates the relative vulnerability of every U.S. Census tract–subdivisions of counties for which the Census collects statistical data. SVI ranks the tracts on 15 social factors, including unemployment, minority status, and disability, and further groups them into four related themes. Thus, each tract receives a ranking for each Census variable and for each of the four themes, as well as an overall ranking.\n\nScientific research\nThe SVI Overall Score provides the overall, summed social vulnerability score for a given tract. The Overall Score SVI Grid is part of the U.S. Census Grids collection, and displays the Center for Disease Control & Prevention (CDC) SVI score. Funding for the final development, processing and dissemination of this data set by the Socioeconomic Data and Applications Center (SEDAC) was provided under the U.S. National Aeronautics and Space Administration (NASA)¹.\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nInterpreting the data\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nCredits\n\nCenter for International Earth Science Information Network, (CIESIN), Columbia University. 2021. Documentation for the U.S. Social Vulnerability Index Grids. Palisades, NY: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/fjr9-a973. Accessed 13 May 2022.\nCenters for Disease Control and Prevention/ Agency for Toxic Substances and Disease Registry/ Geospatial Research, Analysis, and Services Program. CDC/ATSDR Social Vulnerability Index Database. https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation_01192022_1.pdf\n\n\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport hvplot.xarray  # noqa",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "title": "Open and plot COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection = \"social-vulnerability-index-overall-nopop\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "href": "notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "title": "Open and plot COGs",
    "section": "Find items in collection",
    "text": "Find items in collection\nUse pystac_client to search the STAC collection.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection])\n\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 5 items",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#read-data",
    "href": "notebooks/quickstarts/open-and-plot.html#read-data",
    "title": "Open and plot COGs",
    "section": "Read data",
    "text": "Read data\nRead in data using xarray using a combination of xpystac, stackstac, and rasterio.\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)}).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-482085ade34713c184d81145b7646d22' (time: 5,\n                                                                y: 6298,\n                                                                x: 13354)&gt;\ndask.array&lt;getitem, shape=(5, 6298, 13354), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/15)\n    id              (time) &lt;U38 'svi_2018_tract_overall_wgs84_nopop_cog' ... ...\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -178.2 -178.2 -178.2 ... -66.98 -66.97 -66.97\n  * y               (y) float64 71.38 71.37 71.37 71.36 ... 18.92 18.92 18.91\n    start_datetime  (time) &lt;U19 '2018-01-01T00:00:00' ... '2000-01-01T00:00:00'\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-178.2333333...\n    ...              ...\n    proj:transform  object {0.00833333330000749, 0.0, 1.0, -0.008333333299984...\n    end_datetime    (time) &lt;U19 '2018-12-31T00:00:00' ... '2000-12-31T00:00:00'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    title           &lt;U17 'Default COG Layer'\n    epsg            int64 4326\n  * time            (time) datetime64[ns] 2018-01-01 2016-01-01 ... 2000-01-01\nAttributes:\n    spec:           RasterSpec(epsg=4326, bounds=(-178.24166595386018, 18.899...\n    crs:            epsg:4326\n    transform:      | 0.01, 0.00,-178.24|\\n| 0.00,-0.01, 71.38|\\n| 0.00, 0.00...\n    resolution_xy:  (0.00833333330000749, 0.00833333329998412)xarray.DataArray'stackstac-482085ade34713c184d81145b7646d22'time: 5y: 6298x: 13354dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.13 GiB\n8.00 MiB\n\n\nShape\n(5, 6298, 13354)\n(1, 1024, 1024)\n\n\nDask graph\n490 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                       13354 6298 5\n\n\n\n\nCoordinates: (15)id(time)&lt;U38'svi_2018_tract_overall_wgs84_no...array(['svi_2018_tract_overall_wgs84_nopop_cog',\n       'svi_2016_tract_overall_wgs84_nopop_cog',\n       'svi_2014_tract_overall_wgs84_nopop_cog',\n       'svi_2010_tract_overall_wgs84_nopop_cog',\n       'svi_2000_tract_overall_wgs84_nopop_cog'], dtype='&lt;U38')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-178.2 -178.2 ... -66.97 -66.97array([-178.241666, -178.233333, -178.224999, ...,  -66.983333,  -66.975   ,\n        -66.966666])y(y)float6471.38 71.37 71.37 ... 18.92 18.91array([71.383333, 71.375   , 71.366666, ..., 18.925   , 18.916667, 18.908333])start_datetime(time)&lt;U19'2018-01-01T00:00:00' ... '2000-...array(['2018-01-01T00:00:00', '2016-01-01T00:00:00',\n       '2014-01-01T00:00:00', '2010-01-01T00:00:00',\n       '2000-01-01T00:00:00'], dtype='&lt;U19')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-178.23333334, 18.908332897999998], [-66.958333785, 18.908332897999998], [-66.958333785, 71.383332688], [-178.23333334, 71.383332688], [-178.23333334, 18.908332897999998]]]},\n      dtype=object)proj:epsg()float644.326e+03array(4326.)proj:shape()object{6297.0, 13353.0}array({6297.0, 13353.0}, dtype=object)proj:bbox()object{-178.23333334, 18.9083328979999...array({-178.23333334, 18.908332897999998, -66.958333785, 71.383332688},\n      dtype=object)proj:transform()object{0.00833333330000749, 0.0, 1.0, ...array({0.00833333330000749, 0.0, 1.0, -0.00833333329998412, 71.383332688, -178.23333334},\n      dtype=object)end_datetime(time)&lt;U19'2018-12-31T00:00:00' ... '2000-...array(['2018-12-31T00:00:00', '2016-12-31T00:00:00',\n       '2014-12-31T00:00:00', '2010-12-31T00:00:00',\n       '2000-12-31T00:00:00'], dtype='&lt;U19')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')epsg()int644326array(4326)time(time)datetime64[ns]2018-01-01 ... 2000-01-01array(['2018-01-01T00:00:00.000000000', '2016-01-01T00:00:00.000000000',\n       '2014-01-01T00:00:00.000000000', '2010-01-01T00:00:00.000000000',\n       '2000-01-01T00:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)xPandasIndexPandasIndex(Index([-178.24166595386018, -178.23333262056016, -178.22499928726018,\n       -178.21666595396016, -178.20833262066014, -178.19999928736013,\n       -178.19166595406014, -178.18333262076013,  -178.1749992874601,\n       -178.16666595416012,\n       ...\n        -67.04166639856027,  -67.03333306526025,  -67.02499973196025,\n        -67.01666639866023,  -67.00833306536023,  -66.99999973206023,\n        -66.99166639876022,  -66.98333306546022,   -66.9749997321602,\n         -66.9666663988602],\n      dtype='float64', name='x', length=13354))yPandasIndexPandasIndex(Index([ 71.38333304766397,  71.37499971436398,    71.366666381064,\n        71.35833304776402,  71.34999971446403,  71.34166638116405,\n        71.33333304786406,  71.32499971456407,   71.3166663812641,\n        71.30833304796411,\n       ...\n       18.983333257363824, 18.974999924063845, 18.966666590763857,\n        18.95833325746387,  18.94999992416389, 18.941666590863903,\n       18.933333257563923, 18.924999924263936, 18.916666590963956,\n        18.90833325766397],\n      dtype='float64', name='y', length=6298))timePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2016-01-01', '2014-01-01', '2010-01-01',\n               '2000-01-01'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-178.24166595386018, 18.899999924363982, -66.95833306556018, 71.38333304766397), resolutions_xy=(0.00833333330000749, 0.00833333329998412))crs :epsg:4326transform :| 0.01, 0.00,-178.24|\n| 0.00,-0.01, 71.38|\n| 0.00, 0.00, 1.00|resolution_xy :(0.00833333330000749, 0.00833333329998412)\n\n\nThere are 5 items representing the 5 years of data in the collection (2000, 2010, 2014, 2016, and 2018).",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#plot-data",
    "href": "notebooks/quickstarts/open-and-plot.html#plot-data",
    "title": "Open and plot COGs",
    "section": "Plot data",
    "text": "Plot data\nPlot data using hvplot. By using rasterize=True we tell hvplot to use datashader behind the scenes to make the plot render more quickly and re-render on zoom.\n\n%%time\nda.hvplot(x=\"x\", y=\"y\", rasterize=True, clim=(0, 1), coastline=True, cmap=\"viridis\", widget_location=\"bottom\")\n\nCPU times: user 1.46 s, sys: 68 ms, total: 1.53 s\nWall time: 1.63 s",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and plot COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html",
    "href": "notebooks/quickstarts/visualize-multiple-times.html",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#approach",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#approach",
    "title": "Open and visualize COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items\nUse rioxarray to crop data to AOI\nUse hvplot to render the COG at every timestep\n\n\nimport requests\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport rioxarray  # noqa\nimport hvplot.xarray  # noqa",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "title": "Open and visualize COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection_id = \"no2-monthly\"",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Open and visualize COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection_id], sortby=\"start_datetime\")\n\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 93 items",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "title": "Open and visualize COGs",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "title": "Open and visualize COGs",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataArray using stackstac\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)}).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-a148846a6e7930da708c1b52af0a7d72' (time: 93,\n                                                                y: 1800, x: 3600)&gt;\ndask.array&lt;getitem, shape=(93, 1800, 3600), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/15)\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ... '...\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    proj:bbox       object {90.0, 180.0, -90.0, -180.0}\n    start_datetime  (time) &lt;U19 '2016-01-01T00:00:00' ... '2023-09-01T00:00:00'\n    ...              ...\n    proj:transform  object {0.1, 0.0, 1.0, -0.1, -180.0, 90.0}\n    proj:shape      object {1800.0, 3600.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    epsg            int64 4326\n  * time            (time) datetime64[ns] 2016-01-01 2016-02-01 ... 2023-09-01\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    crs:         epsg:4326\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-a148846a6e7930da708c1b52af0a7d72'time: 93y: 1800x: 3600dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.49 GiB\n8.00 MiB\n\n\nShape\n(93, 1800, 3600)\n(1, 1024, 1024)\n\n\nDask graph\n744 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                     3600 1800 93\n\n\n\n\nCoordinates: (15)id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202301_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202302_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202303_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202304_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202305_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202306_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202307_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202308_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202309_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)start_datetime(time)&lt;U19'2016-01-01T00:00:00' ... '2023-...array(['2016-01-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-12-01T00:00:00',\n       '2017-01-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-12-01T00:00:00',\n       '2018-01-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-12-01T00:00:00',\n       '2019-01-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-04-01T00:00:00',\n...\n       '2020-07-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-12-01T00:00:00',\n       '2021-01-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-12-01T00:00:00',\n       '2022-01-01T00:00:00', '2022-02-01T00:00:00',\n       '2022-03-01T00:00:00', '2022-04-01T00:00:00',\n       '2022-05-01T00:00:00', '2022-06-01T00:00:00',\n       '2022-07-01T00:00:00', '2022-08-01T00:00:00',\n       '2022-09-01T00:00:00', '2022-10-01T00:00:00',\n       '2022-11-01T00:00:00', '2022-12-01T00:00:00',\n       '2023-01-01T00:00:00', '2023-02-01T00:00:00',\n       '2023-03-01T00:00:00', '2023-04-01T00:00:00',\n       '2023-05-01T00:00:00', '2023-06-01T00:00:00',\n       '2023-07-01T00:00:00', '2023-08-01T00:00:00',\n       '2023-09-01T00:00:00'], dtype='&lt;U19')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)end_datetime(time)&lt;U19'2016-01-31T00:00:00' ... '2023-...array(['2016-01-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-03-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-07-31T00:00:00', '2016-08-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-12-31T00:00:00',\n       '2017-01-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-03-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-07-31T00:00:00', '2017-08-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-12-31T00:00:00',\n       '2018-01-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-03-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-07-31T00:00:00', '2018-08-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-12-31T00:00:00',\n       '2019-01-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-03-31T00:00:00', '2019-04-30T00:00:00',\n...\n       '2020-07-31T00:00:00', '2020-08-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-12-31T00:00:00',\n       '2021-01-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-03-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-07-31T00:00:00', '2021-08-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-12-31T00:00:00',\n       '2022-01-31T00:00:00', '2022-02-28T00:00:00',\n       '2022-03-31T00:00:00', '2022-04-30T00:00:00',\n       '2022-05-31T00:00:00', '2022-06-30T00:00:00',\n       '2022-07-31T00:00:00', '2022-08-31T00:00:00',\n       '2022-09-30T00:00:00', '2022-10-31T00:00:00',\n       '2022-11-30T00:00:00', '2022-12-31T00:00:00',\n       '2023-01-31T00:00:00', '2023-02-28T00:00:00',\n       '2023-03-31T00:00:00', '2023-04-30T00:00:00',\n       '2023-05-31T00:00:00', '2023-06-30T00:00:00',\n       '2023-07-31T00:00:00', '2023-08-31T00:00:00',\n       '2023-09-30T00:00:00'], dtype='&lt;U19')proj:epsg()float644.326e+03array(4326.)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)time(time)datetime64[ns]2016-01-01 ... 2023-09-01array(['2016-01-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2022-01-01T00:00:00.000000000', '2022-02-01T00:00:00.000000000',\n       '2022-03-01T00:00:00.000000000', '2022-04-01T00:00:00.000000000',\n       '2022-05-01T00:00:00.000000000', '2022-06-01T00:00:00.000000000',\n       '2022-07-01T00:00:00.000000000', '2022-08-01T00:00:00.000000000',\n       '2022-09-01T00:00:00.000000000', '2022-10-01T00:00:00.000000000',\n       '2022-11-01T00:00:00.000000000', '2022-12-01T00:00:00.000000000',\n       '2023-01-01T00:00:00.000000000', '2023-02-01T00:00:00.000000000',\n       '2023-03-01T00:00:00.000000000', '2023-04-01T00:00:00.000000000',\n       '2023-05-01T00:00:00.000000000', '2023-06-01T00:00:00.000000000',\n       '2023-07-01T00:00:00.000000000', '2023-08-01T00:00:00.000000000',\n       '2023-09-01T00:00:00.000000000'], dtype='datetime64[ns]')Indexes: (3)xPandasIndexPandasIndex(Index([            -180.0,             -179.9,             -179.8,\n                   -179.7,             -179.6,             -179.5,\n                   -179.4,             -179.3,             -179.2,\n                   -179.1,\n       ...\n                    179.0, 179.10000000000002, 179.20000000000005,\n                    179.3, 179.40000000000003,              179.5,\n       179.60000000000002, 179.70000000000005,              179.8,\n       179.90000000000003],\n      dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Index([              90.0,               89.9,               89.8,\n                     89.7,               89.6,               89.5,\n                     89.4,               89.3,               89.2,\n                     89.1,\n       ...\n                    -89.0, -89.10000000000002, -89.20000000000002,\n       -89.30000000000001,              -89.4,              -89.5,\n       -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                    -89.9],\n      dtype='float64', name='y', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01', '2016-04-01',\n               '2016-05-01', '2016-06-01', '2016-07-01', '2016-08-01',\n               '2016-09-01', '2016-10-01', '2016-11-01', '2016-12-01',\n               '2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01',\n               '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01',\n               '2017-09-01', '2017-10-01', '2017-11-01', '2017-12-01',\n               '2018-01-01', '2018-02-01', '2018-03-01', '2018-04-01',\n               '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n               '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n               '2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-06-01', '2019-07-01', '2019-08-01',\n               '2019-09-01', '2019-10-01', '2019-11-01', '2019-12-01',\n               '2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',\n               '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',\n               '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01',\n               '2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01',\n               '2021-05-01', '2021-06-01', '2021-07-01', '2021-08-01',\n               '2021-09-01', '2021-10-01', '2021-11-01', '2021-12-01',\n               '2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01',\n               '2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01',\n               '2022-09-01', '2022-10-01', '2022-11-01', '2022-12-01',\n               '2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "title": "Open and visualize COGs",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.rio.clip([france_aoi[\"geometry\"]])\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-a148846a6e7930da708c1b52af0a7d72' (time: 93,\n                                                                y: 97, x: 143)&gt;\ndask.array&lt;getitem, shape=(93, 97, 143), dtype=float64, chunksize=(1, 97, 143), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ... '...\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -4.7 -4.6 -4.5 -4.4 -4.3 ... 9.1 9.2 9.3 9.4 9.5\n  * y               (y) float64 51.0 50.9 50.8 50.7 50.6 ... 41.7 41.6 41.5 41.4\n    proj:bbox       object {90.0, 180.0, -90.0, -180.0}\n    start_datetime  (time) &lt;U19 '2016-01-01T00:00:00' ... '2023-09-01T00:00:00'\n    ...              ...\n    proj:shape      object {1800.0, 3600.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    epsg            int64 4326\n  * time            (time) datetime64[ns] 2016-01-01 2016-02-01 ... 2023-09-01\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    resolution:  0.1xarray.DataArray'stackstac-a148846a6e7930da708c1b52af0a7d72'time: 93y: 97x: 143dask.array&lt;chunksize=(1, 97, 143), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n9.84 MiB\n108.37 kiB\n\n\nShape\n(93, 97, 143)\n(1, 97, 143)\n\n\nDask graph\n93 chunks in 8 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                             143 97 93\n\n\n\n\nCoordinates: (16)id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202301_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202302_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202303_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202304_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202305_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202306_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202307_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202308_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202309_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-4.7 -4.6 -4.5 -4.4 ... 9.3 9.4 9.5axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4. , -3.9, -3.8, -3.7, -3.6,\n       -3.5, -3.4, -3.3, -3.2, -3.1, -3. , -2.9, -2.8, -2.7, -2.6, -2.5, -2.4,\n       -2.3, -2.2, -2.1, -2. , -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2,\n       -1.1, -1. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,\n        1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,\n        2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,\n        3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,\n        4.9,  5. ,  5.1,  5.2,  5.3,  5.4,  5.5,  5.6,  5.7,  5.8,  5.9,  6. ,\n        6.1,  6.2,  6.3,  6.4,  6.5,  6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,\n        7.3,  7.4,  7.5,  7.6,  7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,\n        8.5,  8.6,  8.7,  8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5])y(y)float6451.0 50.9 50.8 ... 41.6 41.5 41.4axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([51. , 50.9, 50.8, 50.7, 50.6, 50.5, 50.4, 50.3, 50.2, 50.1, 50. , 49.9,\n       49.8, 49.7, 49.6, 49.5, 49.4, 49.3, 49.2, 49.1, 49. , 48.9, 48.8, 48.7,\n       48.6, 48.5, 48.4, 48.3, 48.2, 48.1, 48. , 47.9, 47.8, 47.7, 47.6, 47.5,\n       47.4, 47.3, 47.2, 47.1, 47. , 46.9, 46.8, 46.7, 46.6, 46.5, 46.4, 46.3,\n       46.2, 46.1, 46. , 45.9, 45.8, 45.7, 45.6, 45.5, 45.4, 45.3, 45.2, 45.1,\n       45. , 44.9, 44.8, 44.7, 44.6, 44.5, 44.4, 44.3, 44.2, 44.1, 44. , 43.9,\n       43.8, 43.7, 43.6, 43.5, 43.4, 43.3, 43.2, 43.1, 43. , 42.9, 42.8, 42.7,\n       42.6, 42.5, 42.4, 42.3, 42.2, 42.1, 42. , 41.9, 41.8, 41.7, 41.6, 41.5,\n       41.4])proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)start_datetime(time)&lt;U19'2016-01-01T00:00:00' ... '2023-...array(['2016-01-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-12-01T00:00:00',\n       '2017-01-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-12-01T00:00:00',\n       '2018-01-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-12-01T00:00:00',\n       '2019-01-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-04-01T00:00:00',\n...\n       '2020-07-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-12-01T00:00:00',\n       '2021-01-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-12-01T00:00:00',\n       '2022-01-01T00:00:00', '2022-02-01T00:00:00',\n       '2022-03-01T00:00:00', '2022-04-01T00:00:00',\n       '2022-05-01T00:00:00', '2022-06-01T00:00:00',\n       '2022-07-01T00:00:00', '2022-08-01T00:00:00',\n       '2022-09-01T00:00:00', '2022-10-01T00:00:00',\n       '2022-11-01T00:00:00', '2022-12-01T00:00:00',\n       '2023-01-01T00:00:00', '2023-02-01T00:00:00',\n       '2023-03-01T00:00:00', '2023-04-01T00:00:00',\n       '2023-05-01T00:00:00', '2023-06-01T00:00:00',\n       '2023-07-01T00:00:00', '2023-08-01T00:00:00',\n       '2023-09-01T00:00:00'], dtype='&lt;U19')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)end_datetime(time)&lt;U19'2016-01-31T00:00:00' ... '2023-...array(['2016-01-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-03-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-07-31T00:00:00', '2016-08-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-12-31T00:00:00',\n       '2017-01-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-03-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-07-31T00:00:00', '2017-08-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-12-31T00:00:00',\n       '2018-01-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-03-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-07-31T00:00:00', '2018-08-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-12-31T00:00:00',\n       '2019-01-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-03-31T00:00:00', '2019-04-30T00:00:00',\n...\n       '2020-07-31T00:00:00', '2020-08-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-12-31T00:00:00',\n       '2021-01-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-03-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-07-31T00:00:00', '2021-08-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-12-31T00:00:00',\n       '2022-01-31T00:00:00', '2022-02-28T00:00:00',\n       '2022-03-31T00:00:00', '2022-04-30T00:00:00',\n       '2022-05-31T00:00:00', '2022-06-30T00:00:00',\n       '2022-07-31T00:00:00', '2022-08-31T00:00:00',\n       '2022-09-30T00:00:00', '2022-10-31T00:00:00',\n       '2022-11-30T00:00:00', '2022-12-31T00:00:00',\n       '2023-01-31T00:00:00', '2023-02-28T00:00:00',\n       '2023-03-31T00:00:00', '2023-04-30T00:00:00',\n       '2023-05-31T00:00:00', '2023-06-30T00:00:00',\n       '2023-07-31T00:00:00', '2023-08-31T00:00:00',\n       '2023-09-30T00:00:00'], dtype='&lt;U19')proj:epsg()float644.326e+03array(4326.)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)time(time)datetime64[ns]2016-01-01 ... 2023-09-01array(['2016-01-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2022-01-01T00:00:00.000000000', '2022-02-01T00:00:00.000000000',\n       '2022-03-01T00:00:00.000000000', '2022-04-01T00:00:00.000000000',\n       '2022-05-01T00:00:00.000000000', '2022-06-01T00:00:00.000000000',\n       '2022-07-01T00:00:00.000000000', '2022-08-01T00:00:00.000000000',\n       '2022-09-01T00:00:00.000000000', '2022-10-01T00:00:00.000000000',\n       '2022-11-01T00:00:00.000000000', '2022-12-01T00:00:00.000000000',\n       '2023-01-01T00:00:00.000000000', '2023-02-01T00:00:00.000000000',\n       '2023-03-01T00:00:00.000000000', '2023-04-01T00:00:00.000000000',\n       '2023-05-01T00:00:00.000000000', '2023-06-01T00:00:00.000000000',\n       '2023-07-01T00:00:00.000000000', '2023-08-01T00:00:00.000000000',\n       '2023-09-01T00:00:00.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-4.7499999999999885 0.09999999999999992 0.0 51.05 0.0 -0.10000000000000002array(0)Indexes: (3)xPandasIndexPandasIndex(Index([ -4.699999999999989,  -4.599999999999994,                -4.5,\n        -4.399999999999977,  -4.299999999999983,  -4.199999999999989,\n        -4.099999999999994,                -4.0, -3.8999999999999773,\n        -3.799999999999983,\n       ...\n         8.600000000000023,   8.700000000000017,   8.800000000000011,\n         8.900000000000006,                 9.0,   9.100000000000023,\n         9.200000000000017,   9.300000000000011,   9.400000000000006,\n                       9.5],\n      dtype='float64', name='x', length=143))yPandasIndexPandasIndex(Index([              51.0,               50.9,               50.8,\n       50.699999999999996, 50.599999999999994,               50.5,\n                     50.4,               50.3, 50.199999999999996,\n       50.099999999999994,               50.0,               49.9,\n                     49.8, 49.699999999999996, 49.599999999999994,\n                     49.5,               49.4,               49.3,\n       49.199999999999996, 49.099999999999994,               49.0,\n                     48.9,               48.8, 48.699999999999996,\n       48.599999999999994,               48.5,               48.4,\n                     48.3, 48.199999999999996, 48.099999999999994,\n                     48.0,               47.9,               47.8,\n       47.699999999999996, 47.599999999999994,               47.5,\n                     47.4,               47.3, 47.199999999999996,\n       47.099999999999994,               47.0,               46.9,\n                     46.8, 46.699999999999996, 46.599999999999994,\n                     46.5,               46.4,               46.3,\n       46.199999999999996, 46.099999999999994,               46.0,\n                     45.9,               45.8, 45.699999999999996,\n       45.599999999999994,               45.5,               45.4,\n                     45.3, 45.199999999999996, 45.099999999999994,\n                     45.0,               44.9,               44.8,\n       44.699999999999996, 44.599999999999994,               44.5,\n                     44.4,               44.3, 44.199999999999996,\n       44.099999999999994,               44.0,               43.9,\n                     43.8, 43.699999999999996, 43.599999999999994,\n                     43.5,               43.4,               43.3,\n       43.199999999999996, 43.099999999999994,               43.0,\n                     42.9,               42.8, 42.699999999999996,\n       42.599999999999994,               42.5,               42.4,\n                     42.3, 42.199999999999996, 42.099999999999994,\n                     42.0,               41.9,               41.8,\n       41.699999999999996, 41.599999999999994,               41.5,\n                     41.4],\n      dtype='float64', name='y'))timePandasIndexPandasIndex(DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01', '2016-04-01',\n               '2016-05-01', '2016-06-01', '2016-07-01', '2016-08-01',\n               '2016-09-01', '2016-10-01', '2016-11-01', '2016-12-01',\n               '2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01',\n               '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01',\n               '2017-09-01', '2017-10-01', '2017-11-01', '2017-12-01',\n               '2018-01-01', '2018-02-01', '2018-03-01', '2018-04-01',\n               '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n               '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n               '2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-06-01', '2019-07-01', '2019-08-01',\n               '2019-09-01', '2019-10-01', '2019-11-01', '2019-12-01',\n               '2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',\n               '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',\n               '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01',\n               '2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01',\n               '2021-05-01', '2021-06-01', '2021-07-01', '2021-08-01',\n               '2021-09-01', '2021-10-01', '2021-11-01', '2021-12-01',\n               '2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01',\n               '2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01',\n               '2022-09-01', '2022-10-01', '2022-11-01', '2022-12-01',\n               '2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01',\n               '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',\n               '2023-09-01'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "title": "Open and visualize COGs",
    "section": "Compute and plot",
    "text": "Compute and plot\nSo far we have just been setting up a calculation lazily in Dask. Now we can trigger computation using .compute().\n\n%%time\n\nimage_stack = subset.compute()\n\nCPU times: user 3.21 s, sys: 581 ms, total: 3.79 s\nWall time: 7.03 s\n\n\n\n# get the 2% and 98% percentiles for min and max bounds of color\nvmin, vmax = image_stack.quantile(0.02).item(), image_stack.quantile(0.98).item()\n\nimage_stack.hvplot(\n    groupby=\"time\",\n    tiles=True,\n    colorbar=False,\n    clim=(vmin, vmax),\n    cmap=\"viridis\",\n    alpha=0.8,\n    frame_height=512,\n    widget_location=\"bottom\",\n)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "Open and visualize COGs"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html",
    "href": "notebooks/quickstarts/list-collections.html",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#run-this-notebook",
    "href": "notebooks/quickstarts/list-collections.html#run-this-notebook",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#approach",
    "href": "notebooks/quickstarts/list-collections.html#approach",
    "title": "List STAC collections",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog\nIterate over collections and print the title of each collection",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "href": "notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "title": "List STAC collections",
    "section": "Open STAC catalog",
    "text": "Open STAC catalog\n\nfrom pystac_client import Client\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncatalog = Client.open(STAC_API_URL)",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#list-collections",
    "href": "notebooks/quickstarts/list-collections.html#list-collections",
    "title": "List STAC collections",
    "section": "List collections",
    "text": "List collections\n\ncollections = list(catalog.get_collections())\nfor collection in sorted(collections, key=lambda x: x.title):\n    print(collection.title)\n\n\n\n0-100 cm Volumetric Soil Moisture (%)\nActive Fire Line\nAerosol Optical Depth (AOD)\nAnnual LAI maps for 2003 and 2021 (Bangladesh)\nAnnual land cover maps for 2001 and 2020\nBlack Marble 500m Nightlights Daily Dataset\nBlack Marble High Definition Nightlights 1 band Dataset\nBlack Marble High Definition Nightlights Monthly Dataset\nBurn Area Reflectance Classification for Thomas Fire\nCO₂ (Avg)\nCO₂ (Diff)\nCaldor Fire Behavior\nCaldor Fire Burn Severity\nChange in ET for 2020 fires using LIS outputs\nChange in transpiration for 2020 fires using LIS outputs\nECCO sea-surface height change from 1992 to 2017\nEvapotranspiration - LIS 10km Global DA\nFire Perimeters\nGEOGLAM Crop Monitor\nGRDI BUILT Constituent Raster\nGRDI CDR Constituent Raster\nGRDI Filled Missing Values Count\nGRDI IMR Constituent Raster\nGRDI SHDI Constituent Raster\nGRDI V1 raster\nGRDI VNL Constituent Raster\nGRDI VNL Slope Constituent Raster\nGlobal TWS Non-Stationarity Index\nGridded 2012 EPA Methane Emissions - Abandoned Coal Mines\nGridded 2012 EPA Methane Emissions - Composting\nGridded 2012 EPA Methane Emissions - Domestic Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Enteric Fermentation\nGridded 2012 EPA Methane Emissions - Ferroalloy Production\nGridded 2012 EPA Methane Emissions - Field Burning\nGridded 2012 EPA Methane Emissions - Field Burning (monthly)\nGridded 2012 EPA Methane Emissions - Forest Fires\nGridded 2012 EPA Methane Emissions - Forest Fires (daily)\nGridded 2012 EPA Methane Emissions - Industrial Landfills\nGridded 2012 EPA Methane Emissions - Industrial Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Manure Management\nGridded 2012 EPA Methane Emissions - Manure Management (monthly)\nGridded 2012 EPA Methane Emissions - Mobile Combustion\nGridded 2012 EPA Methane Emissions - Municipal Landfills\nGridded 2012 EPA Methane Emissions - Natural Gas Distribution\nGridded 2012 EPA Methane Emissions - Natural Gas Processing\nGridded 2012 EPA Methane Emissions - Natural Gas Production\nGridded 2012 EPA Methane Emissions - Natural Gas Production (monthly)\nGridded 2012 EPA Methane Emissions - Natural Gas Transmission\nGridded 2012 EPA Methane Emissions - Petrochemical Production\nGridded 2012 EPA Methane Emissions - Petroleum\nGridded 2012 EPA Methane Emissions - Petroleum (monthly)\nGridded 2012 EPA Methane Emissions - Rice Cultivation\nGridded 2012 EPA Methane Emissions - Rice Cultivation (monthly)\nGridded 2012 EPA Methane Emissions - Stationary Combustion\nGridded 2012 EPA Methane Emissions - Stationary Combustion (monthly)\nGridded 2012 EPA Methane Emissions - Surface Coal Mines\nGridded 2012 EPA Methane Emissions - Underground Coal Mines\nGross Primary Productivity - LIS 10km Global DA\nGross Primary Productivity Trend - LIS 10km Global DA\nGroundwater Storage - LIS 10km Global DA\nHLSL30.002 Environmental Justice Events\nHLSS30.002 Environmental Justice Events\nHouston LST (Diff)\nHouston Land Cover\nHouston NDVI: decadal average\nHouston land surface temperature at night time - decadal average\nHouston land surface temperature during daytime - decadal average\nHurricane Ida - Blue Tarps PlanetScope Image\nHurricane Ida - Detected Blue Tarps\nICESat-2 L4 Monthly Gridded Sea Ice Thickness (COGs)\nMTBS Burn Severity\nMaximum Fire Radiative Power for Thomas Fire\nNCEO Africa Aboveground Woody Biomass 2017\nNO₂\nNO₂ (Diff)\nOMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)\nOMI_trno2 - 0.10 x 0.10 Annual as Cloud-Optimized GeoTIFFs (COGs)\nPopulation Density Maps using satellite imagery built by Meta\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjections of Snow Water Equivalent (SWE)\nProjections of Snow Water Equivalent (SWE) - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) - SSP5-8.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP5-8.5\nProjections of Snow Water Equivalent (SWE) losses\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Pine Island Glacier\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Thwaites Glacier\nSnow Water Equivalent - LIS 10km Global DA\nSocial Vulnerability Index (Household)\nSocial Vulnerability Index (Household) (Masked)\nSocial Vulnerability Index (Housing)\nSocial Vulnerability Index (Housing) (Masked)\nSocial Vulnerability Index (Minority)\nSocial Vulnerability Index (Minority) (Masked)\nSocial Vulnerability Index (Overall)\nSocial Vulnerability Index (Overall) (Masked)\nSocial Vulnerability Index (SocioEconomic)\nSocial Vulnerability Index (SocioEconomic) (Masked)\nStream network across the Contiguous United States\nStreamflow - LIS 10km Global DA\nSubsurface Runoff - LIS 10km Global DA\nSurface runoff - LIS 10km Global DA\nTerrestrial Water Storage (TWS) Anomalies\nTerrestrial Water Storage - LIS 10km Global DA\nTerrestrial Water Storage Trend - LIS 10km Global DA\nTotal Precipitation - LIS 10km Global DA\nTrend in Terrestrial Water Storage (TWS) Anomalies\nVIIRs Fire Detections\ndisalexi-etsuppression",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#alternate-approachs",
    "href": "notebooks/quickstarts/list-collections.html#alternate-approachs",
    "title": "List STAC collections",
    "section": "Alternate approachs",
    "text": "Alternate approachs\nInstead of exploring STAC catalog programatically, you can discover available collections the following ways:\n\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com",
    "crumbs": [
      "Usage Examples",
      "Quickstarts",
      "Accessing the Data Directly",
      "List STAC collections"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html",
    "href": "notebooks/tutorials/zonal-statistics-validation.html",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#run-this-notebook",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#run-this-notebook",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#approach",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#approach",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Approach",
    "text": "Approach\n\nMotivation\nThe VEDA backend (via TiTiler) provides an API endpoint for computing zonal statistics (average, standard deviation, and other metrics over a geographic subset) across gridded (raster) data such as satellite imagery or climate datasets.\nSome statistics, such as average, median, standard deviation, or percentiles are sensitive to differences in grid cell / pixel sizes: when some grid cells are (in metric units) have a larger area than others, the values in these cells will be under-represented. Grid cell sizes depends on the grid / projection of the data.\nVarying grid cell sizes is common for climate datasets that are stored on a grid in geographic coordinates (lat/lon), for example a 0.1 degree by 0.1 degree global grid. Here, grid cell size will decrease from low to high latitudes. Computing averages over large spans of latitude will result in statistics where values closer to the poles are strongly over-represented.\nTo avoid this inaccuracy in zonal statistics computed with our API, we introduced a method to reproject the source data to an equal-area projection as an intermediate step in calculating statistics.\nNote: this reprojection is not needed for example for accurate zonal statistics on a Sentinel-2 scene, using the Military Grid Reference System (MGRS) and a Mercator (UTM) projection. Here, pixel areas are the same across the scene in the native projection.\n\n\nIn this notebook\nThis notebook presents a validation of VEDA’s API for zonal statistics against a known way to compute area-weighted averages for gridded datasets on a regular lat/lon grid.\nFor illustration, we choose a real dataset from the VEDA data catalog and a subsetting area that spans a large latitude range.\nThe figures below show the average calculated over that area of interest with the different methods, for comparison.\n\n%pip install pystac_client\n\nRequirement already satisfied: pystac_client in /srv/conda/envs/notebook/lib/python3.10/site-packages (0.7.2)\nRequirement already satisfied: requests&gt;=2.28.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pystac_client) (2.31.0)\nRequirement already satisfied: pystac[validation]&gt;=1.7.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pystac_client) (1.8.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pystac_client) (2.8.2)\nRequirement already satisfied: jsonschema&gt;=4.0.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pystac[validation]&gt;=1.7.2-&gt;pystac_client) (4.17.3)\nRequirement already satisfied: six&gt;=1.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pystac_client) (1.16.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests&gt;=2.28.2-&gt;pystac_client) (3.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests&gt;=2.28.2-&gt;pystac_client) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests&gt;=2.28.2-&gt;pystac_client) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests&gt;=2.28.2-&gt;pystac_client) (2023.5.7)\nRequirement already satisfied: attrs&gt;=17.4.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jsonschema&gt;=4.0.1-&gt;pystac[validation]&gt;=1.7.2-&gt;pystac_client) (23.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jsonschema&gt;=4.0.1-&gt;pystac[validation]&gt;=1.7.2-&gt;pystac_client) (0.19.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport tqdm\nimport requests\nimport rasterio\nimport rasterio.crs\nimport xarray as xr\nimport rioxarray\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pystac_client import Client",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#load-and-inspect-dataset-from-ghgc-stac-catalog",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#load-and-inspect-dataset-from-ghgc-stac-catalog",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Load and inspect dataset from GHGC STAC catalog",
    "text": "Load and inspect dataset from GHGC STAC catalog\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com/\"\n\nCOLLECTION_ID = \"MO_NPP_npp_vgpm\"\nASSET_NAME = \"cog_default\"\n\n\ncatalog = Client.open(STAC_API_URL)\ncollection = catalog.get_collection(COLLECTION_ID)\n\n\nitems = list(collection.get_all_items())[:15]\n\n\nwith rasterio.open(items[0].assets[ASSET_NAME].href) as ds:\n    print(ds.profile)\n    \n    assert ds.crs == rasterio.crs.CRS.from_epsg(4326)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -32767.0, 'width': 8640, 'height': 4320, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.041666667844178655, 0.0, -180.0000050868518,\n       0.0, -0.04166666725549082, 89.9999974571629), 'blockxsize': 512, 'blockysize': 512, 'tiled': True, 'compress': 'deflate', 'interleave': 'band'}",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#define-formula-for-grid-cell-area-for-geographic-coordinates",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#define-formula-for-grid-cell-area-for-geographic-coordinates",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Define formula for grid cell area for geographic coordinates",
    "text": "Define formula for grid cell area for geographic coordinates\n\ndef _get_unique_diff(arr):\n    assert np.ndim(arr) == 1\n    deltas = np.diff(arr)\n    \n    if not np.allclose(deltas, deltas[0]):    \n        raise ValueError(f\"The spacing in the array is not uniform: {list(np.unique(deltas))}\")\n    return deltas[0]\n\n\ndef grid_cell_area(lat):\n    \"\"\"\n    https://www.mathworks.com/matlabcentral/answers/447847-how-to-calculate-the-area-of-each-grid-cell\n    https://gis.stackexchange.com/a/28156\n    \"\"\"\n    # get lat spacing asserting it is uniform\n    dlat = _get_unique_diff(lat)\n    \n    # calculate cell edge lat\n    lat_edge = lat - dlat / 2.\n    \n    # radius of Earth in meters\n    R_e = 6371e3\n    \n    # calculate cell area as a function of latitude\n    return R_e ** 2 * (np.sin(np.radians(lat_edge + dlat)) - np.sin(np.radians(lat_edge))) * np.radians(dlat)",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#define-a-geometry-to-average-over",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#define-a-geometry-to-average-over",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Define a geometry to average over",
    "text": "Define a geometry to average over\n\nAOI_NAME = \"Americas south to north\"\n\nAOI = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              -115,\n              82\n            ],\n            [\n              -115,\n              -82\n            ],\n            [\n              -43,\n              -82\n            ],\n            [\n              -43,\n              82\n            ],\n            [\n              -115,\n              82\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    }\n  ]\n}",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#compute-averages-with-xarray",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#compute-averages-with-xarray",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compute averages with Xarray",
    "text": "Compute averages with Xarray\n\ntimeseries = {\n    \"start_datetime\": [],\n    \"average_weighted\": [],\n    \"average_unweighted\": []\n}\n\nfor item in tqdm.tqdm(items):\n    item_uri = item.assets[ASSET_NAME].href\n\n    with xr.open_dataset(item_uri, engine=\"rasterio\") as xds:\n        \n        # calculate area as a function of latitude\n        area_lat = grid_cell_area(xds.y.values)\n        area_lat_2d = np.ones((len(xds.y), len(xds.x))) * area_lat[:, np.newaxis]\n        xds[\"area\"] = xr.DataArray(area_lat_2d, dims=(\"y\", \"x\"))\n        \n        # clip to geometry\n        xds_clip = xds.rio.clip([AOI[\"features\"][0][\"geometry\"]])\n        \n        # get data arrays\n        data = xds_clip[\"band_data\"].isel(band=0).to_masked_array()\n        weights = xds_clip[\"area\"].to_masked_array()\n        weights.mask = data.mask\n        \n        # calculate averages\n        average_weighted = (data * weights).sum() / weights.sum()\n        average_unweighted = data.mean()\n        \n        timeseries[\"average_weighted\"].append(average_weighted)\n        timeseries[\"average_unweighted\"].append(average_unweighted)\n        timeseries[\"start_datetime\"].append(item.properties[\"start_datetime\"])\n        \n        lat_vals = xds.y.values\n\n100%|██████████| 12/12 [00:51&lt;00:00,  4.27s/it]\n\n\n\ndf = pd.DataFrame(timeseries).set_index(\"start_datetime\")\ndf.index = pd.to_datetime(df.index)\n\n\ndf.plot(ylabel=f\"average {ASSET_NAME}\", title=f\"{ASSET_NAME} averaged over {AOI_NAME}\") ;",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#plot-grid-cell-area-as-a-function-of-latitude",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#plot-grid-cell-area-as-a-function-of-latitude",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Plot grid cell area as a function of latitude",
    "text": "Plot grid cell area as a function of latitude\n\nplt.plot(lat_vals, area_lat_2d[:, 0])\nplt.ylabel(\"Grid cell area (m²)\")\nplt.xlabel(\"Latitude\") ;",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#compute-zonal-averages-using-titiler-api",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#compute-zonal-averages-using-titiler-api",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compute zonal averages using TiTiler API",
    "text": "Compute zonal averages using TiTiler API\nWe make use of the option on TiTiler to reproject the data subset to an equal-area projection (Equal Area Cylindrical) before computing the statistics.\n\nWORKING_PROJECTION = \"+proj=cea\"\n\n\ndef generate_stats(item, geojson, asset_name, params=None):\n    params = params or {}\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\n            \"url\": item.assets[asset_name].href,\n            **params\n        },\n        json=geojson,\n    )\n    result.raise_for_status()\n    result_data = result.json()\n    return {\n        **result_data[\"features\"][0][\"properties\"][\"statistics\"][\"b1\"],\n        \"start_datetime\": item.properties[\"start_datetime\"],\n    }\n\n\ntimeseries_titiler_noproj = []\ntimeseries_titiler_proj = []\n\nfor item in tqdm.tqdm(items):\n    # generate stats with and without reprojection\n    stats_noproj = generate_stats(item, AOI, ASSET_NAME)\n    stats_proj = generate_stats(item, AOI, ASSET_NAME, params={\"dst_crs\": WORKING_PROJECTION})\n\n    timeseries_titiler_noproj.append(stats_noproj)\n    timeseries_titiler_proj.append(stats_proj)\n\n100%|██████████| 12/12 [03:05&lt;00:00, 15.47s/it]\n\n\n\ndef _to_dataframe(stats):\n    df = pd.DataFrame(stats)\n    df = df[[\"start_datetime\", \"mean\"]]\n    df = df.set_index(\"start_datetime\")\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\ndf_titiler_proj = _to_dataframe(timeseries_titiler_proj)[\"mean\"]\ndf_titiler_noproj = _to_dataframe(timeseries_titiler_noproj)[\"mean\"]",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/zonal-statistics-validation.html#compare-all-methods-against-the-xarray-computed-weighted-average",
    "href": "notebooks/tutorials/zonal-statistics-validation.html#compare-all-methods-against-the-xarray-computed-weighted-average",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compare all methods against the xarray-computed weighted average",
    "text": "Compare all methods against the xarray-computed weighted average\n\ndf_all = df.copy()\ndf_all[\"average_titiler_noproj\"] = df_titiler_noproj\ndf_all[\"average_titiler_proj\"] = df_titiler_proj\n\n\nfig = plt.figure()\nax = fig.gca()\n\nax.set_prop_cycle(\n    linestyle=['-', '--', '-.', ':'],\n    color=[\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\"],\n    marker=['.', 'o', 'x', '*']\n)\ndf_all.plot(title=f\"{ASSET_NAME} averaged over {AOI_NAME}\", ax=ax) ;\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nax = fig.gca()\n\nax.plot(df_all[\"average_weighted\"], df_all[\"average_weighted\"], color=\"grey\")\n\nfor key, style in {\n    \"average_titiler_proj\": dict(c=\"#e41a1c\"),\n    \"average_unweighted\": dict(c=\"#377eb8\"),\n    \"average_titiler_noproj\": dict(c=\"#984ea3\", marker=\"x\", s=100)\n}.items():\n    df_all.plot.scatter(\"average_weighted\", key, ax=ax, label=key, **style)\n\nax.set_title(f\"{ASSET_NAME} averaged over {AOI_NAME}\")\nax.set_ylabel(\"average calculated with different methods\") ;\n\n\n\n\n\n\n\n\n\ndef rmse(a, b):\n    return np.sqrt(\n        np.mean(\n            (a - b) ** 2\n        )\n    )\n\n\nkeys = [\"average_unweighted\", \"average_titiler_noproj\", \"average_titiler_proj\"]\n\ndf_rmse = pd.DataFrame(\n    {\n        \"algorithm\": keys,\n        \"average\": [rmse(df_all[key], df_all[\"average_weighted\"]) for key in keys]\n    }    \n)\n\ndf_rmse = df_rmse.set_index(\"algorithm\")\ndf_rmse.plot.barh(xlabel=\"RMSE wrt average_weighted\", title=f\"{ASSET_NAME} averaged over {AOI_NAME}\") ;",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "Calculating accurate zonal statistics with varying grid cell / pixel area"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html",
    "href": "notebooks/tutorials/gif-generation.html",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "This notebook demonstrates how to use the cog/feature endpoint to generate GIFs from data in the VEDA API.\nThe overall process will be: 1. Use the STAC API to gather a list of STAC Items which will each become on frame in our gif 2. Query the /cog/feater endpoint with the asset URL and a geojson geometry 3. Stack all of the generated images into a animated GIF\n\n\n\n# Standard lib imports\nfrom concurrent.futures import ThreadPoolExecutor\nimport datetime\nimport glob\nimport json\nimport os\nimport requests\nimport tempfile\nimport time\nimport io\nfrom IPython import display\n\n# 3rd party imports\nimport folium\nimport numpy as np\n\n# import PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport rasterio\nimport rasterio.features\nimport rasterio.plot\n\n\n\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Collection we'll be using to generate the GIF\ncollection = \"no2-monthly\"\n\n\n\n\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[45, 0],\n    zoom_start=5,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm\n\n\n\n\n\n# NO2 monthly has a global extent, so we don't need to specify an area within\n# which to search. For non-global datasets, use the `bbox` parameter to specify\n# the bounding box within which to search.\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection}/items?limit=100\").json()[\n    \"features\"\n]\n\n\n# Available dates:\ndates = [item[\"properties\"][\"start_datetime\"] for item in items]\nprint(f\"Dates available: {dates[:5]} ... {dates[-5:]}\")\n\n\n\n\nThe endpoint accepts the following parameters, among others: - format (tif, jpeg, webp, etc) - height and width - url (for the COG file to extract data from)\nAnd any other visualization parameters specific to that dataset (eg: rescale and color_map values)\n\n\n\n# get visualization parameters from collection summaries\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"no2-monthly\"\n][0][\"summaries\"][\"cog_default\"]\n\n\n\n\n\n# get PNG bytes from API\nresponse = requests.post(\n    f\"{RASTER_API_URL}/cog/feature\",\n    params={\n        \"format\": \"png\",\n        \"height\": 512,\n        \"width\": 512,\n        \"url\": items[0][\"assets\"][\"cog_default\"][\"href\"],\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n    json=france_aoi,\n)\n\nassert response.ok, response.text\n\nimage_bytes = response.content\n\n# Write to temporary file in order to display\nf = tempfile.NamedTemporaryFile(suffix=\".png\")\nf.write(image_bytes)\n\n# display PNG!\ndisplay.Image(filename=f.name, height=512, width=512)\n\n\n\n\n\nTo generate a GIF we request a PNG for each STAC Item and then use the Python Imaging Library (PIL) to combine them into a GIF. We will use a temporary directory to store all the generated PNGs and we will use multi-threading to speed up the operation\n\n# for convenience we will wrap the API call from above into a method that will\n# save the contents of the image file into a file stored within the temp directory\nfrom gif_generation_dependencies.helper_functions import generate_frame\n\n# temporary directory to hold PNGs\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    start = time.time()\n\n    args = (\n        (\n            item,  # stac item\n            france_aoi,  # aoi to crop\n            tmpdirname,  # tmpdir (optional)\n            \"png\",  # image format\n            None,  # overlay (will be discussed further)\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },  # visualization parameters\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = (Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\"))))\n    img = next(imgs)  # extract first image from iterator\n    img.save(\n        fp=\"./output.gif\",\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output.gif\")\n\n\n\n\nTo provide more interesting or engaging data to the users, we can add temporal and geospatial context to the GIF. This is possible because API can return images in geo-referenced tif format.\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    filepath = generate_frame(items[0], france_aoi, tmpdirname, image_format=\"tif\")\n\n    # Verify that the tif returned by the API is correctly georeferenced\n    georeferenced_raster_data = rasterio.open(filepath)\n\n    print(\"Data bounds: \", georeferenced_raster_data.bounds)\n    print(\"Data CRS: \", georeferenced_raster_data.crs)\n\n\n\nIn order to overlay GeoJSON over the raster, we will have to convert the geojson boundaries to a raster format. We do this with the following steps:\nFor each feature in the geojson we rasterize the feature into a mask. We use binary dialation to detect the edges of the mask, and set the values corresponding to the mask edges to 255. This approach has one known problem: if multiple features share a border (eg: two adjoining provinces) the border between then will be detected twice, once from each side (or from each feature sharing that border). This means that internal borders will be twice as thick as external borders\n\nfrom gif_generation_dependencies.helper_functions import overlay_geojson\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        geojson = json.loads(f.read())\n\n    filepath = generate_frame(\n        items[0],\n        france_aoi,\n        tmpdirname,\n        image_format=\"tif\",\n        additional_cog_feature_args={\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n    )\n\n    filepath = overlay_geojson(filepath, geojson)\n    rasterio.plot.show(rasterio.open(filepath))\n\n\n\n\nAnother way to contextualize where in the GIF’s data is, is by overlaying the GIF on top of a base map. This process is a bit more complicated: - Generate a raster image (.tif) - Overlay in on a folium map interface - Save the map interface to html - Open the html file with a headless chrome webdriver (using the selenium library) - Save a screenshot of the rendered html as a .png\n\nfrom gif_generation_dependencies.helper_functions import overlay_raster_on_folium\n\ntmpdirname = tempfile.TemporaryDirectory()\n\nimage_filepath = generate_frame(\n    items[0],\n    france_aoi,\n    tmpdirname.name,\n    image_format=\"tif\",\n    overlay=None,\n    additional_cog_feature_args={\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n)\n\nimage_filepath = overlay_raster_on_folium(image_filepath)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\nNow that we have the raster data displayed over the basemap, we want to add the date of each file\n\nfrom gif_generation_dependencies.helper_functions import overlay_date\n\ndate = items[0][\"properties\"][\"start_datetime\"]\n\n# get datestring from STAC Item properties and reformat\ndatestring = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\").date().isoformat()\n\n# Reuse the raster overlayed on the OSM basemap using folium from above:\noverlay_date(image_filepath, datestring)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\n\nI’ve combined all of the above functionality, along with a few helper functions in the file: ./gif_generation_dependencies/helper_functions.py\nI’ve also added the contextualizaiton steps (overlaying geojson, date, and folium basemap) directly into the generate_frame() method\n\n\n\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        overlay = json.loads(f.read())\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            geojson,\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.tif\")))]\n    imgs[0].save(\n        fp=\"./output_with_geojson.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_geojson.gif\")\n\n\n\n\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            \"folium\",\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    # Note: I'm searching for `*.png` files instead of *.tif files because the webdriver screenshot\n    # of the folium map interface is exported in png format (this also helps reduce the size of\n    # the final gif )\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\")))]\n    imgs[0].save(\n        fp=\"./output_with_osm_basemap.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_osm_basemap.gif\")\n\n\n\n\nRun the following cell to remove the following generated images/gifs: - output.gif - output_with_geojson.gif - output_with_osm_basemap.gif\n\nfor f in glob.glob(os.path.join(\".\", \"output*.gif\")):\n    os.remove(f)",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "href": "notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "# Standard lib imports\nfrom concurrent.futures import ThreadPoolExecutor\nimport datetime\nimport glob\nimport json\nimport os\nimport requests\nimport tempfile\nimport time\nimport io\nfrom IPython import display\n\n# 3rd party imports\nimport folium\nimport numpy as np\n\n# import PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport rasterio\nimport rasterio.features\nimport rasterio.plot",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#define-global-variables",
    "href": "notebooks/tutorials/gif-generation.html#define-global-variables",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "STAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Collection we'll be using to generate the GIF\ncollection = \"no2-monthly\"",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "href": "notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "We can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[45, 0],\n    zoom_start=5,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "href": "notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "# NO2 monthly has a global extent, so we don't need to specify an area within\n# which to search. For non-global datasets, use the `bbox` parameter to specify\n# the bounding box within which to search.\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection}/items?limit=100\").json()[\n    \"features\"\n]\n\n\n# Available dates:\ndates = [item[\"properties\"][\"start_datetime\"] for item in items]\nprint(f\"Dates available: {dates[:5]} ... {dates[-5:]}\")",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#the-cogfeature-endpoint",
    "href": "notebooks/tutorials/gif-generation.html#the-cogfeature-endpoint",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "The endpoint accepts the following parameters, among others: - format (tif, jpeg, webp, etc) - height and width - url (for the COG file to extract data from)\nAnd any other visualization parameters specific to that dataset (eg: rescale and color_map values)\n\n\n\n# get visualization parameters from collection summaries\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"no2-monthly\"\n][0][\"summaries\"][\"cog_default\"]\n\n\n\n\n\n# get PNG bytes from API\nresponse = requests.post(\n    f\"{RASTER_API_URL}/cog/feature\",\n    params={\n        \"format\": \"png\",\n        \"height\": 512,\n        \"width\": 512,\n        \"url\": items[0][\"assets\"][\"cog_default\"][\"href\"],\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n    json=france_aoi,\n)\n\nassert response.ok, response.text\n\nimage_bytes = response.content\n\n# Write to temporary file in order to display\nf = tempfile.NamedTemporaryFile(suffix=\".png\")\nf.write(image_bytes)\n\n# display PNG!\ndisplay.Image(filename=f.name, height=512, width=512)",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#generating-a-gif",
    "href": "notebooks/tutorials/gif-generation.html#generating-a-gif",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "To generate a GIF we request a PNG for each STAC Item and then use the Python Imaging Library (PIL) to combine them into a GIF. We will use a temporary directory to store all the generated PNGs and we will use multi-threading to speed up the operation\n\n# for convenience we will wrap the API call from above into a method that will\n# save the contents of the image file into a file stored within the temp directory\nfrom gif_generation_dependencies.helper_functions import generate_frame\n\n# temporary directory to hold PNGs\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    start = time.time()\n\n    args = (\n        (\n            item,  # stac item\n            france_aoi,  # aoi to crop\n            tmpdirname,  # tmpdir (optional)\n            \"png\",  # image format\n            None,  # overlay (will be discussed further)\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },  # visualization parameters\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = (Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\"))))\n    img = next(imgs)  # extract first image from iterator\n    img.save(\n        fp=\"./output.gif\",\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output.gif\")",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#adding-context",
    "href": "notebooks/tutorials/gif-generation.html#adding-context",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "To provide more interesting or engaging data to the users, we can add temporal and geospatial context to the GIF. This is possible because API can return images in geo-referenced tif format.\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    filepath = generate_frame(items[0], france_aoi, tmpdirname, image_format=\"tif\")\n\n    # Verify that the tif returned by the API is correctly georeferenced\n    georeferenced_raster_data = rasterio.open(filepath)\n\n    print(\"Data bounds: \", georeferenced_raster_data.bounds)\n    print(\"Data CRS: \", georeferenced_raster_data.crs)\n\n\n\nIn order to overlay GeoJSON over the raster, we will have to convert the geojson boundaries to a raster format. We do this with the following steps:\nFor each feature in the geojson we rasterize the feature into a mask. We use binary dialation to detect the edges of the mask, and set the values corresponding to the mask edges to 255. This approach has one known problem: if multiple features share a border (eg: two adjoining provinces) the border between then will be detected twice, once from each side (or from each feature sharing that border). This means that internal borders will be twice as thick as external borders\n\nfrom gif_generation_dependencies.helper_functions import overlay_geojson\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        geojson = json.loads(f.read())\n\n    filepath = generate_frame(\n        items[0],\n        france_aoi,\n        tmpdirname,\n        image_format=\"tif\",\n        additional_cog_feature_args={\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n    )\n\n    filepath = overlay_geojson(filepath, geojson)\n    rasterio.plot.show(rasterio.open(filepath))\n\n\n\n\nAnother way to contextualize where in the GIF’s data is, is by overlaying the GIF on top of a base map. This process is a bit more complicated: - Generate a raster image (.tif) - Overlay in on a folium map interface - Save the map interface to html - Open the html file with a headless chrome webdriver (using the selenium library) - Save a screenshot of the rendered html as a .png\n\nfrom gif_generation_dependencies.helper_functions import overlay_raster_on_folium\n\ntmpdirname = tempfile.TemporaryDirectory()\n\nimage_filepath = generate_frame(\n    items[0],\n    france_aoi,\n    tmpdirname.name,\n    image_format=\"tif\",\n    overlay=None,\n    additional_cog_feature_args={\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n)\n\nimage_filepath = overlay_raster_on_folium(image_filepath)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\nNow that we have the raster data displayed over the basemap, we want to add the date of each file\n\nfrom gif_generation_dependencies.helper_functions import overlay_date\n\ndate = items[0][\"properties\"][\"start_datetime\"]\n\n# get datestring from STAC Item properties and reformat\ndatestring = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\").date().isoformat()\n\n# Reuse the raster overlayed on the OSM basemap using folium from above:\noverlay_date(image_filepath, datestring)\n\ndisplay.Image(filename=image_filepath)",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "href": "notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "I’ve combined all of the above functionality, along with a few helper functions in the file: ./gif_generation_dependencies/helper_functions.py\nI’ve also added the contextualizaiton steps (overlaying geojson, date, and folium basemap) directly into the generate_frame() method",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "href": "notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "with tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        overlay = json.loads(f.read())\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            geojson,\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.tif\")))]\n    imgs[0].save(\n        fp=\"./output_with_geojson.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_geojson.gif\")",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "href": "notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "with tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            \"folium\",\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    # Note: I'm searching for `*.png` files instead of *.tif files because the webdriver screenshot\n    # of the folium map interface is exported in png format (this also helps reduce the size of\n    # the final gif )\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\")))]\n    imgs[0].save(\n        fp=\"./output_with_osm_basemap.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_osm_basemap.gif\")",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#cleanup",
    "href": "notebooks/tutorials/gif-generation.html#cleanup",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "Run the following cell to remove the following generated images/gifs: - output.gif - output_with_geojson.gif - output_with_osm_basemap.gif\n\nfor f in glob.glob(os.path.join(\".\", \"output*.gif\")):\n    os.remove(f)",
    "crumbs": [
      "Usage Examples",
      "Tutorials",
      "GIF generation using the TiTiler /cog/feature endpoint"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html",
    "href": "notebooks/veda-operations/stac-item-creation.html",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "href": "notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "href": "notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "title": "STAC Item Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U rio_stac parse xpystac pystac nbss-upload --quiet\n\n\nfrom parse import parse\nfrom datetime import datetime\nimport rio_stac\nimport xarray as xr\nimport hvplot.xarray",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "href": "notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "title": "STAC Item Creation",
    "section": "Create pystac.Item",
    "text": "Create pystac.Item\nIn this section we will be creating a pystac.Item object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nITEM_ID = \"OMI_trno2_0.10x0.10_202212_Col3_V4.nc\"\nSOURCE = \"s3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif\"\n\n\n\nCalculate datetime\nCreate a function that calculates datetime when given an item_id. You can change this to depend on the source instead if that works better.\n\ndef datetime_func(item_id: str) -&gt; datetime:\n    \"\"\"Given the item_id, figure out the datetime\"\"\"\n    \n    fields = parse(\"OMI_trno2_0.10x0.10_{year:4}{month:2}_Col3_V4.nc\", item_id)\n    year = int(fields[\"year\"])\n    month = int(fields[\"month\"])\n    day = 1\n    return datetime(year, month, day)\n\nTest out the datetime function:\n\ndatetime_func(ITEM_ID)\n\ndatetime.datetime(2022, 12, 1, 0, 0)\n\n\n\n\nPut it together\nNow take your constants and datetime function and create the STAC Item using rio_stac.\n\nitem = rio_stac.stac.create_stac_item(\n  id=ITEM_ID,\n  source=SOURCE,\n  collection=COLLECTION_ID, \n  input_datetime=datetime_func(ITEM_ID),\n  with_proj=True,\n  with_raster=True,\n  asset_name=\"cog_default\",\n  asset_roles=[\"data\", \"layer\"],\n  asset_media_type=\"image/tiff; application=geotiff; profile=cloud-optimized\",\n)",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "href": "notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "title": "STAC Item Creation",
    "section": "Try it out!",
    "text": "Try it out!\nNow that you have an item you can try it out and make sure it looks good and passes validation checks.\n\nitem.validate()\n\n['https://schemas.stacspec.org/v1.0.0/item-spec/json-schema/item.json',\n 'https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/raster/v1.1.0/schema.json']\n\n\n\nitem.to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n 'properties': {'proj:epsg': 4326,\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:shape': [1800, 3600],\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'proj:projjson': {'$schema': 'https://proj.org/schemas/v0.5/projjson.schema.json',\n   'type': 'GeographicCRS',\n   'name': 'WGS 84',\n   'datum': {'type': 'GeodeticReferenceFrame',\n    'name': 'World Geodetic System 1984',\n    'ellipsoid': {'name': 'WGS 84',\n     'semi_major_axis': 6378137,\n     'inverse_flattening': 298.257223563}},\n   'coordinate_system': {'subtype': 'ellipsoidal',\n    'axis': [{'name': 'Geodetic latitude',\n      'abbreviation': 'Lat',\n      'direction': 'north',\n      'unit': 'degree'},\n     {'name': 'Geodetic longitude',\n      'abbreviation': 'Lon',\n      'direction': 'east',\n      'unit': 'degree'}]},\n   'id': {'authority': 'EPSG', 'code': 4326}},\n  'datetime': '2022-12-01T00:00:00Z'},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[(-180.0, -90.0),\n    (180.0, -90.0),\n    (180.0, 90.0),\n    (-180.0, 90.0),\n    (-180.0, -90.0)]]},\n 'links': [{'rel': &lt;RelType.COLLECTION: 'collection'&gt;,\n   'href': 'no2-monthly-diff',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'raster:bands': [{'data_type': 'float32',\n     'scale': 1.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'nodata': -1.2676506002282294e+30,\n     'statistics': {'mean': 12233282717799.0,\n      'minimum': -1.30282195779584e+16,\n      'maximum': 2.082349180465971e+16,\n      'stddev': 416857512760678.5,\n      'valid_percent': 82.7056884765625},\n     'histogram': {'count': 11,\n      'min': -1.30282195779584e+16,\n      'max': 2.082349180465971e+16,\n      'buckets': [20, 138, 881, 421049, 11300, 203, 20, 3, 1, 1]}}],\n   'roles': ['data', 'layer']}},\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json'],\n 'collection': 'no2-monthly-diff'}\n\n\n\nPlot it (optional)\nCreate a quick visual to make sure that data loads and visualizes properly.\n\ndata = xr.open_dataset(item).cog_default.isel(time=0)\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'cog_default' (y: 1800, x: 3600)&gt;\n[6480000 values with dtype=float64]\nCoordinates:\n    time            datetime64[ns] 2022-12-01\n    id              &lt;U37 ...\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    proj:transform  object ...\n    proj:projjson   object ...\n    proj:epsg       int64 ...\n    proj:bbox       object ...\n    proj:shape      object ...\n    proj:geometry   object ...\n    raster:bands    object ...\n    epsg            int64 ...xarray.DataArray'cog_default'y: 1800x: 3600...[6480000 values with dtype=float64]Coordinates: (12)time()datetime64[ns]2022-12-01array('2022-12-01T00:00:00.000000000', dtype='datetime64[ns]')id()&lt;U37...[1 values with dtype=&lt;U37]x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])proj:transform()object...[1 values with dtype=object]proj:projjson()object...[1 values with dtype=object]proj:epsg()int64...[1 values with dtype=int64]proj:bbox()object...[1 values with dtype=object]proj:shape()object...[1 values with dtype=object]proj:geometry()object...[1 values with dtype=object]raster:bands()object...[1 values with dtype=object]epsg()int64...[1 values with dtype=int64]Indexes: (2)xPandasIndexPandasIndex(Float64Index([            -180.0,             -179.9,             -179.8,\n                          -179.7,             -179.6,             -179.5,\n                          -179.4,             -179.3,             -179.2,\n                          -179.1,\n              ...\n                           179.0, 179.10000000000002, 179.20000000000005,\n                           179.3, 179.40000000000003,              179.5,\n              179.60000000000002, 179.70000000000005,              179.8,\n              179.90000000000003],\n             dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Float64Index([              90.0,               89.9,               89.8,\n                            89.7,               89.6,               89.5,\n                            89.4,               89.3,               89.2,\n                            89.1,\n              ...\n                           -89.0, -89.10000000000002, -89.20000000000002,\n              -89.30000000000001,              -89.4,              -89.5,\n              -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                           -89.9],\n             dtype='float64', name='y', length=1800))Attributes: (0)\n\n\n\ncolor_range = tuple(data.quantile([.02, .98]).values)\n\n\ndata.hvplot(\"x\", \"y\", clim=color_range, cmap=\"jet\", rasterize=True)\n\n\n\n\n\n  \n\n\n\n\nNOTE: Jet is a bad colormap because it overemphasizes certain values, but it has a very large number of colors so it is good for spotting odd patterns in the data.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "href": "notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "title": "STAC Item Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-item.ipynb",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "STAC Item Creation",
      "STAC Item Creation"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#run-this-notebook",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#run-this-notebook",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#approach",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#approach",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Approach",
    "text": "Approach\nThis notebook demonstrates how to create a kerchunk reference for the AWS Open Data Registry of NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) NetCDF files on S3. Because the NetCDF files are publicly avaialble, this notebook should be runnable in any environment with the imported libraries, up until the last step where the kerchunk reference file is stored in the veda-data-store-staging S3 bucket, as that is a protected bucket.\nTo see how to publish a kerchunk reference to a STAC collection, see the Publishing a CMIP6 Kerchunk Reference to STAC notebook.",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-1-setup",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-1-setup",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 1: Setup",
    "text": "Step 1: Setup\nImport necessary libraries and define some variables for which CMIP6 variable and model we will create references for.\n\nfrom tempfile import TemporaryDirectory\nimport boto3\nfrom dask_gateway import GatewayCluster, Gateway\nimport dask.bag as db\nimport fsspec\nimport json\nimport os\nimport ujson\nimport xarray as xr\nfrom kerchunk.combine import MultiZarrToZarr\nfrom kerchunk.hdf import SingleHdf5ToZarr\nfrom typing import Dict\n\n# Specify the CMIP model and variable to use. \n# Here we are using near-surface air temperature from the GISS-E2-1-G GCM \nmodel = \"GISS-E2-1-G\"\nvariable = \"tas\"\n# If this code were re-used for a protected bucket, anon should be False.\nanon = True\n# Note: We are only using the historical data in this example.\n# More years of data are available from multiple Shared Socio-Economic Pathways (SSPs) in the s3://nex-gddp-cmip6 bucket.\ns3_path = f\"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/{model}/historical/r1i1p1*/{variable}/*\"",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-2-initiate-file-systems-for-reading-and-temporary-writing",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-2-initiate-file-systems-for-reading-and-temporary-writing",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 2: Initiate file systems for reading and (temporary) writing",
    "text": "Step 2: Initiate file systems for reading and (temporary) writing\n\nfs_read = fsspec.filesystem(\"s3\", anon=anon, skip_instance_cache=False)\n\n# Create a temporary directory to store the .json reference files\n# Alternately, you could write these to cloud storage.\ntd = TemporaryDirectory()\ntemp_dir = td.name\nprint(f\"Writing single file references to {temp_dir}\")\n\nWriting single file references to /tmp/tmppngepuvp",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-3-discover-files-from-s3",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-3-discover-files-from-s3",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 3: Discover files from S3",
    "text": "Step 3: Discover files from S3\n\n# List available files for this model and variable\nall_files = sorted([\"s3://\" + f for f in fs_read.glob(s3_path)])\nprint(f\"{len(all_files)} discovered from {s3_path}\")\n\n65 discovered from s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/GISS-E2-1-G/historical/r1i1p1*/tas/*",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-4-define-some-functions-for-creating-and-storing-kerchunk-reference-files-for-single-files",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-4-define-some-functions-for-creating-and-storing-kerchunk-reference-files-for-single-files",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 4: Define some functions for creating and storing Kerchunk reference files for single files",
    "text": "Step 4: Define some functions for creating and storing Kerchunk reference files for single files\n\nso = dict(mode=\"rb\", anon=anon, default_fill_cache=False, default_cache_type=\"first\")\n\n# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\ndef generate_json_reference(u):\n    with fs_read.open(u, **so) as infile:\n        fname = u.split(\"/\")[-1].strip(\".nc\")        \n        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n        return fname, ujson.dumps(h5chunks.translate()).encode()\n    \ndef write_json(fname, reference_json, temp_dir):\n    outf = os.path.join(temp_dir, f\"{fname}.json\")\n    with open(outf, \"wb\") as f:\n        f.write(reference_json)\n    return outf    \n\nTest we can create a kerchunk reference for one file.\n\nfname, ref_json = generate_json_reference(all_files[0])\nwrite_json(fname, ref_json, temp_dir)\n\n'/tmp/tmppngepuvp/tas_day_GISS-E2-1-G_historical_r1i1p1f2_gn_1950.json'",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-5-use-a-dask-cluster-to-generate-references-for-all-the-data",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-5-use-a-dask-cluster-to-generate-references-for-all-the-data",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 5: Use a dask cluster to generate references for all the data",
    "text": "Step 5: Use a dask cluster to generate references for all the data\nStart the cluster and check out the dashboard for active workers, as it may take a few seconds or minutes for them to start up.\nThis was run on the VEDA JupyterHub which has access to a distributed cluster. You could also create a LocalCluster instance to run the dask bag code below. But because this code is not using DaskArrays, you could also use a regular multiprocessing library to distribute the generate_json_refence tasks.\n\ngateway = Gateway()\nclusters = gateway.list_clusters()\n\n# connect to an existing cluster - this is useful when the kernel shutdown in the middle of an interactive session\nif clusters:\n    cluster = gateway.connect(clusters[0].name)\nelse:\n    cluster = GatewayCluster(shutdown_on_close=True)\n\ncluster.scale(16)\nclient = cluster.get_client()\nclient\n\n\n     \n    \n        Client\n        Client-ed7dc353-62fb-11ee-85b5-56638a2020e1\n        \n\n\n\nConnection method: Cluster object\nCluster type: dask_gateway.GatewayCluster\n\n\nDashboard: /services/dask-gateway/clusters/prod.c614b56e6272421d8b24f9b4ab9252ec/status\n\n\n\n\n\n\n        \n            \n                Launch dashboard in JupyterLab\n            \n        \n\n        \n            \n            Cluster Info\n            \n  GatewayCluster\n  \n    Name: prod.c614b56e6272421d8b24f9b4ab9252ec\n    Dashboard: /services/dask-gateway/clusters/prod.c614b56e6272421d8b24f9b4ab9252ec/status\n  \n\n\n            \n        \n\n    \n\n\n\n\nGenerate a dask bag for all the files and store files in the temp_dir\n\n%%time\nbag = db.from_sequence(all_files, partition_size=1)\nresult = db.map(generate_json_reference, bag)\nall_references = result.compute()\noutput_files = [write_json(fname, reference_json, temp_dir) for fname, reference_json in all_references]\n\nCPU times: user 74.7 ms, sys: 3.64 ms, total: 78.3 ms\nWall time: 38.9 s\n\n\n\n\nStep 6: Combine individual references into a single consolidated reference\nStore it to local storage and test opening it.\n\n%%time\nmzz = MultiZarrToZarr(\n    output_files,\n    remote_protocol='s3',\n    remote_options={'anon': anon},\n    concat_dims=['time'],\n    coo_map={\"time\": \"cf:time\"},\n    inline_threshold=0\n)\nmulti_kerchunk = mzz.translate()\n\nCPU times: user 1.02 s, sys: 29.1 ms, total: 1.05 s\nWall time: 1.03 s\n\n\nWrite the kerchunk .json file to local storage\n\noutput_fname = f\"combined_CMIP6_daily_{model}_{variable}_kerchunk.json\"\noutput_location = os.path.join(temp_dir, output_fname)\nwith open(f\"{output_location}\", \"wb\") as f:\n    print(f\"Writing combined kerchunk reference file {output_location}\")\n    f.write(ujson.dumps(multi_kerchunk).encode())\n\nWriting combined kerchunk reference file /tmp/tmppngepuvp/combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json\n\n\n\n# open dataset as zarr object using fsspec reference file system and Xarray\nfs = fsspec.filesystem(\n    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": anon}\n)\nm = fs.get_mapper(\"\")\n\n\n# Check the data\nds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\ndisplay(ds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 600, lon: 1440, time: 23725)\nCoordinates:\n  * lat      (lat) float64 -59.88 -59.62 -59.38 -59.12 ... 89.38 89.62 89.88\n  * lon      (lon) float64 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\n  * time     (time) object 1950-01-01 12:00:00 ... 2014-12-31 12:00:00\nData variables:\n    tas      (time, lat, lon) float32 ...\nAttributes: (12/23)\n    Conventions:           CF-1.7\n    activity:              NEX-GDDP-CMIP6\n    cmip6_institution_id:  NASA-GISS\n    cmip6_license:         CC-BY-SA 4.0\n    cmip6_source_id:       GISS-E2-1-G\n    contact:               Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget...\n    ...                    ...\n    scenario:              historical\n    source:                BCSD\n    title:                 GISS-E2-1-G, r1i1p1f2, historical, global downscal...\n    tracking_id:           25d6baa3-0404-4eba-a3f1-afddbf69d4cc\n    variant_label:         r1i1p1f2\n    version:               1.0xarray.DatasetDimensions:lat: 600lon: 1440time: 23725Coordinates: (3)lat(lat)float64-59.88 -59.62 ... 89.62 89.88axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([-59.875, -59.625, -59.375, ...,  89.375,  89.625,  89.875])lon(lon)float640.125 0.375 0.625 ... 359.6 359.9axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02])time(time)object1950-01-01 12:00:00 ... 2014-12-...axis :Tlong_name :timestandard_name :timearray([cftime.DatetimeNoLeap(1950, 1, 1, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1950, 1, 2, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1950, 1, 3, 12, 0, 0, 0, has_year_zero=True), ...,\n       cftime.DatetimeNoLeap(2014, 12, 29, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 30, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 31, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)Data variables: (1)tas(time, lat, lon)float32...cell_measures :area: areacellacell_methods :area: mean time: maximumcomment :near-surface (usually, 2 meter) air temperature; derived from downscaled tasmax & tasminlong_name :Daily Near-Surface Air Temperaturestandard_name :air_temperatureunits :K[20498400000 values with dtype=float32]Indexes: (3)latPandasIndexPandasIndex(Index([-59.875, -59.625, -59.375, -59.125, -58.875, -58.625, -58.375, -58.125,\n       -57.875, -57.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,   1.875,\n         2.125,   2.375,\n       ...\n       357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125, 359.375,\n       359.625, 359.875],\n      dtype='float64', name='lon', length=1440))timePandasIndexPandasIndex(CFTimeIndex([1950-01-01 12:00:00, 1950-01-02 12:00:00, 1950-01-03 12:00:00,\n             1950-01-04 12:00:00, 1950-01-05 12:00:00, 1950-01-06 12:00:00,\n             1950-01-07 12:00:00, 1950-01-08 12:00:00, 1950-01-09 12:00:00,\n             1950-01-10 12:00:00,\n             ...\n             2014-12-22 12:00:00, 2014-12-23 12:00:00, 2014-12-24 12:00:00,\n             2014-12-25 12:00:00, 2014-12-26 12:00:00, 2014-12-27 12:00:00,\n             2014-12-28 12:00:00, 2014-12-29 12:00:00, 2014-12-30 12:00:00,\n             2014-12-31 12:00:00],\n            dtype='object', length=23725, calendar='noleap', freq='D'))Attributes: (23)Conventions :CF-1.7activity :NEX-GDDP-CMIP6cmip6_institution_id :NASA-GISScmip6_license :CC-BY-SA 4.0cmip6_source_id :GISS-E2-1-Gcontact :Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.orgcreation_date :2021-10-04T18:41:40.796912+00:00disclaimer :This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.downscalingModel :BCSDexternal_variables :areacellafrequency :dayhistory :2021-10-04T18:41:40.796912+00:00: install global attributesinstitution :NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035product :outputrealm :atmosreferences :BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.resolution_id :0.25 degreescenario :historicalsource :BCSDtitle :GISS-E2-1-G, r1i1p1f2, historical, global downscaled CMIP6 climate projection datatracking_id :25d6baa3-0404-4eba-a3f1-afddbf69d4ccvariant_label :r1i1p1f2version :1.0",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#final-step-upload-kerchunk-to-veda-bucket",
    "href": "notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#final-step-upload-kerchunk-to-veda-bucket",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Final Step: Upload Kerchunk to VEDA Bucket",
    "text": "Final Step: Upload Kerchunk to VEDA Bucket\nYou can skip this if you are not trying to upload the reference file to veda-data-store-staging.\nIf you are on the VEDA JupyterHub, you should have access to veda-data-store-staging.\n\ns3 = boto3.client('s3')\nupload_bucket_name = 'veda-data-store-staging'\nresponse = s3.upload_file(output_location, upload_bucket_name, f'cmip6-{model}-{variable}-kerchunk/{output_fname}')\n# None is good.\nprint(f\"Response uploading {output_fname} to {upload_bucket_name} was {response}.\")\n\nResponse uploading combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json to veda-data-store-staging was None.\n\n\n2023-10-04 21:59:02,340 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client",
    "crumbs": [
      "Contributing",
      "Dataset Ingestion",
      "Generate Kerchunk Reference from CMIP6 NetCDF files"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Usage Examples",
    "section": "",
    "text": "The example notebooks are divided into three sections:\n\nQuickstarts: Notebooks to get you started quickly and help you become more familiar with cloud-native geospatial technologies.\nTutorials: Longer notebooks that walk through more advanced use cases and examples.\nDatasets: Notebooks that showcase a particular VEDA dataset and walk through an applied geospatial analyses.\n\n\n\nThe Quickstarts examples are further divided into two sections, which you can choose from depending on your data needs:\n\nAccessing the Data Directly: For when you want to access the raw data (e.g., to do a specific analysis). In this case, permissions are required to access the data (i.e., must be run on VEDA JupyterHub) and computation happens within the user’s instance (i.e., the user needs to think about instance size). This approach is suitable for use within notebooks. All examples provided in this section require VEDA JupyterHub access to run.\nUsing the Raster API: For when you want to show outputs to other people or do standard processing. No permissions required (i.e., notebooks can be run on mybinder). Additionally, the computation happens somehwere else (i.e., user does not have to think about instance size). Lastly, this approach is suitable for use within notebooks as well as web application frontends (e.g., like dataset discoveries). These notebook examples can be run on both VEDA JupyterHub, as well as outside of the Hub (see instructions below) and within mybinder.",
    "crumbs": [
      "Usage Examples"
    ]
  },
  {
    "objectID": "notebooks/index.html#getting-started",
    "href": "notebooks/index.html#getting-started",
    "title": "Usage Examples",
    "section": "",
    "text": "The example notebooks are divided into three sections:\n\nQuickstarts: Notebooks to get you started quickly and help you become more familiar with cloud-native geospatial technologies.\nTutorials: Longer notebooks that walk through more advanced use cases and examples.\nDatasets: Notebooks that showcase a particular VEDA dataset and walk through an applied geospatial analyses.\n\n\n\nThe Quickstarts examples are further divided into two sections, which you can choose from depending on your data needs:\n\nAccessing the Data Directly: For when you want to access the raw data (e.g., to do a specific analysis). In this case, permissions are required to access the data (i.e., must be run on VEDA JupyterHub) and computation happens within the user’s instance (i.e., the user needs to think about instance size). This approach is suitable for use within notebooks. All examples provided in this section require VEDA JupyterHub access to run.\nUsing the Raster API: For when you want to show outputs to other people or do standard processing. No permissions required (i.e., notebooks can be run on mybinder). Additionally, the computation happens somehwere else (i.e., user does not have to think about instance size). Lastly, this approach is suitable for use within notebooks as well as web application frontends (e.g., like dataset discoveries). These notebook examples can be run on both VEDA JupyterHub, as well as outside of the Hub (see instructions below) and within mybinder.",
    "crumbs": [
      "Usage Examples"
    ]
  },
  {
    "objectID": "notebooks/index.html#how-to-run",
    "href": "notebooks/index.html#how-to-run",
    "title": "Usage Examples",
    "section": "How to run",
    "text": "How to run\nEvery notebook contains information about how to run it. Some can run on mybinder and all can run on the VEDA JupyterHub. See VEDA Analytics JupyterHub Access for information about how to gain access.\n\nRunning outside of VEDA JupyterHub\nTo run the notebooks locally, you can use can install the Python packages (a virtual environment is recommended)\npip install -r requirements.txt\nOnce you have installed the packages you can run the notebooks using Jupyter.\njupyter lab\nIf the notebook needs access to protected data on S3, you will need to specifically get access. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN).",
    "crumbs": [
      "Usage Examples"
    ]
  },
  {
    "objectID": "notebooks/index.html#how-to-contribute",
    "href": "notebooks/index.html#how-to-contribute",
    "title": "Usage Examples",
    "section": "How to contribute",
    "text": "How to contribute\nPlease refer to the notebook style guide in these docs.",
    "crumbs": [
      "Usage Examples"
    ]
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html",
    "href": "notebooks/templates/template-accessing-the-data-directly.html",
    "title": "VEDA Documentation",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that access the data directly. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#run-this-notebook",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#run-this-notebook",
    "title": "VEDA Documentation",
    "section": "Run this notebook",
    "text": "Run this notebook\nYou can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\nInside the Hub\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\nOutside the Hub\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'\n\n\nFill in the text in italics in the following cells"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#approach",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#approach",
    "title": "VEDA Documentation",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests\nimport stackstac\n\nfrom pystac_client import Client"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "title": "VEDA Documentation",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "title": "VEDA Documentation",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. We use pystac-client to do a search. Here is an some example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "VEDA Documentation",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nbbox = []\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\n\nsearch = catalog.search(\n    bbox=bbox, datetime=datetime, collections=[collection_id], limit=1000\n)\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "title": "VEDA Documentation",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON from an authoritative online source for instance: https://gadm.org/download_country.html\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\n\nNext some notebooks read in the data. If you are using the raster API to trigger computation server side skip this section. Here is an example of reading the data in using stackstac and clipping using rasterio.\n\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "title": "VEDA Documentation",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataSet using stackstac\n\n# This is a workaround that is planning to move up into stackstac itself\nimport rasterio as rio\nimport boto3\n\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(AWS_NO_SIGN_REQUEST=True, session=rio.session.AWSSession(boto3.Session())\n)\n\n\nda = stackstac.stack(search.item_collection(), gdal_env=gdal_env)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)})\nda"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "title": "VEDA Documentation",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.rio.clip([aoi[\"geometry\"]])\nsubset\n\n\nWith the STAC object, and optionally the AOI and/or the data in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is an idea though :)"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "title": "VEDA Documentation",
    "section": "Select a band of data",
    "text": "Select a band of data\nThere is just one band in this case, cog_default.\n\ndata_band = da.sel(band=\"cog_default\")"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "title": "VEDA Documentation",
    "section": "Compute and plot",
    "text": "Compute and plot\nCalculate the mean at each time across the whole dataset. Note this is the first time that the data is actually loaded.\n\n# Average over entire AOI for each month\nmeans = data_band.mean(dim=(\"x\", \"y\")).compute()\n\n\nmeans.plot()"
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html",
    "href": "notebooks/datasets/volcano-so2-monitoring.html",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#approach",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#approach",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - SO2\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize tiles for each of the time steps of interest using folium",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "About the Data",
    "text": "About the Data\nCollecting measurements of Sulfur Dioxide (SO2) plumes from space is a valuable way to monitor changes in emissions. The SO2 index product is used by NASA to monitor volcanic clouds and pre-eruptive volcanic gas emissions activity. Additionally, this information is used in advisories to airlines for operational decisions.\nIn this notebook, we will explore the Sulfur Dioxide dataset and how it was used in this VEDA Discovery article to monitor air pollution across the globe.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Sulfur Dioxide\ncollection_name = \"OMSO2PCA-COG\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'OMSO2PCA-COG',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/OMSO2PCA-COG/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/OMSO2PCA-COG'}],\n 'title': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2005-01-01T00:00:00Z',\n     '2021-01-01T00:00:00Z']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2005-01-01T00:00:00Z', '2021-01-01T00:00:00Z'],\n  'cog_default': {'max': 28.743701934814453, 'min': -4.941379070281982}},\n 'description': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'year'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from 2005 to 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is yearly.\nWe can verify this by checking the total items returned from our STAC API requests.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 17 items\n\n\nThis makes sense as there are 17 years between 2005 - 2021.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API",
    "text": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API\nWe’ll explore three different time steps to show how NASA has observed volcanic activity in the Galápagos islands (2005), detected large scale emissions on the Kamchatka Peninsula (2009), and monitored the eruptions of Fagradalsfjall in Iceland (2021). We’ll then visualize the outputs on a map using folium.\nTo start, we’ll identify which item value corresponds to each year of interest and setting a rescaling_factor for the SO2 index, so that values range from 0 to 1.\n\n# to access the year value from each item more easily\nitems = {item[\"properties\"][\"start_datetime\"][:4]: item for item in items}\n\n\nrescaling_factor = \"0,1\"\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this three times, one for each time step of interest, so that we can visualize each event independently.\n\ntile_2005 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2005']['collection']}&item={items['2005']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2005\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2005&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2009 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2009']['collection']}&item={items['2009']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2009\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2009&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2021 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2021']['collection']}&item={items['2021']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2021\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2021&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWe will then use the tile URL prepared above to create a simple visualization for each time step using folium. In each of these visualizations you can zoom in and out of the map’s focus area to explore the data layer for that year.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Galápagos islands (2005)",
    "text": "Visualizing Galápagos islands (2005)\n\n# Set initial zoom and map for Galápagos islands\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -0.915435,\n        -89.57216,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2005[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Kamchatka Peninsula (2009)",
    "text": "Visualizing Kamchatka Peninsula (2009)\n\n# Set initial zoom and map for Kamchatka Peninsula\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        53.018234,\n        158.67016,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2009[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Fagradalsfjall, Iceland (2021)",
    "text": "Visualizing Fagradalsfjall, Iceland (2021)\n\n# Set initial zoom and map for Fagradalsfjall, Iceland\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        65.0294256,\n        -18.393870,\n    ],\n    zoom_start=6,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2021[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#summary",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#summary",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors sulfur dioxide emissions from space, by showcasing three different examples across the globe: volcanic activity in the Galápagos islands (2005), large scale emissions on the Kamchatka Peninsula (2009), and eruptions of Fagradalsfjall in Iceland (2021).",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Monitoring Volcanic Sulfur Dioxide Emissions"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html",
    "href": "notebooks/datasets/nceo-biomass-statistics.html",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#approach",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#approach",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Approach",
    "text": "Approach\n\nQuery STAC API and explore item contents for a given collection\nRead and access the data\nVisualize the collection with hvplot\nRun zonal statistics on collection using rasterstats\nVisualize resultant zonal statistics on a choropleth map",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "About the Data",
    "text": "About the Data\nThe NCEO Aboveground Woody Biomass 2017 dataset is a map for Africa at 100 m spatial resolution which was developed using a combination of LiDAR, Synthetic Aperture Radar (SAR) and optical based data. Aboveground woody biomass (AGB) plays an key role in the study of the Earth’s carbon cycle and response to climate change. Estimation based on Earth Observation measurements is an effective method for regional scale studies and the results are expressed as dry matter in Mg ha-1.\nImportant Note: Users of this dataset should keep in mind that the map is a continental-scale dataset, generated using a combination of different remote sensing data types, with a single method for the whole study area produced in 2017. Users, therefore, should understand that accuracy may vary for different regions and vegetation types.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "The Case Study - Guinea",
    "text": "The Case Study - Guinea\nMapping and understanding the spatial distribution of AGB is key to understanding carbon dioxide emissions from tropical deforestation through the loss of woody carbon stocks. The resulting carbon fluxes from these land-use changes and vegetation degradation can have negative impacts on the global carbon cycle. Change analysis between AGB maps overtime can display losses in high biomass forests, due to suspected deforestation and forest degredation.\nThe forests of southern Guinea are reported to have some of the highest density AGB of any forest in the world and are one of the most threatened ecoregions in Africa. Importantly, this area was also the epicenter of the 2014 Ebola outbreak, which had a lasting impact on the region. There is more and more evidence that human deforestation activities in this area may have accelerated the spread of the deadly virus as a result of increasing human-bat interactions in the region.\nIn this example we explore the NCEO AGB dataset for 2017, running zonal statistics at the district (administrative 2) level to understand those areas in Guinea that need greatest prioritization for protection and conservation.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Setting up the Environment",
    "text": "Setting up the Environment\nTo run zonal statistics we’ll need to import a python package called rasterstats into our environment. You can uncomment the following line for installation. This cell needs only needs to be run once.\n\n!pip install rasterstats\n\nRequirement already satisfied: rasterstats in /srv/conda/envs/notebook/lib/python3.10/site-packages (0.18.0)\nRequirement already satisfied: fiona&lt;1.9 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.8.22)\nRequirement already satisfied: shapely in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.8.5.post1)\nRequirement already satisfied: cligj&gt;=0.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (0.7.2)\nRequirement already satisfied: simplejson in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (3.19.1)\nRequirement already satisfied: affine&lt;3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (2.3.1)\nRequirement already satisfied: click&gt;7.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (8.1.3)\nRequirement already satisfied: numpy&gt;=1.9 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.23.5)\nRequirement already satisfied: rasterio&gt;=1.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.3.4)\nRequirement already satisfied: attrs&gt;=17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (22.2.0)\nRequirement already satisfied: six&gt;=1.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (1.16.0)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (65.6.3)\nRequirement already satisfied: certifi in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (2022.12.7)\nRequirement already satisfied: click-plugins&gt;=1.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (1.1.1)\nRequirement already satisfied: munch in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (2.5.0)\nRequirement already satisfied: snuggs&gt;=1.4.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterio&gt;=1.0-&gt;rasterstats) (1.4.7)\nRequirement already satisfied: pyparsing&gt;=2.1.6 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from snuggs&gt;=1.4.1-&gt;rasterio&gt;=1.0-&gt;rasterstats) (3.0.9)",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nfrom pystac_client import Client\n\n\n# Provide STAC API endpoint\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\n\n# Declare collection of interest - NCEO Biomass\ncollection = \"nceo_africa_2017\"\n\nNow let’s check how many total items are available.\n\nsearch = Client.open(STAC_API_URL).search(collections=[collection])\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\nFound 1 items\n\n\nThis makes sense as there is only one item available: a map for 2017.\n\n# Explore the \"cog_default\" asset of one item to see what it contains\nitems[0].assets[\"cog_default\"].to_dict()\n\n{'href': 's3://nasa-maap-data-store/file-staging/nasa-map/nceo-africa-2017/AGB_map_2017v0m_COG.tif',\n 'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n 'title': 'Default COG Layer',\n 'description': 'Cloud optimized default layer to display on map',\n 'raster:bands': [{'scale': 1.0,\n   'nodata': 'inf',\n   'offset': 0.0,\n   'sampling': 'area',\n   'data_type': 'uint16',\n   'histogram': {'max': 429.0,\n    'min': 0.0,\n    'count': 11.0,\n    'buckets': [405348.0,\n     44948.0,\n     18365.0,\n     6377.0,\n     3675.0,\n     3388.0,\n     3785.0,\n     9453.0,\n     13108.0,\n     1186.0]},\n   'statistics': {'mean': 37.58407913145342,\n    'stddev': 81.36678677343947,\n    'maximum': 429.0,\n    'minimum': 0.0,\n    'valid_percent': 50.42436439336373}}],\n 'roles': ['data', 'layer']}\n\n\nExplore through the item’s assets. We can see from the data’s statistics values that the min and max values for the observed values range from 0 to 429 Mg ha-1.\nExploring the properties [\"proj:epsg\"] we also notice something strange, as the value is float and should be integer. We’ll revise this using the code below, but it will be revised upstream in the future.\n\nitems[0].properties[\"proj:epsg\"] = int(items[0].properties[\"proj:epsg\"])",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Reading and accessing the data",
    "text": "Reading and accessing the data\nNow that we’ve explored the dataset through the STAC API, let’s read and access the dataset itself.\n\n# This is a workaround that is planning to move up into stackstac itself\n\nimport boto3\nimport stackstac\nimport rasterio as rio\nimport rioxarray\n\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(\n        AWS_NO_SIGN_REQUEST=True, session=rio.session.AWSSession(boto3.Session())\n    )\n)\n\n\nda = stackstac.stack(items[0], gdal_env=gdal_env)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                band: 1,\n                                                                y: 81025,\n                                                                x: 78078)&gt;\ndask.array&lt;fetch_raster_window, shape=(1, 1, 81025, 78078), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -18.27 -18.27 -18.27 ... 51.86 51.86 51.86\n  * y               (y) float64 37.73 37.73 37.73 37.73 ... -35.05 -35.05 -35.05\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-18.27352950...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    crs:         epsg:4326\n    transform:   | 0.00, 0.00,-18.27|\\n| 0.00,-0.00, 37.73|\\n| 0.00, 0.00, 1.00|\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1band: 1y: 81025x: 78078dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n47.13 GiB\n8.00 MiB\n\n\nShape\n(1, 1, 81025, 78078)\n(1, 1, 1024, 1024)\n\n\nDask graph\n6160 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         1 1                                                                                                                                                      78078 81025 1\n\n\n\n\nCoordinates: (16)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-18.27 -18.27 ... 51.86 51.86array([-18.274428, -18.27353 , -18.272631, ...,  51.861538,  51.862436,\n        51.863335])y(y)float6437.73 37.73 37.73 ... -35.05 -35.05array([ 37.731937,  37.731039,  37.73014 , ..., -35.051364, -35.052262,\n       -35.053161])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{37.73103856358817, 51.864232928...array({37.73103856358817, 51.86423292864056, -35.054059016911935, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([-18.274427824843425, -18.273529509559307, -18.272631194275185,\n              -18.271732878991067,  -18.27083456370695, -18.269936248422827,\n               -18.26903793313871, -18.268139617854587,  -18.26724130257047,\n               -18.26634298728635,\n              ...\n               51.855249775799365,   51.85614809108348,    51.8570464063676,\n                51.85794472165172,   51.85884303693584,  51.859741352219956,\n               51.860639667504074,   51.86153798278819,  51.862436298072325,\n                51.86333461335644],\n             dtype='float64', name='x', length=78078))yPandasIndexPandasIndex(Float64Index([  37.73193687887226,   37.73103856358814,  37.730140248304025,\n                 37.7292419330199,   37.72834361773578,   37.72744530245166,\n               37.726546987167545,   37.72564867188343,   37.72475035659931,\n                37.72385204131518,\n              ...\n               -35.04507586407077, -35.045974179354886, -35.046872494639004,\n               -35.04777080992312,  -35.04866912520724,  -35.04956744049136,\n               -35.05046575577549,  -35.05136407105961,  -35.05226238634373,\n              -35.053160701627846],\n             dtype='float64', name='y', length=81025))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))crs :epsg:4326transform :| 0.00, 0.00,-18.27|\n| 0.00,-0.00, 37.73|\n| 0.00, 0.00, 1.00|resolution :0.0008983152841195214\n\n\n\n# Create an AOI for our study area\n\n# Guinea\nguinea_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                [-15.519958756713947, 12.732440363049193],\n                [-15.519958756713947, 6.771426493209475],\n                [-7.078554695621165, 6.771426493209475],\n                [-7.078554695621165, 12.732440363049193],\n                [-15.519958756713947, 12.732440363049193],\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# Subset to bounding box of Guinea\nsubset = da.rio.clip([guinea_aoi[\"geometry\"]])\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                band: 1,\n                                                                y: 6636, x: 9397)&gt;\ndask.array&lt;getitem, shape=(1, 1, 6636, 9397), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -15.52 -15.52 -15.52 ... -7.081 -7.08 -7.079\n  * y               (y) float64 12.73 12.73 12.73 12.73 ... 6.773 6.772 6.772\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1band: 1y: 6636x: 9397dask.array&lt;chunksize=(1, 1, 842, 5), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n475.76 MiB\n8.00 MiB\n\n\nShape\n(1, 1, 6636, 9397)\n(1, 1, 1024, 1024)\n\n\nDask graph\n77 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         1 1                                                          9397 6636 1\n\n\n\n\nCoordinates: (17)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-15.52 -15.52 ... -7.08 -7.079axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-15.519295, -15.518397, -15.517498, ...,  -7.080521,  -7.079623,\n        -7.078724])y(y)float6412.73 12.73 12.73 ... 6.772 6.772axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([12.731823, 12.730924, 12.730026, ...,  6.773297,  6.772399,  6.771501])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{-35.054059016911935, 51.8642329...array({-35.054059016911935, 51.86423292864056, 37.73103856358817, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-15.519744006090912 0.0008983152841195214 0.0 12.732271679468038 0.0 -0.0008983152841195215array(0)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([-15.519294848448853, -15.518396533164733, -15.517498217880615,\n              -15.516599902596495, -15.515701587312375, -15.514803272028256,\n              -15.513904956744136, -15.513006641460017, -15.512108326175897,\n              -15.511210010891777,\n              ...\n               -7.086809276418906,  -7.085910961134786,  -7.085012645850668,\n               -7.084114330566548,  -7.083216015282428,  -7.082317699998308,\n                -7.08141938471419,   -7.08052106943007,   -7.07962275414595,\n                -7.07872443886183],\n             dtype='float64', name='x', length=9397))yPandasIndexPandasIndex(Float64Index([12.731822521825979,  12.73092420654186, 12.730025891257743,\n               12.72912757597362, 12.728229260689503, 12.727330945405381,\n              12.726432630121263, 12.725534314837144, 12.724635999553023,\n              12.723737684268905,\n              ...\n               6.779585449250032,   6.77868713396591,  6.777788818681792,\n               6.776890503397674,  6.775992188113552,  6.775093872829434,\n               6.774195557545315,  6.773297242261194, 6.7723989269770755,\n               6.771500611692954],\n             dtype='float64', name='y', length=6636))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))resolution :0.0008983152841195214\n\n\n\n# select the band of interest, as there is only one in this dataset we'll select the default\ndata_band = subset.sel(band=\"cog_default\")\ndata_band\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                y: 6636, x: 9397)&gt;\ndask.array&lt;getitem, shape=(1, 6636, 9397), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -15.52 -15.52 -15.52 ... -7.081 -7.08 -7.079\n  * y               (y) float64 12.73 12.73 12.73 12.73 ... 6.773 6.772 6.772\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1y: 6636x: 9397dask.array&lt;chunksize=(1, 842, 5), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n475.76 MiB\n8.00 MiB\n\n\nShape\n(1, 6636, 9397)\n(1, 1024, 1024)\n\n\nDask graph\n77 chunks in 8 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                         9397 6636 1\n\n\n\n\nCoordinates: (17)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-15.52 -15.52 ... -7.08 -7.079axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-15.519295, -15.518397, -15.517498, ...,  -7.080521,  -7.079623,\n        -7.078724])y(y)float6412.73 12.73 12.73 ... 6.772 6.772axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([12.731823, 12.730924, 12.730026, ...,  6.773297,  6.772399,  6.771501])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{-35.054059016911935, 51.8642329...array({-35.054059016911935, 51.86423292864056, 37.73103856358817, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-15.519744006090912 0.0008983152841195214 0.0 12.732271679468038 0.0 -0.0008983152841195215array(0)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))xPandasIndexPandasIndex(Float64Index([-15.519294848448853, -15.518396533164733, -15.517498217880615,\n              -15.516599902596495, -15.515701587312375, -15.514803272028256,\n              -15.513904956744136, -15.513006641460017, -15.512108326175897,\n              -15.511210010891777,\n              ...\n               -7.086809276418906,  -7.085910961134786,  -7.085012645850668,\n               -7.084114330566548,  -7.083216015282428,  -7.082317699998308,\n                -7.08141938471419,   -7.08052106943007,   -7.07962275414595,\n                -7.07872443886183],\n             dtype='float64', name='x', length=9397))yPandasIndexPandasIndex(Float64Index([12.731822521825979,  12.73092420654186, 12.730025891257743,\n               12.72912757597362, 12.728229260689503, 12.727330945405381,\n              12.726432630121263, 12.725534314837144, 12.724635999553023,\n              12.723737684268905,\n              ...\n               6.779585449250032,   6.77868713396591,  6.777788818681792,\n               6.776890503397674,  6.775992188113552,  6.775093872829434,\n               6.774195557545315,  6.773297242261194, 6.7723989269770755,\n               6.771500611692954],\n             dtype='float64', name='y', length=6636))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))resolution :0.0008983152841195214",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-nceo-biomass-2017-layer-for-our-study-area-in-guinea",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-nceo-biomass-2017-layer-for-our-study-area-in-guinea",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Visualizing the NCEO Biomass 2017 layer for our study area in Guinea",
    "text": "Visualizing the NCEO Biomass 2017 layer for our study area in Guinea\nNow that we’ve got the NCEO data layer subsetted for Guinea, let’s visualize it using hvplot.\n\nimport hvplot.xarray\n\nbiomass = data_band.squeeze()\nbiomass\n\nbiomass.hvplot(\n    x=\"x\",\n    y=\"y\",\n    coastline=True,\n    rasterize=True,\n    cmap=\"viridis\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Zonal Statistics",
    "text": "Zonal Statistics\nThis map we created above is great, but let’s focus on which districts (administrative level 2 boundaries) should be prioritized for forest conservation.\nZonal statistics is an operation that calculates statistics on the cell values of a raster layer (e.g., the NCEO AGB dataset) within the zones (i.e., polygons) of another dataset. It is an analytical tool that can calculate the mean, median, sum, minimum, maximum, or range in each zone. The zonal extent, often polygons, can be in the form of objects like administrative boundaries, water catchment areas, or field boundaries.\nIn this example, we’ll explore the data contained in the NCEO AGB collection and analyze it for each of the districts in Guinea. To do this we will need to import district (administrative level 2) boundary layers from below. We will use the Humanitarian Data Exchange (HDX) site to retrieve subnational administrative boundaries for Guinea. Specifically, we will use the geoBoundaries-GIN-ADM2_simplified.geojson which can be accessed here and read them in directly using geopandas.\n\nimport geopandas as gpd\n\nadmin2_gdf = gpd.read_file(\n    \"https://raw.githubusercontent.com/wmgeolab/geoBoundaries/0f0b6f5fb638e7faf115f876da4e77d8f7fa319f/releaseData/gbOpen/GIN/ADM2/geoBoundaries-GIN-ADM2_simplified.geojson\"\n)\n\n\n# check the CRS\nprint(admin2_gdf.crs)\n\nepsg:4326\n\n\n\nimport pandas as pd\nfrom rasterstats import zonal_stats\n\n\nadmin2_biomass = pd.DataFrame(\n    zonal_stats(\n        admin2_gdf,\n        biomass.values,\n        affine=biomass.rio.transform(),\n        nodata=biomass.rio.nodata,\n        band=1\n        # geojson_out=True\n    ),\n    index=admin2_gdf.index,\n)\nadmin2_biomass\n\n/srv/conda/envs/notebook/lib/python3.10/site-packages/rasterstats/io.py:335: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n0.0\n546.0\n46.846892\n527269\n\n\n2\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n0.0\n345.0\n31.315987\n41660\n\n\n4\n0.0\n494.0\n47.809879\n116515\n\n\n5\n0.0\n422.0\n57.583787\n530195\n\n\n6\n0.0\n548.0\n48.555337\n317191\n\n\n7\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n0.0\n448.0\n44.744092\n403399\n\n\n9\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n0.0\n422.0\n46.973538\n426420\n\n\n11\n0.0\n452.0\n50.016374\n161593\n\n\n12\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n0.0\n563.0\n89.595375\n429232\n\n\n14\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n0.0\n558.0\n65.559085\n955743\n\n\n16\n0.0\n486.0\n48.230132\n897380\n\n\n17\n0.0\n590.0\n90.110284\n630818\n\n\n18\n0.0\n413.0\n47.668705\n364092\n\n\n19\n0.0\n339.0\n37.304653\n544541\n\n\n20\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n0.0\n470.0\n46.776737\n216368\n\n\n22\n0.0\n469.0\n57.435206\n278955\n\n\n23\n0.0\n592.0\n71.918267\n461576\n\n\n24\n0.0\n623.0\n123.937151\n814877\n\n\n25\n0.0\n451.0\n45.058698\n859223\n\n\n26\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n0.0\n592.0\n86.547049\n413435\n\n\n29\n0.0\n560.0\n57.591189\n460766\n\n\n30\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n0.0\n554.0\n57.991285\n771431\n\n\n32\n0.0\n389.0\n49.663329\n615108\n\n\n33\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nNow we’ll join the administrative level 2 boundaries to the zonal statistics results, so that we can map the districts on a choropleth map.\n\nconcat_df = admin2_gdf.join(admin2_biomass)\nconcat_df\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n0\n1\nGN-BE\nBeyla\nADM2\nGIN-ADM2-49546643B63767081\nGIN\nADM2\nPOLYGON ((-8.24559 8.44255, -8.24158 8.45044, ...\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n2\nGN-BF\nBoffa\nADM2\nGIN-ADM2-49546643B69790359\nGIN\nADM2\nMULTIPOLYGON (((-13.77147 9.84445, -13.76994 9...\n0.0\n546.0\n46.846892\n527269\n\n\n2\n3\nGN-BK\nBoke\nADM2\nGIN-ADM2-49546643B67680147\nGIN\nADM2\nMULTIPOLYGON (((-14.57512 10.76872, -14.57633 ...\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n4\nGN-C\nConakry\nADM2\nGIN-ADM2-49546643B26553537\nGIN\nADM2\nMULTIPOLYGON (((-13.78686 9.46592, -13.79013 9...\n0.0\n345.0\n31.315987\n41660\n\n\n4\n5\nGN-CO\nCoyah\nADM2\nGIN-ADM2-49546643B29309121\nGIN\nADM2\nPOLYGON ((-13.49399 9.53945, -13.48050 9.55304...\n0.0\n494.0\n47.809879\n116515\n\n\n5\n6\nGN-DB\nDabola\nADM2\nGIN-ADM2-49546643B70320134\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46752 10.545...\n0.0\n422.0\n57.583787\n530195\n\n\n6\n7\nGN-DL\nDalaba\nADM2\nGIN-ADM2-49546643B47404564\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -12.03171 11.288...\n0.0\n548.0\n48.555337\n317191\n\n\n7\n8\nGN-DI\nDinguiraye\nADM2\nGIN-ADM2-49546643B47728803\nGIN\nADM2\nPOLYGON ((-10.72063 11.13326, -10.72092 11.144...\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n9\nGN-DU\nDubreka\nADM2\nGIN-ADM2-49546643B78750611\nGIN\nADM2\nMULTIPOLYGON (((-13.76504 9.82404, -13.75194 9...\n0.0\n448.0\n44.744092\n403399\n\n\n9\n10\nGN-FA\nFaranah\nADM2\nGIN-ADM2-49546643B99428691\nGIN\nADM2\nPOLYGON ((-11.38731 10.39356, -11.38273 10.350...\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n11\nGN-FO\nForecariah\nADM2\nGIN-ADM2-49546643B32851960\nGIN\nADM2\nMULTIPOLYGON (((-13.32015 9.14776, -13.32062 9...\n0.0\n422.0\n46.973538\n426420\n\n\n11\n12\nGN-FR\nFria\nADM2\nGIN-ADM2-49546643B75641357\nGIN\nADM2\nPOLYGON ((-13.76799 10.27884, -13.73119 10.276...\n0.0\n452.0\n50.016374\n161593\n\n\n12\n13\nGN-GA\nGaoual\nADM2\nGIN-ADM2-49546643B44796554\nGIN\nADM2\nPOLYGON ((-13.84293 11.29667, -13.83242 11.291...\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n14\n15\nGN-KA\nKankan\nADM2\nGIN-ADM2-49546643B19447005\nGIN\nADM2\nPOLYGON ((-8.14727 9.58395, -8.15293 9.58911, ...\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n16\nGN-KE\nKerouane\nADM2\nGIN-ADM2-49546643B28981869\nGIN\nADM2\nPOLYGON ((-8.61661 9.50260, -8.60868 9.51354, ...\n0.0\n558.0\n65.559085\n955743\n\n\n16\n17\nGN-KD\nKindia\nADM2\nGIN-ADM2-49546643B38105311\nGIN\nADM2\nPOLYGON ((-13.11475 9.58669, -13.10890 9.58190...\n0.0\n486.0\n48.230132\n897380\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n18\n19\nGN-KB\nKoubia\nADM2\nGIN-ADM2-49546643B329053\nGIN\nADM2\nPOLYGON ((-11.30453 12.01713, -11.31240 12.021...\n0.0\n413.0\n47.668705\n364092\n\n\n19\n20\nGN-KN\nKoundara\nADM2\nGIN-ADM2-49546643B74925550\nGIN\nADM2\nPOLYGON ((-12.82676 12.14425, -12.76880 12.221...\n0.0\n339.0\n37.304653\n544541\n\n\n20\n21\nGN-KO\nKouroussa\nADM2\nGIN-ADM2-49546643B81289084\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46733 10.531...\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n22\nGN-LA\nLabe\nADM2\nGIN-ADM2-49546643B47788034\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -11.98685 11.320...\n0.0\n470.0\n46.776737\n216368\n\n\n22\n23\nGN-LE\nLelouma\nADM2\nGIN-ADM2-49546643B80531036\nGIN\nADM2\nPOLYGON ((-12.99636 11.18952, -12.98648 11.187...\n0.0\n469.0\n57.435206\n278955\n\n\n23\n24\nGN-LO\nLola\nADM2\nGIN-ADM2-49546643B51651521\nGIN\nADM2\nPOLYGON ((-8.46455 8.27185, -8.44429 8.25379, ...\n0.0\n592.0\n71.918267\n461576\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n25\n26\nGN-ML\nMali\nADM2\nGIN-ADM2-49546643B68291102\nGIN\nADM2\nPOLYGON ((-12.76304 11.85482, -12.74823 11.857...\n0.0\n451.0\n45.058698\n859223\n\n\n26\n27\nGN-MM\nMamou\nADM2\nGIN-ADM2-49546643B49157402\nGIN\nADM2\nPOLYGON ((-11.15547 11.05524, -11.13717 11.074...\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n28\nGN-MD\nMandiana\nADM2\nGIN-ADM2-49546643B49348937\nGIN\nADM2\nPOLYGON ((-8.13614 10.00000, -8.13498 10.00774...\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435\n\n\n29\n30\nGN-PI\nPita\nADM2\nGIN-ADM2-49546643B22597757\nGIN\nADM2\nPOLYGON ((-12.20899 11.16225, -12.21822 11.152...\n0.0\n560.0\n57.591189\n460766\n\n\n30\n31\nGN-SI\nSiguiri\nADM2\nGIN-ADM2-49546643B98837050\nGIN\nADM2\nPOLYGON ((-10.00475 11.40696, -10.00285 11.401...\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n32\nGN-TE\nTelimele\nADM2\nGIN-ADM2-49546643B10795278\nGIN\nADM2\nPOLYGON ((-13.65247 10.66825, -13.59967 10.711...\n0.0\n554.0\n57.991285\n771431\n\n\n32\n33\nGN-TO\nTougue\nADM2\nGIN-ADM2-49546643B67909893\nGIN\nADM2\nPOLYGON ((-11.74293 10.98745, -11.70851 11.019...\n0.0\n389.0\n49.663329\n615108\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nBy sorting the results, we can identify those top districts with the highest mean AGB.\n\nconcat_df_sorted = concat_df.sort_values(by=\"mean\", ascending=False)\nconcat_df_sorted.head()\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Visualizing the results with a choropleth map",
    "text": "Visualizing the results with a choropleth map\nNow, let’s visualize the results!\n\nimport hvplot.pandas\n\n# renaming the shapeName to District for improved legend\nconcat_df.rename(columns={\"shapeName\": \"District\"}, inplace=True)\n\n\nagb = concat_df.hvplot(\n    c=\"mean\",\n    width=900,\n    height=500,\n    geo=True,\n    hover_cols=[\"mean\", \"District\"],\n    cmap=\"viridis\",\n    hover_fill_color=\"white\",\n    line_width=1,\n    title=\"Mean Aboveground Woody Biomass per Guinean District (Mg ha-1)\",\n    tiles=\"CartoLight\",\n)\n\nagb\n\n/srv/conda/envs/notebook/lib/python3.10/site-packages/geoviews/operation/projection.py:79: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  polys = [g for g in geom if g.area &gt; 1e-15]\n\n\n\n\n\n\n  \n\n\n\n\nBy hovering over the map, we can identify the names and mean AGB per district.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#summary",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#summary",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully performed zonal statistics on the NCEO AGB dataset in Guinea and displayed the results on a choropleth map. The results of this analysis can dispaly those districts which contain the greatest average amount of AGB and should be prioritized for forest protection efforts.",
    "crumbs": [
      "Usage Examples",
      "Datasets",
      "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas"
    ]
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "VEDA promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with VEDA can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.openveda.cloud. Please find instructions for requesting access below.\nIf you are a scientist affiliated with NASA projects such as EIS and MAAP, you can also keep using the resources provided by these projects.\nThrough the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#getting-access-to-vedas-jupyterhub-environment",
    "href": "services/jupyterhub.html#getting-access-to-vedas-jupyterhub-environment",
    "title": "JupyterHub",
    "section": "Getting access to VEDA’s JupyterHub environment",
    "text": "Getting access to VEDA’s JupyterHub environment\nAccess to the VEDA notebook environment is currently on an as-need basis. If you are a user afficiliated with VEDA, you can gain access by following these steps:\n\nMake sure you have a Github Account. Take note of your Github username\nSend an email to the VEDA team (veda@uah.edu) asking for access to the VEDA notebook environment. Please include your Github username. They will invite you through Github to join the VEDA Analytics Github Team. Please watch your email for the invite.\nOnce you accepted the invitation, you should be able to go to https://hub.openveda.cloud/ and login via your Github credentials.",
    "crumbs": [
      "Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/jupyterhub.html#instructory-notebooks",
    "href": "services/jupyterhub.html#instructory-notebooks",
    "title": "JupyterHub",
    "section": "Instructory notebooks",
    "text": "Instructory notebooks\nThis documentation site provides Jupyter notebooks on how to load and analyze Earth data an interactive cloud computing environment.",
    "crumbs": [
      "Services",
      "JupyterHub"
    ]
  },
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Most of the VEDA APIs are hosted out of a single project (veda-backend) that combines multiple standalone services.",
    "crumbs": [
      "Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#environments",
    "href": "services/apis.html#environments",
    "title": "APIs",
    "section": "Environments",
    "text": "Environments\nWhile some of our services are already very mature, VEDA is currently in the build-up phase. Therefore, we do not yet have a production environment for users. Maintenance on the staging environment will be announced internally and selected known stakeholders will be informed of any larger changes.\n\nProduction (stable):\n\nSTAC browser: veda-stac-browser\nSTAC API (metadata): https://openveda.cloud/api/stac/docs\nList collections: https://openveda.cloud/api/stac/collections\nRaster API (tiling): https://openveda.cloud/api/raster/docs\n\n\n\nStaging (maintenance will be announced):\n\nSTAC browser: veda-staging-stac-browser\nSTAC API (metadata): staging-stac.delta-backend.com/docs\nList collections: staging-stac.delta-backend.com/collections\nRaster API (map tiles and timeseries): staging-raster.delta-backend.com/docs\nFeatures API (vector data): firenrt.delta-backend.com - see also the usage tutorial\nSTAC viewer (experimental): staging-stac.delta-backend.com",
    "crumbs": [
      "Services",
      "APIs"
    ]
  },
  {
    "objectID": "services/apis.html#using-tile-layers-in-external-services",
    "href": "services/apis.html#using-tile-layers-in-external-services",
    "title": "APIs",
    "section": "Using tile layers in external services",
    "text": "Using tile layers in external services\n\nUI for single tile layers\nAs you can see from our API docs referenced above, our raster API provides WMTS and XYZ tiles for public consumption.\nFor any layer you are seeing in the VEDA dataset Explorer, you can retrieve the tile URL:\n\n\n\nVEDA Dashboard Exploration API grab\n\n\nAnd paste that into any client that loads these tiles, like QGIS, ArcGIS, Leaflet, even online tools such as geojson.io or felt.com.\n\n\nSTAC for layer timeseries\nIf you want to integrate tile layer time series into your application, you will need to fetch the information about which time steps exist and what the layer URLs are from our Spatio Tempoeral Asset Catalog (STAC) API (see above).\nThat is because, unfortunately, neither XYZ nor WMTS have time series capabilities (unlike good old WMS, which our services do not provide, though).\nYou can see how to retrieve time steps and tile layer URLs from these tutorial Python notebooks (mostly REST API calls):\n\nUsing /stac/tilejson.json with STAC collection and item IDs\nCreating layers from filters and mosaics (advanced)\n\nIt comes down to querying for STAC items (timesteps) and then asking the Raster API for tilejson.json specifications for the items you are interested in.\nOnce you retrieved the WMTS or XYZ layer URLs this way, you can use them seamlessly with all mapping clients.",
    "crumbs": [
      "Services",
      "APIs"
    ]
  }
]