[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VEDA User Documentation",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is an innovative platform empowering researchers to explore and analyze Earth science data in the cloud. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.\nThese pages provide documentation for onboarding users to cloud-enabled open science with Earth data, from computing and API usage to publication on the VEDA Dashboard."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "VEDA User Documentation",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is an innovative platform empowering researchers to explore and analyze Earth science data in the cloud. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.\nThese pages provide documentation for onboarding users to cloud-enabled open science with Earth data, from computing and API usage to publication on the VEDA Dashboard."
  },
  {
    "objectID": "index.html#data-services",
    "href": "index.html#data-services",
    "title": "VEDA User Documentation",
    "section": "Data Services",
    "text": "Data Services\nIn VEDA’s open-source science environment, datasets can be discovered and accessed via open-standard data services such as a Spatio Temporal Asset Catalog (STAC) and WMTS map tiles APIs. Here is an overview of the API endpoints."
  },
  {
    "objectID": "index.html#resources-for-open-source-data-science",
    "href": "index.html#resources-for-open-source-data-science",
    "title": "VEDA User Documentation",
    "section": "Resources for Open-Source Data Science",
    "text": "Resources for Open-Source Data Science\nIf you are just getting started with geospatial data science (in Python) or want to learn more, you may find the Collection of External Resources useful.\nTo learn from examples, see the VEDA Example Notebooks - examples of open-source data science using VEDA services.\nSelected user groups can also get access to a VEDA-provided JupyterHub service."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "VEDA User Documentation",
    "section": "Contributing",
    "text": "Contributing\nContributions of data, stories, and example code are open to all users affiliated with the project. We strive to make the process as easy as possible. Please see the docs on Contributing."
  },
  {
    "objectID": "contributing/index.html",
    "href": "contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Please see the sections below for documentation on\n\nDataset ingestion into the VEDA data store and STAC\nConfiguration of Dataset information pages and Discoveries on the VEDA Dashboard\nExample notebooks that illustrate the use of datasets or compute methods"
  },
  {
    "objectID": "contributing/docs-and-notebooks.html",
    "href": "contributing/docs-and-notebooks.html",
    "title": "Example Notebook Submission",
    "section": "",
    "text": "Contribution to VEDA’s documentation is always welcome - just open a Pull Request on the veda-docs repository.\nPlease note that this documentation site is rendered using Quarto, which adds a small set of configuration options on top of vanilla Markdown and Jupyter Notebooks."
  },
  {
    "objectID": "contributing/docs-and-notebooks.html#notebook-author-guidelines",
    "href": "contributing/docs-and-notebooks.html#notebook-author-guidelines",
    "title": "Example Notebook Submission",
    "section": "Notebook Author Guidelines",
    "text": "Notebook Author Guidelines\nThere are two template notebooks in this directory titled: template-using-the-raster-api.ipynb and template-accessing-the-data-directly.ipynb that you can use as a starting place. Alternatively you can pull specific cells from that notebook into your own.\n\nStyle\n\nEach code cell should come after a markdown cell with some explanatory text. This is preferred over comments in the code cells.\nThe max header should be ##.\nOnly include imports that are needed for the notebook to run.\nWe don’t enforce any formatting, but periodically run black on all the notebooks. If you would like to run black yourself do pip install black[jupyter] and then black.\n\n\n\nRendering information\nThe first cell in every notebook is a raw cell that contains the following metadata for rendering with our site builder Quarto.\n---\ntitle: Short title\ndescription: One sentence description\nauthor: Author Name\ndate: May 2, 2023\nexecute:\n  freeze: true\n---\nWe store evaluted notebooks in this repository. So before you commit your notebook, you should restart your kernel and run all cells in order.\n\n\nStandard sections\nTo give the notebooks a standard look and feel we typically include the following sections:\n\nRun this Notebook: The section explains how to run the notebook locally, on VEDA JupyterHub or on mybinder. There are several examples of what this section can look like in the template notebooks.\nApproach: List a few steps that outline the approach you be taking in this notebook.\nAbout the data: Optional description of the datatset\nDeclare your collection of interest: This section reiterates how you can discover which collections are available. You can copy the example of this section from one of the template notebooks.\n\nFrom then on the standard sections diverge depending on whether the notebook access the data directly or uses the raster API. Check the template notebooks for some ideas of common patterns.\n\n\nUsing complex geometries\nIf you are defining the AOI using a bounding box, you can include it in the text of the notebook, but for more complex geometries we prefer that the notebook access the geometry directly from a canonical source. You can check the template notebooks for exmples of this. If the complex geometry is not available online the VEDA team can help get it up in a public s3 bucket.\n\n\nRecommended librarires\n\nMapping + Visualization\n\nfolium: folium adds Leaflet.js support to python projects for visualizing data in a map.\nholoviz: High-level tools that make it easier to apply Python plotting libraries to your data.\nipyleaflet: Interactive maps in the Jupyter notebook. ipyleaflet is built on ipywidgets allowing for bidirectional communication between front- and backends (learn more: Interactive GIS in Jupyter with ipyleaflet).\n\n\n\nUsing STAC for cataloging data\nTo present consistent best practices, we always access data via the STAC API. Often we use stackstac for this.\n\npystac: PySTAC is a library for creating SpatioTemporal Asset Catalogs (STAC) in Python 3.\npystac-client: A Python client for working with STAC Catalogs and APIs.\n\n\n\nAnalyzing data\n\nrioxarray: rasterio xarray extension\nstackstac: stackstac.stack turns a STAC collection into a lazy xarray.DataArray, backed by dask.\n\n\n\n\nGenerate “Launch in VEDA JupyterHub” link\nWe use nbgitpuller links to open the VEDA JupyterHub with a particular notebook pulled in. These links have the form: https://nasa-veda.2i2c.cloud/hub/user-redirect/git-pull?repo=https://github.com/NASA-IMPACT/veda-docs&urlpath=lab/tree/veda-docs/notebooks/quickstarts/open-and-plot.ipynb&branch=main\nIf you are writing a notebook and want to share it with others you can generate your own nbgitpuller link using this link generator."
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html",
    "href": "contributing/dashboard-configuration/index.html",
    "title": "Dashboard Configuration",
    "section": "",
    "text": "This guide explains how to publish content in the VEDA Dashboard, the graphical user interface for exploring NASA Earth Data datasets and science stories VEDA UI.\nBy following this document, you should have a good understanding of how to start from having an idea for some content to show on the VEDA Dashboard all the way to having your data and content appear in the production version of the VEDA Dashboard. Detailed technical documentation for each of the steps is available on GitHub and other places, links provided in the Appendix below.\nflowchart LR\n    A(Data & Content Prep) --&gt; B{Is the data already in VEDA?}\n    B --&gt;|No| C[Go to Dataset Ingestion]\n    C --&gt; E\n    B --&gt;|Yes| E{Do you have a story?}\n    E --&gt;|Yes| D[Go to Discovery Configuration]\n    E --&gt;|No| F[Go to Dataset Configuration]\n    click C \"../dataset-ingestion/index.html\" \"Docs on Dataset Ingestion\" _blank\n    click D \"./discovery-configuration.html\" \"Docs on Discovery Configuration\" _blank\n    click F \"./dataset-configuration.html\" \"Docs on Dataset Configuration\" _blank"
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html#data-content-preparation",
    "href": "contributing/dashboard-configuration/index.html#data-content-preparation",
    "title": "Dashboard Configuration",
    "section": "Data & Content Preparation",
    "text": "Data & Content Preparation\nThis is an important step before ingesting or configuring anything within VEDA. This will set you up for success in later steps.\n\nKey Steps\n🧑‍💻 Collaborate with partners familiar with the data context, to draft the necessary content.\nFor Discoveries, the required content is:\n\nText for the actual story itself\nAny visuals you would like to include, whether that be images, charts, maps, or other\n\nIf maps, identify which dataset and layer you would like to show and whether that is included in VEDA. (⚠️ If the dataset is not yet included in VEDA you’ll have to provide information about it and configure it as explained below).\nIf charts, gather the relevant data to build the chart. A csv file is the most common, but json is also supported\n\nA cover image for the dataset as it will appear in the Dashboard\nA title and short description/sub-title (5-10 words) for the Discovery\n\nNext step: Discovery Configuration.\nFor Datasets, the required content is:\n\nA descriptive overview of the dataset, how it came to exist, who maintains it, and how it should be used\nShort descriptions for each layer that you will want to reveal within VEDA (an example of this would be “CO2 mean vs CO2 difference”) for users to explore on a map\nA cover image for the dataset as it will appear in the Dashboard\nAny other relevant metadata you might want included https://nasa-impact.github.io/veda-docs/contributing/dashboard-content.html\nFor any datasets that need to be ingested, convert data to Cloud-Optimized GeoTIFFs (COGs) (⚠️ This is currently the only format supported in the VEDA Dashboard. More formats to come in the future)\n\nNext step: If your data is already in VEDA go to Dataset Configuration. Otherwise go to Dataset Ingestion."
  },
  {
    "objectID": "contributing/dashboard-configuration/index.html#useful-links",
    "href": "contributing/dashboard-configuration/index.html#useful-links",
    "title": "Dashboard Configuration",
    "section": "Useful Links",
    "text": "Useful Links\n\nContent repository for the VEDA Dashboard - veda-config\nData processing from EIS\nAlexey’s notes on helpful tips"
  },
  {
    "objectID": "contributing/dashboard-configuration/dataset-configuration.html",
    "href": "contributing/dashboard-configuration/dataset-configuration.html",
    "title": "Dataset Configuration",
    "section": "",
    "text": "Once you have ingested a dataset into the VEDA backend (following the steps in the Dataset Ingestion docs), you will need to configure the Dashboard.\nPlease note that the VEDA Dashboard relies on its own set of metadata about datasets. No information from STAC is loaded initially, so all the Dashboard needs to list your datasets needs to be configured in the following steps, which may require copying some of the information from the catalog metadata records, such as title, description, and dataset providers.\nTo configure your dataset, you can use the experimental VEDA Configuration UI, more closely described in its documentation on Github.\nAlternatively, you can directly submit configuration files and open a pull request in the content repository for the VEDA Dashboard, veda-config.\nIf you have any questions along the way, we prefer that you open tickets in veda-config. Alternatively, you can reach the VEDA team at veda@uah.edu."
  },
  {
    "objectID": "contributing/dashboard-configuration/discovery-configuration.html",
    "href": "contributing/dashboard-configuration/discovery-configuration.html",
    "title": "Discovery Configuration",
    "section": "",
    "text": "By this point, you should have a few things:\n🧑‍🏫 We recommend you follow the video walkthrough on how to setup a virtual environment to facilitate discovery creation."
  },
  {
    "objectID": "contributing/dashboard-configuration/discovery-configuration.html#sec-video-walkthrough",
    "href": "contributing/dashboard-configuration/discovery-configuration.html#sec-video-walkthrough",
    "title": "Discovery Configuration",
    "section": "Video Walkthrough",
    "text": "Video Walkthrough\n\nSetting up github codespaces\nCodespaces will allow you to have a development environment in the cloud without the need to setup anything on your local machine. VIDEO\n\n\nCreating a discovery\nWalkthrough of how to use github codespaces to create a discovery. From creating the needed files to the Pull Request that will eventually get the content published. VIDEO"
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html",
    "href": "contributing/dataset-ingestion/file-preparation.html",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The docs have a guide on preparing COGs, too.\n\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the right resampling method for overviews, for example average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore)."
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "href": "contributing/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The docs have a guide on preparing COGs, too.\n\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the right resampling method for overviews, for example average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore)."
  },
  {
    "objectID": "contributing/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "href": "contributing/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "title": "File preparation",
    "section": "STEP II: Upload to the VEDA data store",
    "text": "STEP II: Upload to the VEDA data store\nOnce you have the COGs, obtain permissions to upload them to the veda-data-store-staging bucket.\nUpload the data to a sensible location inside the bucket. Example: s3://veda-data-store-staging/&lt;collection-id&gt;/"
  },
  {
    "objectID": "contributing/dataset-ingestion/index.html",
    "href": "contributing/dataset-ingestion/index.html",
    "title": "Dataset Ingestion",
    "section": "",
    "text": "VEDA uses a centralized Spatio-Temporal Asset Catalog (STAC) for data dissemination and prefers to hosts datasets in cloud-object storage (AWS S3 in the region us-west-2) in the cloud-optimized file formats Cloud-Optimized GeoTIFF (COG) and Zarr, which enables viewing and efficient access in the cloud directly from the original datafiles without copies or multiple versions."
  },
  {
    "objectID": "contributing/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "href": "contributing/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "title": "Dataset Ingestion",
    "section": "Steps for ingesting a dataset",
    "text": "Steps for ingesting a dataset\nFor dataset ingestion, generally four steps are required. Depending on the capacity of the dataset provider, some of the steps can be completed by the VEDA team on request.\nComplete as many steps of the process as you have capacity or authorization to. Please see the guides below on\n\nOpen a dedicated pull request in the veda-data-pipelines repository\nTransform datasets to conform with cloud-optimized file formats - see file preparation\nUpload files to storage (may be skipped, if data is cloud-optimized and in us-west-2)\nCreate compliant metadata records for our STAC - see example and conventions for STAC collections and example and conventions for STAC items.\nLoad those records into the VEDA STAC - see catalog ingestion"
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. Examples no2-mean and no2-diff should be two different collections, because one measures the mean and the other the diff. no2-monthly and no2-yearly should be different because the periodicity is different.\nOne you’ve logically grouped the datasets into collectionss, create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"cogify\": false,\n      \"upload\": false,\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: The steps after this are technical, so at this point the scientists can send the json to the VEDAteam and they’ll handle the publication process."
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. Examples no2-mean and no2-diff should be two different collections, because one measures the mean and the other the diff. no2-monthly and no2-yearly should be different because the periodicity is different.\nOne you’ve logically grouped the datasets into collectionss, create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"cogify\": false,\n      \"upload\": false,\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: The steps after this are technical, so at this point the scientists can send the json to the VEDAteam and they’ll handle the publication process."
  },
  {
    "objectID": "contributing/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "href": "contributing/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "title": "Catalog Ingestion",
    "section": "STEP IV: Publication",
    "text": "STEP IV: Publication\nThe publication process involves 3 steps:\n\n[VEDA] Publishing to the development STAC catalog https://dev-stac.delta-backend.com\n[EIS] Reviewing the collection/items published to the dev STAC catalog\n[VEDA] Publishing to the staging STAC catalog https://staging-stac.delta-backend.com\n\nTo use the VEDA Ingestion API to schedule ingestion/publication of the data follow these steps:\n\n1. Obtain credentials from a VEDA team member\nAsk a VEDA team member to create credentials (username and password) for VEDA auth.\n\n\n2. Export username and password\nexport username=\"johndoe\"\nexport password=\"xxxx\"\n\n\n3. Get token\n# Required imports\nimport os\nimport requests\n\n# Pull username and password from environment variables\nusername = os.environ.get(\"username\")\npassword = os.environ.get(\"password\")\n\n# base url for the workflows api\n# experimental / subject to change in the future\nbase_url = \"https://dev-api.delta-backend.com\"\n\n# endpoint to get the token from\ntoken_url = f\"{base_url}/token\"\n\n# authentication credentials to be passed to the token_url\nbody = {\n    \"username\": username,\n    \"password\": password,\n}\n\n# request token\nresponse = requests.post(token_url, data=body)\nif not response.ok:\n    raise Exception(\"Couldn't obtain the token. Make sure the username and password are correct.\")\nelse:\n    # get token from response\n    token = response.json().get(\"AccessToken\")\n    # prepare headers for requests\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n\n\n4. Ingest the dataset\nThen, use the code snippet below to publish the dataset.\n# url for dataset validation / publication\nvalidate_url = f\"{base_url}/dataset/validate\"\n\npublish_url = f\"{base_url}/dataset/publish\"\n\n# prepare the body of the request,\nbody = json.load(open(\"dataset-definition.json\"))\n\n# Validate the data definition using the /validate endpoint\nvalidation_response = requests.post(\n    validate_url,\n    headers=headers,\n    json=body\n)\n\n# look at the response\nvalidation_response.raise_for_status()\n\n# If the validation is successful, publish the dataset using /publish endpoint\npublish_response = requests.post(\n    publish_url,\n    headers=headers,\n    json=body\n)\n\nif publish_response.ok:\n    print(\"Success\")\n\n\nCheck the status of the execution\n# the id of the execution\n# should be available in the response of workflow execution request\nexecution_id = \"xxx\"\n# url for execution status\nexecution_status_url = f\"{workflow_execution_url}/{execution_id}\"\n# make the request\nresponse = requests.get(\n    execution_status_url,\n    headers=headers,\n)\nif response.ok:\n    print(response.json())"
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-collection-conventions.html",
    "href": "contributing/dataset-ingestion/stac-collection-conventions.html",
    "title": "STAC collection conventions",
    "section": "",
    "text": "Copied from veda-backend#29\nDashboard-specific notes that supplement the full stac-api collection specification. Note that there is no schema enforcement on the collection table content in pgstac—this provides flexibility but also requires caution when creating and modifying Collections."
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "href": "contributing/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "title": "STAC collection conventions",
    "section": "Collection field, extension, and naming recommendations",
    "text": "Collection field, extension, and naming recommendations\n\n\n\nField &/or Extension\nRecommendations\n\n\n\n\nid\nIf dataset exists in NASA’s Earthdata or presumably from some other data provider like ESA, use that ID. If appropriate, add a suffix for any additional processing that has been performed, e.g. “OMSO2PCA_cog”. If dataset is not from NASA’s Earthdata, we can use a human readable name with underscores like “facebook_population_density”.\n\n\ndashboard extension\nTo support the delta-ui we have added two new fields in a proposed dashboard extension. For now we are just adding the fields but after testing things out, we can formalize the extension with a hosted json schema. Dashboard extension properties are only required for collections that will be viewed in the delta-ui dashboard.\n\n\ndashboard:is_periodic\nTrue/False This boolean is used when summarizing the collection—if the collection is periodic, the temporal range of the items in the collection and the time density are all the front end needs to generate a time picker. If the items in the collection are not periodic, a complete list of the unique item datetimes is needed.\n\n\ndashboard:time_density\nyear, month, day, hour, minute, or null. These time steps should be treated as enum when the extension is formalized. For collections with a single time snapshot this value is null.\n\n\nitem_assets\nstac-extension/item_assets is used to explain the assets that are provided for each item in the collection. We’re not providing thumbnails yet, but this example below includes a thumbnail asset to illustrate how the extension will be used. The population of this property is not automated, the creator of the collection writes the item assets documentation. Item assets are only required for collections that will be viewed in the delta-ui dashboard.\n\n\nsummaries\nThe implementation of this core stac-spec field is use-case specific. Our implementation is intended to support the dashboard and will supply datetime and raster statistics for the default map layer asset across the entire collection. Currently summaries are manually updated with a delta-ui specific user defined function in pgstac.\n\n\ntitle and description\nUse these properties to provide specific information about the collection to API users and catalog browsers. These properties correspond to dataset name and info in the covid-api but the delta dashboard will use delta-config to set these values in the UI so the information in our stac collections will be for data curators and API users.\n\n\ncollection name style choices\nPrefer lower-case kebab-case collection names. Decision: Should names align with underlying data identifiers or should it be an interpreted name? omi-trno2-dhrm and omi-trno2-dhrm-difference vs no2-monthly and no2-monthly-diff; bmhd-30m-monthly vs nightlights-hd-monthly\n\n\nlicense\nSPDX license id, license is likely available in CMR but we may need to research other sources of data. Default open license: CC0-1.0\n\n\n\nitem_assets example\n\n\"item_assets\": {\n    \"thumbnail\": {\n      \"type\": \"image/jpeg\",\n      \"roles\": [\n        \"thumbnail\"\n      ],\n      \"title\": \"Thumbnail\",\n      \"description\": \"A medium sized thumbnail\"\n    },\n    \"cog_default\": {\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"roles\": [\n        \"data\",\n        \"layer\"\n      ],\n      \"title\": \"Default COG Layer\",\n      \"description\": \"Cloud optimized default layer to display on map\"\n    }\n  }\nsummaries example for periodic collection\n\"summaries\": {\n    \"datetime\": [\"2016-01-01T00:00:00Z\", \"2022-01-01T00:00:00Z\"],\n    \"cog_default\": {\n      \"max\": 50064805976866820,\n      \"min\": -6618294421291008\n    }\n  }\nsummaries example for non-periodic collection\n\"summaries\": {\n    \"datetime\": [\n      \"2020-01-01T00:00:00Z\",\n      \"2020-02-01T00:00:00Z\",\n      \"2020-03-01T00:00:00Z\",\n      \"2020-04-01T00:00:00Z\",\n      \"2020-05-01T00:00:00Z\",\n      \"2020-06-01T00:00:00Z\",\n      \"2020-07-01T00:00:00Z\",\n      \"2020-08-01T00:00:00Z\",\n      \"2020-09-01T00:00:00Z\",\n      \"2020-10-01T00:00:00Z\",\n      \"2020-11-01T00:00:00Z\",\n      \"2020-12-01T00:00:00Z\",\n      \"2021-01-01T00:00:00Z\",\n      \"2021-02-01T00:00:00Z\",\n      \"2021-03-01T00:00:00Z\",\n      \"2021-04-01T00:00:00Z\",\n      \"2021-05-01T00:00:00Z\",\n      \"2021-06-01T00:00:00Z\",\n      \"2021-07-01T00:00:00Z\",\n      \"2021-08-01T00:00:00Z\",\n      \"2021-09-01T00:00:00Z\"\n    ],\n    \"cog_default\": {\n      \"max\": 255,\n      \"min\": 0\n    }\n  }"
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html",
    "title": "STAC item conventions",
    "section": "",
    "text": "Copied from veda-backend#28\nThis document defines a set of conventions for generating STAC Items consistently for the VEDA Dashboard UI and future API users. Ultimately, these represent the minimum metadata API users can expect from the backend."
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "title": "STAC item conventions",
    "section": "Rio-stac conventions for generating STAC Items",
    "text": "Rio-stac conventions for generating STAC Items\nAll of our current ingestion plans will use rio-stac to generate item metadata for COGs so the notes below are organized around the input parameters of the create_stac_item method.\nexample rio-stac python usage\nitem = rio_stac.stac.create_stac_item(\n  id = item_id,\n  source = f\"s3://{obj.bucket_name}/{obj.key}\", \n  collection = collection_id, \n  input_datetime = &lt;datetime.datetime&gt;,\n  with_proj = True,\n  with_raster = True,\n  asset_name = \"cog_default\",\n  asset_roles = [\"data\", \"layer\"],\n  asset_media_type = \"image/tiff; application=geotiff; profile=cloud-optimized\",\n)\nRio-stac create item parameter recommendations These recommendations are for generating STAC Item metadata for collections intended for the dasboard and may not be applicable to all ARCO collections.\n\n\n\nParameter\nRecommendations\n\n\n\n\nid\n(1) When STAC Item metadata is generated from a COG file, strip the full file extension from the filename for the item id. (2) When ids are not unique across collections, append the collection id to the item id. For example the no2-monthly and no2-monthly-diff COGs are stored with unique bucket prefixes but within the prefix all the filenames are the same, so the collection id is appended: OMI_trno2_0.10x0.10_201604_Col3_V4 → OMI_trno2_0.10x0.10_201604_Col3_V4-no2-monthly).\n\n\nwith_proj\nTrue. Generate projection extension metadata for the item for future ARCO datastore users.\n\n\nwith_raster\nTrue. This will generate gdal statistics for every band in the COG—we use these to get the range of values for the full collection.\n\n\nasset_name\nA meaningful asset name for the default cloud optimized asset to be displayed on a map. cog_default is a placeholder—we need to choose and commit to an asset name for all collections. If not set, will default to asset. * TODO Decision: For items with many assets we should ingest all with appropriate keys and duplicate one preferred display asset as the default cog. We should be considering metadata conventions in pgstac-titiler\n\n\nasset_roles\n[\"data\", \"layer\"] data is an appropriate role, we may also choose to add something like layer to indicate that the asset is optimized to be used as a map layer (stac specification for asset roles).\n\n\nasset_media_type\n\"image/tiff; application=geotiff; profile=cloud-optimized (stac best practices for asset media type).\n\n\nproperties\nCMIP6: TODO, CMR: TODO if we don’t store links to the original data, downstream users are not going to be able to pair STAC records with the versioned parent data in CMR"
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "title": "STAC item conventions",
    "section": "Data provenance convention",
    "text": "Data provenance convention\nWhen adding STAC items that were derived from previously published data (such as CMR records), there are multiple ways to preserve the linkage between the item and the more complete source metadata. We should provide at a minimum metadata assets for any items derived from previously published data. Here are three examples from HLS:\nmetadata are assets The CMR properties question in the table above (how to refer the STAC Item to it’s CMR source metadata) could instead be solved by adding a metadata asset. This does not require creating a new extension for CMR, it just involves creating an asset from the CMR granule metadata which should be in the event context for CMR search driven ingests. The example below is from documentation for using HLS cloud optimized data.\n\"assets\": {\n  \"metadata\": {\n    \"href\": \"https://cmr.earthdata.nasa.gov/search/concepts/G2099379244-LPCLOUD.xml\",\n    \"type\": \"application/xml\"\n    },\n    \"thumbnail\": { ...}\n}\nstac-spec scieintific extension\n\"properties\": {\n   \"sci:doi\": \"10.5067/HLS/HLSS30.002\",\n   ...\n}\nItem links to metadata Use a cite-as Item link to the DOI for the source data.\n\"links\": [\n  {\n    \"rel\": \"cite-as\",\n    \"href\": \"https://doi.org/10.5067/HLS/HLSS30.002\"\n  },\n  ...\n]"
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "title": "STAC item conventions",
    "section": "STAC Item validation convention",
    "text": "STAC Item validation convention\nWe are producing pystac.items with rio-stac’s create_stac_item method and we should validate them before publishing them to s3. Testing found that it is possible to produce structurally sound but invalid STAC Items with create_stac_item.\nThe built in pystac validator on the pystac.item returned by create_stac_item can be used to easily validate the metadata—item.validate() will raise an exception for invalid metadata. Pystac does need to be installed with the appropriate dependencies for validation."
  },
  {
    "objectID": "contributing/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "href": "contributing/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "title": "STAC item conventions",
    "section": "Convention for default map layer assets for spectral data",
    "text": "Convention for default map layer assets for spectral data\nMany of the collections for the dashboard have a clear default map layer asset that we can name cog_default. This convention does not map as well to spectral data with many assets (B01, B02,…). A preferred band asset could be duplicated to define a default map layer asset to be consistent but this needs to be decided."
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Usage Examples",
    "section": "",
    "text": "The example notebooks are divided into three sections:\n\nQuickstarts: Notebooks to get you started quickly and help you become more familiar with cloud-native geospatial technologies.\nTutorials: Longer notebooks that walk through more advanced use cases and examples.\nDatasets: Notebooks that showcase a particular VEDA dataset and walk through an applied geospatial analyses.\n\n\n\nThe Quickstarts examples are further divided into two sections, which you can choose from depending on your data needs:\n\nAccessing the Data Directly: For when you want to access the raw data (e.g., to do a specific analysis). In this case, permissions are required to access the data (i.e., must be run on VEDA JupyterHub) and computation happens within the user’s instance (i.e., the user needs to think about instance size). This approach is suitable for use within notebooks. All examples provided in this section require VEDA JupyterHub access to run.\nUsing the Raster API: For when you want to show outputs to other people or do standard processing. No permissions required (i.e., notebooks can be run on mybinder). Additionally, the computation happens somehwere else (i.e., user does not have to think about instance size). Lastly, this approach is suitable for use within notebooks as well as web application frontends (e.g., like dataset discoveries). These notebook examples can be run on both VEDA JupyterHub, as well as outside of the Hub (see instructions below) and within mybinder."
  },
  {
    "objectID": "notebooks/index.html#getting-started",
    "href": "notebooks/index.html#getting-started",
    "title": "Usage Examples",
    "section": "",
    "text": "The example notebooks are divided into three sections:\n\nQuickstarts: Notebooks to get you started quickly and help you become more familiar with cloud-native geospatial technologies.\nTutorials: Longer notebooks that walk through more advanced use cases and examples.\nDatasets: Notebooks that showcase a particular VEDA dataset and walk through an applied geospatial analyses.\n\n\n\nThe Quickstarts examples are further divided into two sections, which you can choose from depending on your data needs:\n\nAccessing the Data Directly: For when you want to access the raw data (e.g., to do a specific analysis). In this case, permissions are required to access the data (i.e., must be run on VEDA JupyterHub) and computation happens within the user’s instance (i.e., the user needs to think about instance size). This approach is suitable for use within notebooks. All examples provided in this section require VEDA JupyterHub access to run.\nUsing the Raster API: For when you want to show outputs to other people or do standard processing. No permissions required (i.e., notebooks can be run on mybinder). Additionally, the computation happens somehwere else (i.e., user does not have to think about instance size). Lastly, this approach is suitable for use within notebooks as well as web application frontends (e.g., like dataset discoveries). These notebook examples can be run on both VEDA JupyterHub, as well as outside of the Hub (see instructions below) and within mybinder."
  },
  {
    "objectID": "notebooks/index.html#how-to-run",
    "href": "notebooks/index.html#how-to-run",
    "title": "Usage Examples",
    "section": "How to run",
    "text": "How to run\nEvery notebook contains information about how to run it. Some can run on mybinder and all can run on the VEDA JupyterHub. See VEDA Analytics JupyterHub Access for information about how to gain access.\n\nRunning outside of VEDA JupyterHub\nTo run the notebooks locally, you can use can install the Python packages (a virtual environment is recommended)\npip install -r requirements.txt\nOnce you have installed the packages you can run the notebooks using Jupyter.\njupyter lab\nIf the notebook needs access to protected data on S3, you will need to specifically get access. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN)."
  },
  {
    "objectID": "notebooks/index.html#how-to-contribute",
    "href": "notebooks/index.html#how-to-contribute",
    "title": "Usage Examples",
    "section": "How to contribute",
    "text": "How to contribute\nPlease refer to the notebook style guide in these docs."
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html",
    "href": "notebooks/veda-operations/stac-item-creation.html",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "href": "notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "href": "notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "title": "STAC Item Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U rio_stac parse xpystac pystac nbss-upload --quiet\n\n\nfrom parse import parse\nfrom datetime import datetime\nimport rio_stac\nimport xarray as xr\nimport hvplot.xarray"
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "href": "notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "title": "STAC Item Creation",
    "section": "Create pystac.Item",
    "text": "Create pystac.Item\nIn this section we will be creating a pystac.Item object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nITEM_ID = \"OMI_trno2_0.10x0.10_202212_Col3_V4.nc\"\nSOURCE = \"s3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif\"\n\n\n\nCalculate datetime\nCreate a function that calculates datetime when given an item_id. You can change this to depend on the source instead if that works better.\n\ndef datetime_func(item_id: str) -&gt; datetime:\n    \"\"\"Given the item_id, figure out the datetime\"\"\"\n    \n    fields = parse(\"OMI_trno2_0.10x0.10_{year:4}{month:2}_Col3_V4.nc\", item_id)\n    year = int(fields[\"year\"])\n    month = int(fields[\"month\"])\n    day = 1\n    return datetime(year, month, day)\n\nTest out the datetime function:\n\ndatetime_func(ITEM_ID)\n\ndatetime.datetime(2022, 12, 1, 0, 0)\n\n\n\n\nPut it together\nNow take your constants and datetime function and create the STAC Item using rio_stac.\n\nitem = rio_stac.stac.create_stac_item(\n  id=ITEM_ID,\n  source=SOURCE,\n  collection=COLLECTION_ID, \n  input_datetime=datetime_func(ITEM_ID),\n  with_proj=True,\n  with_raster=True,\n  asset_name=\"cog_default\",\n  asset_roles=[\"data\", \"layer\"],\n  asset_media_type=\"image/tiff; application=geotiff; profile=cloud-optimized\",\n)"
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "href": "notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "title": "STAC Item Creation",
    "section": "Try it out!",
    "text": "Try it out!\nNow that you have an item you can try it out and make sure it looks good and passes validation checks.\n\nitem.validate()\n\n['https://schemas.stacspec.org/v1.0.0/item-spec/json-schema/item.json',\n 'https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/raster/v1.1.0/schema.json']\n\n\n\nitem.to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n 'properties': {'proj:epsg': 4326,\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:shape': [1800, 3600],\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'proj:projjson': {'$schema': 'https://proj.org/schemas/v0.5/projjson.schema.json',\n   'type': 'GeographicCRS',\n   'name': 'WGS 84',\n   'datum': {'type': 'GeodeticReferenceFrame',\n    'name': 'World Geodetic System 1984',\n    'ellipsoid': {'name': 'WGS 84',\n     'semi_major_axis': 6378137,\n     'inverse_flattening': 298.257223563}},\n   'coordinate_system': {'subtype': 'ellipsoidal',\n    'axis': [{'name': 'Geodetic latitude',\n      'abbreviation': 'Lat',\n      'direction': 'north',\n      'unit': 'degree'},\n     {'name': 'Geodetic longitude',\n      'abbreviation': 'Lon',\n      'direction': 'east',\n      'unit': 'degree'}]},\n   'id': {'authority': 'EPSG', 'code': 4326}},\n  'datetime': '2022-12-01T00:00:00Z'},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[(-180.0, -90.0),\n    (180.0, -90.0),\n    (180.0, 90.0),\n    (-180.0, 90.0),\n    (-180.0, -90.0)]]},\n 'links': [{'rel': &lt;RelType.COLLECTION: 'collection'&gt;,\n   'href': 'no2-monthly-diff',\n   'type': &lt;MediaType.JSON: 'application/json'&gt;}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'raster:bands': [{'data_type': 'float32',\n     'scale': 1.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'nodata': -1.2676506002282294e+30,\n     'statistics': {'mean': 12233282717799.0,\n      'minimum': -1.30282195779584e+16,\n      'maximum': 2.082349180465971e+16,\n      'stddev': 416857512760678.5,\n      'valid_percent': 82.7056884765625},\n     'histogram': {'count': 11,\n      'min': -1.30282195779584e+16,\n      'max': 2.082349180465971e+16,\n      'buckets': [20, 138, 881, 421049, 11300, 203, 20, 3, 1, 1]}}],\n   'roles': ['data', 'layer']}},\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json'],\n 'collection': 'no2-monthly-diff'}\n\n\n\nPlot it (optional)\nCreate a quick visual to make sure that data loads and visualizes properly.\n\ndata = xr.open_dataset(item).cog_default.isel(time=0)\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'cog_default' (y: 1800, x: 3600)&gt;\n[6480000 values with dtype=float64]\nCoordinates:\n    time            datetime64[ns] 2022-12-01\n    id              &lt;U37 ...\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    proj:transform  object ...\n    proj:projjson   object ...\n    proj:epsg       int64 ...\n    proj:bbox       object ...\n    proj:shape      object ...\n    proj:geometry   object ...\n    raster:bands    object ...\n    epsg            int64 ...xarray.DataArray'cog_default'y: 1800x: 3600...[6480000 values with dtype=float64]Coordinates: (12)time()datetime64[ns]2022-12-01array('2022-12-01T00:00:00.000000000', dtype='datetime64[ns]')id()&lt;U37...[1 values with dtype=&lt;U37]x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])proj:transform()object...[1 values with dtype=object]proj:projjson()object...[1 values with dtype=object]proj:epsg()int64...[1 values with dtype=int64]proj:bbox()object...[1 values with dtype=object]proj:shape()object...[1 values with dtype=object]proj:geometry()object...[1 values with dtype=object]raster:bands()object...[1 values with dtype=object]epsg()int64...[1 values with dtype=int64]Indexes: (2)xPandasIndexPandasIndex(Float64Index([            -180.0,             -179.9,             -179.8,\n                          -179.7,             -179.6,             -179.5,\n                          -179.4,             -179.3,             -179.2,\n                          -179.1,\n              ...\n                           179.0, 179.10000000000002, 179.20000000000005,\n                           179.3, 179.40000000000003,              179.5,\n              179.60000000000002, 179.70000000000005,              179.8,\n              179.90000000000003],\n             dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Float64Index([              90.0,               89.9,               89.8,\n                            89.7,               89.6,               89.5,\n                            89.4,               89.3,               89.2,\n                            89.1,\n              ...\n                           -89.0, -89.10000000000002, -89.20000000000002,\n              -89.30000000000001,              -89.4,              -89.5,\n              -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                           -89.9],\n             dtype='float64', name='y', length=1800))Attributes: (0)\n\n\n\ncolor_range = tuple(data.quantile([.02, .98]).values)\n\n\ndata.hvplot(\"x\", \"y\", clim=color_range, cmap=\"jet\", rasterize=True)\n\n\n\n\n\n  \n\n\n\n\nNOTE: Jet is a bad colormap because it overemphasizes certain values, but it has a very large number of colors so it is good for spotting odd patterns in the data."
  },
  {
    "objectID": "notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "href": "notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "title": "STAC Item Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-item.ipynb"
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html",
    "href": "notebooks/veda-operations/stac-collection-creation.html",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "href": "notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "href": "notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "title": "STAC Collection Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U pystac nbss-upload --quiet\n\n\nfrom datetime import datetime, timezone\nimport pystac"
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "href": "notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "title": "STAC Collection Creation",
    "section": "Create pystac.Collection",
    "text": "Create pystac.Collection\nIn this section we will be creating a pystac.Collection object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string and boolean fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nTITLE = \"NO₂ (Diff)\"\nDESCRIPTION = (\n    \"This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors \"\n    \"indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. \"\n    \"Missing pixels indicate areas of no data most likely associated with \"\n    \"cloud cover or snow.\"\n)\nDASHBOARD__IS_PERIODIC = True\nDASHBOARD__TIME_DENSITY = \"month\"\nLICENSE = \"CC0-1.0\"\n\n\n\nExtents\nThe extents indicate the start (and potentially end) times of the data as well as the footprint of the data.\n\n# Time must be in UTC\ndemo_time = datetime.now(tz=timezone.utc)\n\nextent = pystac.Extent(\n    pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]]),\n    pystac.TemporalExtent([[demo_time, None]]),\n)\n\n\n\nProviders\nWe know that the data host, processor, and producter is “VEDA”, but you can include other providers that fill other roles in the data creation pipeline.\n\nproviders = [\n    pystac.Provider(\n        name=\"VEDA\",\n        roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST],\n        url=\"https://github.com/nasa-impact/veda-data-pipelines\",\n    )\n]\n\n\n\nPut it together\nNow take your constants and the extents and providers and create a pystac.Collection\n\ncollection = pystac.Collection(\n    id=COLLECTION_ID,\n    title=TITLE,\n    description=DESCRIPTION,\n    extra_fields={\n        \"dashboard:is_periodic\": DASHBOARD__IS_PERIODIC,\n        \"dashboard:time_density\": DASHBOARD__TIME_DENSITY,\n    },\n    license=LICENSE,\n    extent=extent,\n    providers=providers,\n)\n\n\n\nTry it out!\nNow that you have a collection you can try it out and make sure that it looks how you expect and that it passes validation checks.\n\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.0.0/collection-spec/json-schema/collection.json']\n\n\n\ncollection.to_dict()\n\n{'type': 'Collection',\n 'id': 'no2-monthly-diff',\n 'stac_version': '1.0.0',\n 'description': 'This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'links': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month',\n 'title': 'NO₂ (Diff)',\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2023-06-12T17:36:30.161697Z', None]]}},\n 'license': 'CC0-1.0',\n 'providers': [{'name': 'VEDA',\n   'roles': [&lt;ProviderRole.PRODUCER: 'producer'&gt;,\n    &lt;ProviderRole.PROCESSOR: 'processor'&gt;,\n    &lt;ProviderRole.HOST: 'host'&gt;],\n   'url': 'https://github.com/nasa-impact/veda-data-pipelines'}]}"
  },
  {
    "objectID": "notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "href": "notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "title": "STAC Collection Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-collection.ipynb"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html",
    "href": "notebooks/tutorials/gif-generation.html",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "This notebook demonstrates how to use the cog/crop endpoint to generate GIFs from data in the VEDA API.\nThe overall process will be: 1. Use the STAC API to gather a list of STAC Items which will each become on frame in our gif 2. Query the /cog/crop endpoint with the asset URL and a geojson geometry 3. Stack all of the generated images into a animated GIF\n\n\n\n# Standard lib imports\nfrom concurrent.futures import ThreadPoolExecutor\nimport datetime\nimport glob\nimport json\nimport os\nimport requests\nimport tempfile\nimport time\nimport io\nfrom IPython import display\n\n# 3rd party imports\nimport folium\nimport numpy as np\n\n# import PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport rasterio\nimport rasterio.features\nimport rasterio.plot\n\n\n\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Collection we'll be using to generate the GIF\ncollection = \"no2-monthly\"\n\n\n\n\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[45, 0],\n    zoom_start=5,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm\n\n\n\n\n\n# NO2 monthly has a global extent, so we don't need to specify an area within\n# which to search. For non-global datasets, use the `bbox` parameter to specify\n# the bounding box within which to search.\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection}/items?limit=100\").json()[\n    \"features\"\n]\n\n\n# Available dates:\ndates = [item[\"properties\"][\"start_datetime\"] for item in items]\nprint(f\"Dates available: {dates[:5]} ... {dates[-5:]}\")\n\n\n\n\nThe endpoint accepts the following parameters, among others: - format (tif, jpeg, webp, etc) - height and width - url (for the COG file to extract data from)\nAnd any other visualization parameters specific to that dataset (eg: rescale and color_map values)\n\n\n\n# get visualization parameters from collection summaries\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"no2-monthly\"\n][0][\"summaries\"][\"cog_default\"]\n\n\n\n\n\n# get PNG bytes from API\nresponse = requests.post(\n    f\"{RASTER_API_URL}/cog/crop\",\n    params={\n        \"format\": \"png\",\n        \"height\": 512,\n        \"width\": 512,\n        \"url\": items[0][\"assets\"][\"cog_default\"][\"href\"],\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n    json=france_aoi,\n)\n\nassert response.ok, response.text\n\nimage_bytes = response.content\n\n# Write to temporary file in order to display\nf = tempfile.NamedTemporaryFile(suffix=\".png\")\nf.write(image_bytes)\n\n# display PNG!\ndisplay.Image(filename=f.name, height=512, width=512)\n\n\n\n\n\nTo generate a GIF we request a PNG for each STAC Item and then use the Python Imaging Library (PIL) to combine them into a GIF. We will use a temporary directory to store all the generated PNGs and we will use multi-threading to speed up the operation\n\n# for convenience we will wrap the API call from above into a method that will\n# save the contents of the image file into a file stored within the temp directory\nfrom gif_generation_dependencies.helper_functions import generate_frame\n\n# temporary directory to hold PNGs\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    start = time.time()\n\n    args = (\n        (\n            item,  # stac item\n            france_aoi,  # aoi to crop\n            tmpdirname,  # tmpdir (optional)\n            \"png\",  # image format\n            None,  # overlay (will be discussed further)\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },  # visualization parameters\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = (Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\"))))\n    img = next(imgs)  # extract first image from iterator\n    img.save(\n        fp=\"./output.gif\",\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output.gif\")\n\n\n\n\nTo provide more interesting or engaging data to the users, we can add temporal and geospatial context to the GIF. This is possible because API can return images in geo-referenced tif format.\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    filepath = generate_frame(items[0], france_aoi, tmpdirname, image_format=\"tif\")\n\n    # Verify that the tif returned by the API is correctly georeferenced\n    georeferenced_raster_data = rasterio.open(filepath)\n\n    print(\"Data bounds: \", georeferenced_raster_data.bounds)\n    print(\"Data CRS: \", georeferenced_raster_data.crs)\n\n\n\nIn order to overlay GeoJSON over the raster, we will have to convert the geojson boundaries to a raster format. We do this with the following steps:\nFor each feature in the geojson we rasterize the feature into a mask. We use binary dialation to detect the edges of the mask, and set the values corresponding to the mask edges to 255. This approach has one known problem: if multiple features share a border (eg: two adjoining provinces) the border between then will be detected twice, once from each side (or from each feature sharing that border). This means that internal borders will be twice as thick as external borders\n\nfrom gif_generation_dependencies.helper_functions import overlay_geojson\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        geojson = json.loads(f.read())\n\n    filepath = generate_frame(\n        items[0],\n        france_aoi,\n        tmpdirname,\n        image_format=\"tif\",\n        additional_cog_crop_args={\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n    )\n\n    filepath = overlay_geojson(filepath, geojson)\n    rasterio.plot.show(rasterio.open(filepath))\n\n\n\n\nAnother way to contextualize where in the GIF’s data is, is by overlaying the GIF on top of a base map. This process is a bit more complicated: - Generate a raster image (.tif) - Overlay in on a folium map interface - Save the map interface to html - Open the html file with a headless chrome webdriver (using the selenium library) - Save a screenshot of the rendered html as a .png\n\nfrom gif_generation_dependencies.helper_functions import overlay_raster_on_folium\n\ntmpdirname = tempfile.TemporaryDirectory()\n\nimage_filepath = generate_frame(\n    items[0],\n    france_aoi,\n    tmpdirname.name,\n    image_format=\"tif\",\n    overlay=None,\n    additional_cog_crop_args={\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n)\n\nimage_filepath = overlay_raster_on_folium(image_filepath)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\nNow that we have the raster data displayed over the basemap, we want to add the date of each file\n\nfrom gif_generation_dependencies.helper_functions import overlay_date\n\ndate = items[0][\"properties\"][\"start_datetime\"]\n\n# get datestring from STAC Item properties and reformat\ndatestring = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\").date().isoformat()\n\n# Reuse the raster overlayed on the OSM basemap using folium from above:\noverlay_date(image_filepath, datestring)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\n\nI’ve combined all of the above functionality, along with a few helper functions in the file: ./gif_generation_dependencies/helper_functions.py\nI’ve also added the contextualizaiton steps (overlaying geojson, date, and folium basemap) directly into the generate_frame() method\n\n\n\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        overlay = json.loads(f.read())\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            geojson,\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.tif\")))]\n    imgs[0].save(\n        fp=\"./output_with_geojson.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_geojson.gif\")\n\n\n\n\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            \"folium\",\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    # Note: I'm searching for `*.png` files instead of *.tif files because the webdriver screenshot\n    # of the folium map interface is exported in png format (this also helps reduce the size of\n    # the final gif )\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\")))]\n    imgs[0].save(\n        fp=\"./output_with_osm_basemap.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_osm_basemap.gif\")\n\n\n\n\nRun the following cell to remove the following generated images/gifs: - output.gif - output_with_geojson.gif - output_with_osm_basemap.gif\n\nfor f in glob.glob(os.path.join(\".\", \"output*.gif\")):\n    os.remove(f)"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "href": "notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "# Standard lib imports\nfrom concurrent.futures import ThreadPoolExecutor\nimport datetime\nimport glob\nimport json\nimport os\nimport requests\nimport tempfile\nimport time\nimport io\nfrom IPython import display\n\n# 3rd party imports\nimport folium\nimport numpy as np\n\n# import PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport rasterio\nimport rasterio.features\nimport rasterio.plot"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#define-global-variables",
    "href": "notebooks/tutorials/gif-generation.html#define-global-variables",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "STAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Collection we'll be using to generate the GIF\ncollection = \"no2-monthly\""
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "href": "notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "We can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[45, 0],\n    zoom_start=5,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "href": "notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "# NO2 monthly has a global extent, so we don't need to specify an area within\n# which to search. For non-global datasets, use the `bbox` parameter to specify\n# the bounding box within which to search.\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection}/items?limit=100\").json()[\n    \"features\"\n]\n\n\n# Available dates:\ndates = [item[\"properties\"][\"start_datetime\"] for item in items]\nprint(f\"Dates available: {dates[:5]} ... {dates[-5:]}\")"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#the-cogcrop-endpoint",
    "href": "notebooks/tutorials/gif-generation.html#the-cogcrop-endpoint",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "The endpoint accepts the following parameters, among others: - format (tif, jpeg, webp, etc) - height and width - url (for the COG file to extract data from)\nAnd any other visualization parameters specific to that dataset (eg: rescale and color_map values)\n\n\n\n# get visualization parameters from collection summaries\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"no2-monthly\"\n][0][\"summaries\"][\"cog_default\"]\n\n\n\n\n\n# get PNG bytes from API\nresponse = requests.post(\n    f\"{RASTER_API_URL}/cog/crop\",\n    params={\n        \"format\": \"png\",\n        \"height\": 512,\n        \"width\": 512,\n        \"url\": items[0][\"assets\"][\"cog_default\"][\"href\"],\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n    json=france_aoi,\n)\n\nassert response.ok, response.text\n\nimage_bytes = response.content\n\n# Write to temporary file in order to display\nf = tempfile.NamedTemporaryFile(suffix=\".png\")\nf.write(image_bytes)\n\n# display PNG!\ndisplay.Image(filename=f.name, height=512, width=512)"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#generating-a-gif",
    "href": "notebooks/tutorials/gif-generation.html#generating-a-gif",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "To generate a GIF we request a PNG for each STAC Item and then use the Python Imaging Library (PIL) to combine them into a GIF. We will use a temporary directory to store all the generated PNGs and we will use multi-threading to speed up the operation\n\n# for convenience we will wrap the API call from above into a method that will\n# save the contents of the image file into a file stored within the temp directory\nfrom gif_generation_dependencies.helper_functions import generate_frame\n\n# temporary directory to hold PNGs\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    start = time.time()\n\n    args = (\n        (\n            item,  # stac item\n            france_aoi,  # aoi to crop\n            tmpdirname,  # tmpdir (optional)\n            \"png\",  # image format\n            None,  # overlay (will be discussed further)\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },  # visualization parameters\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = (Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\"))))\n    img = next(imgs)  # extract first image from iterator\n    img.save(\n        fp=\"./output.gif\",\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output.gif\")"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#adding-context",
    "href": "notebooks/tutorials/gif-generation.html#adding-context",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "To provide more interesting or engaging data to the users, we can add temporal and geospatial context to the GIF. This is possible because API can return images in geo-referenced tif format.\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    filepath = generate_frame(items[0], france_aoi, tmpdirname, image_format=\"tif\")\n\n    # Verify that the tif returned by the API is correctly georeferenced\n    georeferenced_raster_data = rasterio.open(filepath)\n\n    print(\"Data bounds: \", georeferenced_raster_data.bounds)\n    print(\"Data CRS: \", georeferenced_raster_data.crs)\n\n\n\nIn order to overlay GeoJSON over the raster, we will have to convert the geojson boundaries to a raster format. We do this with the following steps:\nFor each feature in the geojson we rasterize the feature into a mask. We use binary dialation to detect the edges of the mask, and set the values corresponding to the mask edges to 255. This approach has one known problem: if multiple features share a border (eg: two adjoining provinces) the border between then will be detected twice, once from each side (or from each feature sharing that border). This means that internal borders will be twice as thick as external borders\n\nfrom gif_generation_dependencies.helper_functions import overlay_geojson\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        geojson = json.loads(f.read())\n\n    filepath = generate_frame(\n        items[0],\n        france_aoi,\n        tmpdirname,\n        image_format=\"tif\",\n        additional_cog_crop_args={\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n    )\n\n    filepath = overlay_geojson(filepath, geojson)\n    rasterio.plot.show(rasterio.open(filepath))\n\n\n\n\nAnother way to contextualize where in the GIF’s data is, is by overlaying the GIF on top of a base map. This process is a bit more complicated: - Generate a raster image (.tif) - Overlay in on a folium map interface - Save the map interface to html - Open the html file with a headless chrome webdriver (using the selenium library) - Save a screenshot of the rendered html as a .png\n\nfrom gif_generation_dependencies.helper_functions import overlay_raster_on_folium\n\ntmpdirname = tempfile.TemporaryDirectory()\n\nimage_filepath = generate_frame(\n    items[0],\n    france_aoi,\n    tmpdirname.name,\n    image_format=\"tif\",\n    overlay=None,\n    additional_cog_crop_args={\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n)\n\nimage_filepath = overlay_raster_on_folium(image_filepath)\n\ndisplay.Image(filename=image_filepath)\n\n\n\n\nNow that we have the raster data displayed over the basemap, we want to add the date of each file\n\nfrom gif_generation_dependencies.helper_functions import overlay_date\n\ndate = items[0][\"properties\"][\"start_datetime\"]\n\n# get datestring from STAC Item properties and reformat\ndatestring = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\").date().isoformat()\n\n# Reuse the raster overlayed on the OSM basemap using folium from above:\noverlay_date(image_filepath, datestring)\n\ndisplay.Image(filename=image_filepath)"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "href": "notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "I’ve combined all of the above functionality, along with a few helper functions in the file: ./gif_generation_dependencies/helper_functions.py\nI’ve also added the contextualizaiton steps (overlaying geojson, date, and folium basemap) directly into the generate_frame() method"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "href": "notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "with tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        overlay = json.loads(f.read())\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            geojson,\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.tif\")))]\n    imgs[0].save(\n        fp=\"./output_with_geojson.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_geojson.gif\")"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "href": "notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "with tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            \"folium\",\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    # Note: I'm searching for `*.png` files instead of *.tif files because the webdriver screenshot\n    # of the folium map interface is exported in png format (this also helps reduce the size of\n    # the final gif )\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\")))]\n    imgs[0].save(\n        fp=\"./output_with_osm_basemap.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_osm_basemap.gif\")"
  },
  {
    "objectID": "notebooks/tutorials/gif-generation.html#cleanup",
    "href": "notebooks/tutorials/gif-generation.html#cleanup",
    "title": "GIF generation using the TiTiler /cog/crop endpoint",
    "section": "",
    "text": "Run the following cell to remove the following generated images/gifs: - output.gif - output_with_geojson.gif - output_with_osm_basemap.gif\n\nfor f in glob.glob(os.path.join(\".\", \"output*.gif\")):\n    os.remove(f)"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html",
    "href": "notebooks/tutorials/mapping_fires.html",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#run-this-notebook",
    "href": "notebooks/tutorials/mapping_fires.html#run-this-notebook",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#approach",
    "href": "notebooks/tutorials/mapping_fires.html#approach",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Approach",
    "text": "Approach\n\nUse OWSLib to determine what data is available and inspect the metadata\nUse OWSLib to filter and read the data\nUse geopandas and folium to analyze and plot the data\n\nNote that the default examples environment is missing one requirement: oswlib. We can pip install that before we move on.\n\n!pip install OWSLib==0.28.1\n\nRequirement already satisfied: OWSLib==0.28.1 in /opt/conda/lib/python3.7/site-packages (0.28.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (5.4.1)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (4.9.2)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2021.1)\nRequirement already satisfied: python-dateutil&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2.8.2)\nRequirement already satisfied: requests&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2.24.0)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=1.5-&gt;OWSLib==0.28.1) (1.15.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (2022.9.24)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (3.0.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (1.25.11)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.3; however, version 23.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\n\n\n\nfrom owslib.ogcapi.features import Features\nimport geopandas as gpd\nimport datetime as dt\nfrom datetime import datetime, timedelta"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#about-the-data",
    "href": "notebooks/tutorials/mapping_fires.html#about-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "About the Data",
    "text": "About the Data\nThe fire data shown is generated by the FEDs algorithm. The FEDs algorithm tracks fire movement and severity by ingesting observations from the VIIRS thermal sensors on the Suomi NPP and NOAA-20 satellites. This algorithm uses raw VIIRS observations to generate a polygon of the fire, locations of the active fire line, and estimates of fire mean Fire Radiative Power (FRP). The VIIRS sensors overpass at ~1:30 AM and PM local time, and provide estimates of fire evolution ~ every 12 hours. The data produced by this algorithm describe where fires are in space and how fires evolve through time. This CONUS-wide implementation of the FEDs algorithm is based on Chen et al 2020’s algorithm for California.\nThe data produced by this algorithm is considered experimental."
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "href": "notebooks/tutorials/mapping_fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Look at the data that is availible through the OGC API",
    "text": "Look at the data that is availible through the OGC API\nThe datasets that are distributed throught the OGC API are organized into collections. We can display the collections with the command:\n\nOGC_URL = \"https://firenrt.delta-backend.com\"\n\nw = Features(url=OGC_URL)\nw.feature_collections()\n\n['public.eis_fire_lf_nfplist_nrt',\n 'public.eis_fire_snapshot_newfirepix_nrt',\n 'public.eis_fire_newfirepix',\n 'public.eis_fire_lf_fireline_nrt',\n 'public.eis_fire_lf_perimeter_archive',\n 'public.eis_fire_lf_fireline_archive',\n 'public.eis_fire_fireline',\n 'public.eis_fire_lf_newfirepix_archive',\n 'public.eis_fire_perimeter',\n 'public.eis_fire_lf_nfplist_archive',\n 'public.eis_fire_snapshot_perimeter_nrt',\n 'public.eis_fire_snapshot_fireline_nrt',\n 'public.eis_fire_lf_perimeter_nrt',\n 'public.eis_fire_lf_newfirepix_nrt',\n 'public.st_squaregrid',\n 'public.st_hexagongrid',\n 'public.st_subdivide']\n\n\nWe will focus on the public.eis_fire_snapshot_fireline_nrt collection, the public.eis_fire_snapshot_perimeter_nrt collection, and the public.eis_fire_lf_perimeter_archive collection here.\n\nInspect the metatdata for public.eis_fire_snapshot_perimeter_nrt collection\nWe can access information that drescribes the \"public.eis_fire_snapshot_perimeter_nrt.\n\nperm = w.collection(\"public.eis_fire_snapshot_perimeter_nrt\")\n\nWe are particularly interested in the spatial and temporal extents of the data.\n\nperm[\"extent\"]\n\n{'spatial': {'bbox': [[-124.61687469482422,\n    24.069549560546875,\n    -63.63948059082031,\n    49.40034866333008]],\n  'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84'},\n 'temporal': {'interval': [['2023-05-03T00:00:00+00:00',\n    '2023-05-23T00:00:00+00:00']],\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian'}}\n\n\nIn addition to getting metadata about the data we can access the queryable fields. Each of these fields will represent a column in our dataframe.\n\nperm_q = w.collection_queryables(\"public.eis_fire_snapshot_perimeter_nrt\")\nperm_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'duration': {'name': 'duration', 'type': 'number'},\n 'farea': {'name': 'farea', 'type': 'number'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'flinelen': {'name': 'flinelen', 'type': 'number'},\n 'fperim': {'name': 'fperim', 'type': 'number'},\n 'isactive': {'name': 'isactive', 'type': 'number'},\n 'meanfrp': {'name': 'meanfrp', 'type': 'number'},\n 'n_newpixels': {'name': 'n_newpixels', 'type': 'number'},\n 'n_pixels': {'name': 'n_pixels', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 'pixden': {'name': 'pixden', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#filter-the-data",
    "href": "notebooks/tutorials/mapping_fires.html#filter-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Filter the data",
    "text": "Filter the data\nIt is always a good idea to do any data filtering as early as possible. In this example we know that we want the data for particular spatial and temporal extents. We can apply those and other filters using the OWSLib package.\nIn the below example we are:\n\nchoosing the public.eis_fire_snapshot_perimeter_nrt collection\nsubsetting it by space using the bbox parameter\nsubsetting it by time using the datetime parameter\nfiltering for fires over 5km^2 and over 2 days long using the filter parameter. The filter parameter lets us filter by the columns in ‘public.eis_fire_snapshot_perimeter_nrt’ using SQL-style queries.\n\nNOTE: The limit parameter desginates the maximum number of objects the query will return. The default limit is 10, so if we want to all of the fire perimeters within certain conditions, we need to make sure that the limit is large.\n\n## Get the most recent fire perimeters, and 7 days before most recent fire perimeter\nmost_recent_time = max(*perm[\"extent\"][\"temporal\"][\"interval\"])\nnow = dt.datetime.strptime(most_recent_time, \"%Y-%m-%dT%H:%M:%S+00:00\")\nlast_week = now - dt.timedelta(weeks=1)\nlast_week = dt.datetime.strftime(last_week, \"%Y-%m-%dT%H:%M:%S+00:00\")\nprint(\"Most Recent Time =\", most_recent_time)\nprint(\"Last week =\", last_week)\n\nMost Recent Time = 2023-05-23T00:00:00+00:00\nLast week = 2023-05-16T00:00:00+00:00\n\n\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",  # name of the dataset we want\n    bbox=[\"-106.8\", \"24.5\", \"-72.9\", \"37.3\"],  # coodrinates of bounding box,\n    datetime=[last_week + \"/\" + most_recent_time],  # date range\n    limit=1000,  # max number of items returned\n    filter=\"farea&gt;5 AND duration&gt;2\",  # additional filters based on queryable fields\n)\n\nThe result is a dictionary containing all of the data and some summary fields. We can look at the keys to see what all is in there.\n\nperm_results.keys()\n\ndict_keys(['type', 'id', 'title', 'description', 'numberMatched', 'numberReturned', 'links', 'features'])\n\n\nFor instance you can check the total number of matched items and make sure that it is equal to the number of returned items. This is how you know that the limit you defined above is high enough.\n\nperm_results[\"numberMatched\"] == perm_results[\"numberReturned\"]\n\nTrue\n\n\nYou can also access the data directly in the browser or in an HTTP GET call using the constructed link.\n\nperm_results[\"links\"][1][\"href\"]\n\n'https://firenrt.delta-backend.com/collections/public.eis_fire_snapshot_perimeter_nrt/items?bbox=-106.8%2C24.5%2C-72.9%2C37.3&datetime=2023-05-16T00%3A00%3A00%2B00%3A00%2F2023-05-23T00%3A00%3A00%2B00%3A00&limit=1000&filter=farea%3E5+AND+duration%3E2'"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#read-data",
    "href": "notebooks/tutorials/mapping_fires.html#read-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Read data",
    "text": "Read data\nIn addition to all the summary fields, the perm_results dict contains all the data. We can pass the data into geopandas to make it easier to interact with.\n\ndf = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\ndf\n\n\n\n\n\n\n\n\ngeometry\nduration\nfarea\nfireid\nflinelen\nfperim\nisactive\nmeanfrp\nn_newpixels\nn_pixels\nogc_fid\npixden\nt\n\n\n\n\n0\nPOLYGON ((-78.05380 25.07529, -78.05382 25.075...\n8.5\n6.787331\n63349\n0.0\n10.125732\n1\n0.0\n0\n32\n2\n4.714666\n2023-05-19T00:00:00\n\n\n1\nPOLYGON ((-81.06449 25.95495, -81.06448 25.954...\n16.5\n81.413491\n59462\n0.0\n37.958965\n1\n0.0\n0\n544\n21\n6.681939\n2023-05-18T12:00:00\n\n\n2\nPOLYGON ((-81.66818 29.24672, -81.66808 29.246...\n4.0\n7.789717\n65940\n0.0\n12.359742\n1\n0.0\n0\n66\n180\n8.472709\n2023-05-21T12:00:00\n\n\n3\nPOLYGON ((-85.52621 30.03404, -85.52414 30.037...\n8.0\n12.906472\n64561\n0.0\n17.488802\n1\n0.0\n0\n127\n585\n9.840024\n2023-05-22T12:00:00\n\n\n4\nMULTIPOLYGON (((-84.18103 30.73800, -84.18102 ...\n80.5\n594.630290\n23106\n0.0\n391.297316\n1\n0.0\n0\n1553\n2307\n2.611707\n2023-05-21T00:00:00\n\n\n5\nMULTIPOLYGON (((-103.52577 32.22440, -103.5257...\n17.0\n10.940001\n61561\n0.0\n19.953733\n1\n0.0\n0\n22\n6756\n2.010969\n2023-05-22T00:00:00\n\n\n6\nPOLYGON ((-104.08174 32.22901, -104.08175 32.2...\n11.0\n15.296388\n62322\n0.0\n17.629951\n1\n0.0\n0\n8\n6802\n0.522999\n2023-05-19T00:00:00\n\n\n7\nPOLYGON ((-106.30889 25.15346, -106.30888 25.1...\n6.0\n7.757181\n63955\n0.0\n11.384919\n1\n0.0\n0\n92\n7851\n11.859979\n2023-05-18T12:00:00\n\n\n8\nPOLYGON ((-106.30553 24.99012, -106.30554 24.9...\n8.5\n16.009259\n62262\n0.0\n19.050446\n0\n0.0\n0\n144\n7862\n8.994795\n2023-05-16T00:00:00"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#explore-data",
    "href": "notebooks/tutorials/mapping_fires.html#explore-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Explore data",
    "text": "Explore data\nWe can quickly explore the data by setting the coordinate reference system (crs) and using .explore()\n\ndf = df.set_crs(\"EPSG:4326\")\ndf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "href": "notebooks/tutorials/mapping_fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize Most Recent Fire Perimeters with Firelines",
    "text": "Visualize Most Recent Fire Perimeters with Firelines\nIf we wanted to combine collections to make more informative analyses, we can use some of the same principles.\nFirst we’ll get the queryable fields, and the extents:\n\nfline_q = w.collection_queryables(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_collection = w.collection(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'mergeid': {'name': 'mergeid', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}\n\n\n\nRead\nThen we’ll use those fields to get most recent fire perimeters and fire lines.\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",\n    datetime=most_recent_time,\n    limit=1000,\n)\nperimeters = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\n\n## Get the most recent fire lines\nperimeter_ids = perimeters.fireid.unique()\nperimeter_ids = \",\".join(map(str, perimeter_ids))\n\nfline_results = w.collection_items(\n    \"public.eis_fire_snapshot_fireline_nrt\",\n    limit=1000,\n    filter=\"fireid IN (\"\n    + perimeter_ids\n    + \")\",  # only the fires from the fire perimeter query above\n)\nfline = gpd.GeoDataFrame.from_features(fline_results[\"features\"])\n\n\n\nVisualize\n\nperimeters = perimeters.set_crs(\"epsg:4326\")\nfline = fline.set_crs(\"epsg:4326\")\n\nm = perimeters.explore()\nm = fline.explore(m=m, color=\"orange\")\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#visualize-the-growth-of-the-camp-fire",
    "href": "notebooks/tutorials/mapping_fires.html#visualize-the-growth-of-the-camp-fire",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize the Growth of the Camp Fire",
    "text": "Visualize the Growth of the Camp Fire\nWe may be interested in understanding how a fire evolved through time. To do this, we can work with the “Large fire” or “lf” perimeter collections. The public.eis_fire_lf_perimeter_nrt colelction has the full spread history of fires from this year. public.eis_fire_lf_perimeter_archive has the full spread history of fires from 2018-2021 that were in the Western United States. The Camp Fire was in 2018, so we will work with the public.eis_fire_lf_perimeter_archive collection.\nWe can start by querying with information specific to the Camp Fire, like it’s genreal region (Northern California), and when it was active (November 2018).\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",  \n    bbox=[\"-124.52\", \"39.2\", \"-120\", \"42\"],  # North California bounding box,\n    datetime=[\"2018-11-01T00:00:00+00:00/2018-11-30T12:00:00+00:00\"], \n    limit=3000, \n    filter=\"duration&gt;2\",  # additional filters based on queryable fields\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by = \"t\", ascending = False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\n\nm = perimeters.explore(style_kwds = {'fillOpacity':0})\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#download-data",
    "href": "notebooks/tutorials/mapping_fires.html#download-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Download Data",
    "text": "Download Data\nDownloading pre-filtered data may be useful for working locally, or for working with the data in GIS software.\nWe can download the dataframe we made by writing it out into a shapefile or into a GeoJSON file.\nperimeters.to_file('perimeters.shp') \nperimeters.to_file('perimeters.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "notebooks/tutorials/mapping_fires.html#collection-information",
    "href": "notebooks/tutorials/mapping_fires.html#collection-information",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Collection Information",
    "text": "Collection Information\nThe API hosts 9 different collections. There are four different types of data, and three different time-scales availible for querying through the API. “*snapshot*” collections are useful for visualizing the most recent data. It contains the most recent fires perimeters, active firelines, or VIIRS observations within the last 20 days. “*lf*” collections (short for Large Fire), show every fire perimeter, active fire line, or VIIRS observations for fires over 5 km^2. Collections that end in *archive are for year 2018 - 2021 across the Western United States. Collections with the *nrt ending are for CONUS from this most recent year. FireIDs are consistent only between layers with the same timescale (snapshot, lf_*nrt, and lf_archive*).\npublic.eis_fire_snapshot_perimeter_nrt\nPerimeter of cumulative fire-area. Most recent perimeter from the last 20 days.\npublic.eis_fire_lf_perimeter_nrt\nPerimeter of cumulative fire-area, from fires over 5 km^2. Every fire perimeter from current year to date.\npublic.eis_fire_lf_perimeter_archive\nPerimeter of cumulative fire-area, from fires over 5 km^2 in the Western United States. Every fire perimeter from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nmeanfrp\nMean fire radiative power. The weighted sum of the fire radiative power detected at each new pixel, divided by the number of pixels. If no new pixels are detected, meanfrp is set to zero.\nMW/(pixel area)\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\npixden\nNumber of pixels divided by area of perimeter.\npixels/Km^2\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nKm\n\n\nfperim\nLength of fire perimeter.\nKm\n\n\nfarea\nArea within fire perimeter.\nKm^2\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\nn_pixels\nNumber of pixel-detections in history of fire.\npixels\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\ngeometry\nThe shape of the perimeter.\nGeometry\n\n\n\npublic.eis_fire_snapshot_fireline_nrt\nActive fire line as estimated by new VIIRS detections. Most fire line from the last 20 days.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2. Every fire line from current year to date.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2 in the Western United States. Every fire line from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID\n\n\n\npublic.eis_fire_snapshot_newfirepix_nrt\nNew pixel detections that inform the most recent time-step’s perimeter and fireline calculation from the last 20 days.\npublic.eis_fire_lf_newfirepix_nrt\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible from start of current year to date.\npublic.eis_fire_lf_newfirepix_archive\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible for Western United States from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID"
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Approach",
    "text": "Approach\nCloud-optimized GeoTIFF (COG) is a geospatial raster (image) data format optimized for on-the-fly analytics and visualization of raster data in cloud applications.\nConverting NetCDF (climate data) to COG can be relevant when the data should be included in GIS or web map applications.\nThis tutorial shows how this conversion can be done using Xarray and rioxarray, in-memory, avoiding temporary files on-disk.\n\nStep-by-step guide to conversion from NetCDF to Cloud-Optimized GeoTIFF\nCombined workflow including upload to S3"
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Step by step",
    "text": "Step by step\n\nStep 0 - Installs and imports\n\n!pip install s3fs h5netcdf --quiet\n\n\nimport boto3\nimport s3fs\nimport rioxarray\nimport rasterio\nimport rio_cogeo.cogeo\nimport xarray as xr\nfrom rasterio.io import MemoryFile\n\n\n\nStep 1 - Inspect source NetCDF\n\nSOURCE_URI = 'cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc'\n\n\nfs = s3fs.S3FileSystem()\nfileobj = fs.open(SOURCE_URI)\nds = xr.open_dataset(fileobj, engine=\"h5netcdf\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lat: 600, lon: 1440)\nCoordinates:\n  * lat      (lat) float64 -59.88 -59.62 -59.38 -59.12 ... 89.38 89.62 89.88\n  * lon      (lon) float64 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\nData variables: (12/13)\n    FFMC     (lat, lon) float32 ...\n    DMC      (lat, lon) float32 ...\n    DC       (lat, lon) float32 ...\n    ISI      (lat, lon) float32 ...\n    BUI      (lat, lon) float32 ...\n    FWI      (lat, lon) float32 ...\n    ...       ...\n    FWI_N30  (lat, lon) uint16 ...\n    FWI_N45  (lat, lon) uint16 ...\n    FWI_P25  (lat, lon) float32 ...\n    FWI_P50  (lat, lon) float32 ...\n    FWI_P75  (lat, lon) float32 ...\n    FWI_P95  (lat, lon) float32 ...xarray.DatasetDimensions:lat: 600lon: 1440Coordinates: (2)lat(lat)float64-59.88 -59.62 ... 89.62 89.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([-59.875, -59.625, -59.375, ...,  89.375,  89.625,  89.875])lon(lon)float640.125 0.375 0.625 ... 359.6 359.9units :degrees_eaststandard_name :longitudelong_name :longitudeaxis :Xarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02])Data variables: (13)FFMC(lat, lon)float32...[864000 values with dtype=float32]DMC(lat, lon)float32...[864000 values with dtype=float32]DC(lat, lon)float32...[864000 values with dtype=float32]ISI(lat, lon)float32...[864000 values with dtype=float32]BUI(lat, lon)float32...[864000 values with dtype=float32]FWI(lat, lon)float32...[864000 values with dtype=float32]FWI_N15(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N30(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N45(lat, lon)uint16...[864000 values with dtype=uint16]FWI_P25(lat, lon)float32...[864000 values with dtype=float32]FWI_P50(lat, lon)float32...[864000 values with dtype=float32]FWI_P75(lat, lon)float32...[864000 values with dtype=float32]FWI_P95(lat, lon)float32...[864000 values with dtype=float32]Indexes: (2)latPandasIndexPandasIndex(Float64Index([-59.875, -59.625, -59.375, -59.125, -58.875, -58.625, -58.375,\n              -58.125, -57.875, -57.625,\n              ...\n               87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,\n               89.375,  89.625,  89.875],\n             dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Float64Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,\n                1.875,   2.125,   2.375,\n              ...\n              357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125,\n              359.375, 359.625, 359.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\n\n\nStep 2 - Select data variable\nThe NetCDF contains several data variables. We pick the first one for demo.\n\nVARIABLE_NAME = \"FFMC\"\n\n\nda = ds[VARIABLE_NAME]\n\n\nda.plot();\n\n\n\n\n\nda.encoding\n\n{'chunksizes': None,\n 'fletcher32': False,\n 'shuffle': False,\n 'source': '&lt;File-like object S3FileSystem, cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc&gt;',\n 'original_shape': (600, 1440),\n 'dtype': dtype('&lt;f4'),\n '_FillValue': nan}\n\n\n\n\nStep 3. Conform to raster data conventions\nCommon practice in NetCDF lat/lon data the first grid cell is the south-west corner, i.e. latitude and longitude axes increase along the array dimensions.\nCommon practice in raster formats like GeoTIFF is that the y-axis (latitude in this case) decreases from origin, i.e. the first pixel is the north-west corner.\nWe can reverse the latitude dimension like this:\n\nda = da.isel(lat=slice(None, None, -1))\nda.lat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lat' (lat: 600)&gt;\narray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])\nCoordinates:\n  * lat      (lat) float64 89.88 89.62 89.38 89.12 ... -59.38 -59.62 -59.88\nAttributes:\n    units:          degrees_north\n    standard_name:  latitude\n    long_name:      latitude\n    axis:           Yxarray.DataArray'lat'lat: 60089.88 89.62 89.38 89.12 88.88 ... -58.88 -59.12 -59.38 -59.62 -59.88array([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Coordinates: (1)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Indexes: (1)latPandasIndexPandasIndex(Float64Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,\n               88.125,  87.875,  87.625,\n              ...\n              -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125,\n              -59.375, -59.625, -59.875],\n             dtype='float64', name='lat', length=600))Attributes: (4)units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Y\n\n\nWe would also like the longitude axis to range from -180 to 180 degrees east, instead of 0 to 360 degrees east (matter of taste, not convention).\n\nda = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\nda.lon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lon' (lon: 1440)&gt;\narray([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])\nCoordinates:\n  * lon      (lon) float64 -179.9 -179.6 -179.4 -179.1 ... 179.4 179.6 179.9xarray.DataArray'lon'lon: 1440-179.9 -179.6 -179.4 -179.1 -178.9 ... 178.9 179.1 179.4 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Coordinates: (1)lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Indexes: (1)lonPandasIndexPandasIndex(Float64Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625,\n              -178.375, -178.125, -177.875, -177.625,\n              ...\n               177.625,  177.875,  178.125,  178.375,  178.625,  178.875,\n               179.125,  179.375,  179.625,  179.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck that the data still looks right, just rotated along x-axis:\n\nda.plot();\n\n\n\n\nNow we need to set raster data attributes which are missing from the NetCDF, to help rio-xarray infer the raster information\n\nda.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\nda.rio.write_crs(\"epsg:4326\", inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'FFMC' (lat: 600, lon: 1440)&gt;\n[864000 values with dtype=float32]\nCoordinates:\n  * lat          (lat) float64 89.88 89.62 89.38 89.12 ... -59.38 -59.62 -59.88\n  * lon          (lon) float64 -179.9 -179.6 -179.4 -179.1 ... 179.4 179.6 179.9\n    spatial_ref  int64 0xarray.DataArray'FFMC'lat: 600lon: 1440...[864000 values with dtype=float32]Coordinates: (3)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Indexes: (2)latPandasIndexPandasIndex(Float64Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,\n               88.125,  87.875,  87.625,\n              ...\n              -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125,\n              -59.375, -59.625, -59.875],\n             dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Float64Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625,\n              -178.375, -178.125, -177.875, -177.625,\n              ...\n               177.625,  177.875,  178.125,  178.375,  178.625,  178.875,\n               179.125,  179.375,  179.625,  179.875],\n             dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck CRS\n\nda.rio.crs\n\nCRS.from_epsg(4326)\n\n\nCheck affine image transform:\na = width of a pixel\nb = row rotation (typically zero)\nc = x-coordinate of the upper-left corner of the upper-left pixel\nd = column rotation (typically zero)\ne = height of a pixel (typically negative)\nf = y-coordinate of the of the upper-left corner of the upper-left pixel\n\nda.rio.transform()\n\nAffine(0.25, 0.0, -180.0,\n       0.0, -0.25, 90.0)\n\n\n\n\nStep 4 - Write to COG and validate in-memory\nFor the demonstration here, we do not write the file to disk but to a memory file which can be validated and uploaded in-memory\nGeoTIFFs / COGs can be tuned for performance. Here are some defaults we found to work well (check out this blog post for detail).\n\nCOG_PROFILE = {\n    \"driver\": \"COG\",\n    \"compress\": \"DEFLATE\",\n    \"predictor\": 2\n}\n\n\nwith MemoryFile() as memfile:\n    da.rio.to_raster(memfile.name, **COG_PROFILE)\n    \n    cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n\ncog_valid\n\nTrue"
  },
  {
    "objectID": "notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "href": "notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Combined workflow from conversion to upload",
    "text": "Combined workflow from conversion to upload\n\nSOURCE_URI = 'cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc'\n\n\nVARIABLE_NAME = \"FFMC\"\n\n\nCOG_PROFILE = {\n    \"driver\": \"COG\",\n    \"compress\": \"DEFLATE\",\n    \"predictor\": 2\n}\n\n\nDESTINATION_BUCKET = None\nDESTINATION_KEY = None\n\n\nfs = s3fs.S3FileSystem()\nwith fs.open(SOURCE_URI) as fileobj:\n    with xr.open_dataset(fileobj, engine=\"h5netcdf\") as ds:\n\n        # Read individual metric into data array (only one time in yearly NetCDFs)\n        da = ds[VARIABLE_NAME]\n\n        # Realign the x dimension to -180 origin for dataset\n        da = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\n\n        # Reverse the DataArray's y dimension to comply with raster common practice\n        if da.lat.values[-1] &gt; da.lat.values[0]:\n            da = da.isel(lat=slice(None, None, -1))\n\n        # Set raster attributes\n        da.rio.set_spatial_dims(\"lon\", \"lat\")\n        da.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        with MemoryFile() as memfile:\n            da.rio.to_raster(memfile.name, **COG_PROFILE)\n\n            # Validate COG in-memory\n            cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n            if not cog_valid:\n                raise RuntimeError(\"COG validation failed.\")\n            \n            # Upload to S3\n            if DESTINATION_BUCKET is not None:\n                client = boto3.client(\"s3\")\n                r = client.put_object(\n                    Body=memfile,\n                    Bucket=DESTINATION_BUCKET,\n                    Key=DESTINATION_KEY,\n                )\n                if r[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n                    raise RuntimeError(\"Upload failed.\")"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html",
    "href": "notebooks/quickstarts/downsample-zarr.html",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "href": "notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#approach",
    "href": "notebooks/quickstarts/downsample-zarr.html#approach",
    "title": "Downsample zarr",
    "section": "Approach",
    "text": "Approach\nThis notebook demonstrates 2 strategies for to subselect data from a Zarr dataset in order to visualize using the memory of a notebook.\n\nDownsample the temporal resolution of the data using xarray.DataArray.resample\nCoarsening the spatial aspect of the data using xarray.DataArray.coarsen\n\nA strategy for visualizing any large amount of data is Datashader which bins data into a fixed 2-D array. The call to rasterize ensures the use of the datashader library to bin the data."
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "title": "Downsample zarr",
    "section": "About the data",
    "text": "About the data\nThe SMAP mission is an orbiting observatory that measures the amount of water in the surface soil everywhere on Earth."
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "href": "notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "title": "Downsample zarr",
    "section": "Load libraries",
    "text": "Load libraries\n\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray\nimport geoviews as gv\nimport datashader as dsh\nfrom holoviews.operation.datashader import rasterize\n\ngv.output(size=300)"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "href": "notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "title": "Downsample zarr",
    "section": "Optional: Create and Scale a Dask Cluster",
    "text": "Optional: Create and Scale a Dask Cluster\nWe create a separate Dask cluster to speed up reprojecting the data (and other potential computations which could be required and are parallelizable).\nNote if you skip this cell you will still be using Dask, you’ll just be using the machine where you are running this notebook.\n\nfrom dask_gateway import GatewayCluster, Gateway\n\ngateway = Gateway()\nclusters = gateway.list_clusters()\n\n# connect to an existing cluster - this is useful when the kernel shutdown in the middle of an interactive session\nif clusters:\n    cluster = gateway.connect(clusters[0].name)\nelse:\n    cluster = GatewayCluster(shutdown_on_close=True)\n\ncluster.scale(16)\nclient = cluster.get_client()\nclient"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "href": "notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "title": "Downsample zarr",
    "section": "Open the dataset from S3",
    "text": "Open the dataset from S3\n\ns3 = s3fs.S3FileSystem()\nroot = \"veda-data-store-staging/EIS/zarr/SPL3SMP.zarr\"\nstore = s3fs.S3Map(root=root, s3=s3)\nds = xr.open_zarr(store=store)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                        (northing_m: 406, easting_m: 964,\n                                    datetime: 1679)\nCoordinates:\n  * datetime                       (datetime) datetime64[ns] 2018-01-01 ... 2...\n  * easting_m                      (easting_m) float64 -1.735e+07 ... 1.735e+07\n  * northing_m                     (northing_m) float64 7.297e+06 ... -7.297e+06\nData variables: (12/26)\n    albedo                         (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    albedo_pm                      (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density                   (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density_pm                (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction                  (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction_pm               (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    ...                             ...\n    static_water_body_fraction     (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    static_water_body_fraction_pm  (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag                   (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag_pm                (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature            (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature_pm         (northing_m, easting_m, datetime) float32 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;xarray.DatasetDimensions:northing_m: 406easting_m: 964datetime: 1679Coordinates: (3)datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')easting_m(easting_m)float64-1.735e+07 -1.731e+07 ... 1.735e+07array([-17349514.34, -17313482.12, -17277449.9 , ...,  17277449.08,\n        17313481.3 ,  17349513.52])northing_m(northing_m)float647.297e+06 7.26e+06 ... -7.297e+06array([ 7296524.72,  7260492.5 ,  7224460.28, ..., -7224459.94, -7260492.16,\n       -7296524.38])Data variables: (26)albedo(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;coordinates :/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitudelong_name :Diffuse reflecting power of the Earth&apos;s surface used in DCA within the grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nalbedo_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nDiffuse reflecting power of the Earth&apos;s surface used in DCA retrievals within the grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nbulk_density\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of aggregated bulk_density within the 36 km grid cell.\n\nvalid_max :\n\n2.6500000953674316\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nbulk_density_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of aggregated bulk density within the 36 km grid cell.\n\nvalid_max :\n\n2.6500000953674316\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nclay_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nclay_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nfreeze_thaw_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nFraction of the 36 km grid cell that is denoted as frozen. Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nfreeze_thaw_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nFraction of the 36 km grid cell that is denoted as frozen. Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ngrid_surface_status\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nIndicates if the grid point lies on land (0) or water (1).\n\nvalid_max :\n\n1\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\ngrid_surface_status_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nIndicates if the grid point lies on land (0) or water (1).\n\nvalid_max :\n\n1\n\nvalid_min :\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nradar_water_body_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nradar_water_body_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nretrieval_qual_flag\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nflag_masks :\n\n1s, 2s, 4s, 8s\n\nflag_meanings :\n\nRetrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_success\n\nlong_name :\n\nBit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nretrieval_qual_flag_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nflag_masks :\n\n1s, 2s, 4s, 8s\n\nflag_meanings :\n\nRetrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_success\n\nlong_name :\n\nBit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nroughness_coefficient\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of bare soil roughness used in DCA within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nroughness_coefficient_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nA unitless value that is indicative of bare soil roughness used in DCA retrievals within the 36 km grid cell.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsoil_moisture\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nRepresentative DCA soil moisture measurement for the Earth based grid cell.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.5\n\nvalid_min :\n\n0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsoil_moisture_error\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nNet uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.20000000298023224\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsoil_moisture_error_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nNet uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.20000000298023224\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsoil_moisture_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nRepresentative DCA soil moisture measurement for the Earth based grid cell.\n\nunits :\n\ncm**3/cm**3\n\nvalid_max :\n\n0.5\n\nvalid_min :\n\n0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nstatic_water_body_fraction\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nstatic_water_body_fraction_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nThe fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.\n\nvalid_max :\n\n1.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsurface_flag\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nflag_masks :\n\n1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048s\n\nflag_meanings :\n\n36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radiometer_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_region\n\nlong_name :\n\nBit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsurface_flag_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nflag_masks :\n\n1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048s\n\nflag_meanings :\n\n36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radar_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_region\n\nlong_name :\n\nBit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsurface_temperature\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nTemperature at land surface based on GMAO GEOS-5 data.\n\nunits :\n\nKelvins\n\nvalid_max :\n\n350.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\nsurface_temperature_pm\n\n\n(northing_m, easting_m, datetime)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\n/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitude\n\nlong_name :\n\nTemperature at land surface based on GMAO GEOS-5 data.\n\nunits :\n\nKelvins\n\nvalid_max :\n\n350.0\n\nvalid_min :\n\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (0)\n\n\nSelect the variable of interest (soil moisture for this example).\n\nsoil_moisture = ds.soil_moisture\nsoil_moisture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (northing_m: 406, easting_m: 964,\n                                   datetime: 1679)&gt;\ndask.array&lt;open_dataset-e9352a6cb9ac62da20f04f798593159fsoil_moisture, shape=(406, 964, 1679), dtype=float32, chunksize=(100, 100, 100), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * datetime    (datetime) datetime64[ns] 2018-01-01 2018-01-02 ... 2022-09-09\n  * easting_m   (easting_m) float64 -1.735e+07 -1.731e+07 ... 1.735e+07\n  * northing_m  (northing_m) float64 7.297e+06 7.26e+06 ... -7.26e+06 -7.297e+06\nAttributes:\n    coordinates:  /Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Re...\n    long_name:    Representative DCA soil moisture measurement for the Earth ...\n    units:        cm**3/cm**3\n    valid_max:    0.5\n    valid_min:    0.019999999552965164xarray.DataArray'soil_moisture'northing_m: 406easting_m: 964datetime: 1679dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nCount\n851 Tasks\n850 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')easting_m(easting_m)float64-1.735e+07 -1.731e+07 ... 1.735e+07array([-17349514.34, -17313482.12, -17277449.9 , ...,  17277449.08,\n        17313481.3 ,  17349513.52])northing_m(northing_m)float647.297e+06 7.26e+06 ... -7.297e+06array([ 7296524.72,  7260492.5 ,  7224460.28, ..., -7224459.94, -7260492.16,\n       -7296524.38])Attributes: (5)coordinates :/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitudelong_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 1: Downsample the temporal resolution of the data",
    "text": "Strategy 1: Downsample the temporal resolution of the data\nTo plot one day from every month, resample the data to 1 observation a month.\n\nsomo_one_month = soil_moisture.resample(datetime=\"1M\").nearest()\n\n\nQuick plot\nWe can generate a quick plot using hvplot and datashader.\n\n# workaround to avoid warnings that are triggered within Dask.\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"All-NaN slice encountered\", category=RuntimeWarning\n)\n\n\nsomo_one_month.hvplot(\n    x=\"easting_m\",\n    y=\"northing_m\",\n    groupby=\"datetime\",\n    crs=\"epsg:6933\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    frame_height=150,\n    widget_location=\"bottom\",\n)\n\nWARNING:param.main: Calling the .opts method with options broken down by options group (i.e. separate plot, style and norm groups) is deprecated. Use the .options method converting to the simplified format instead or use hv.opts.apply_groups for backward compatibility.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nReproject before plotting\nReproject the data for map visualization.\n\nsomo_one_month = somo_one_month.transpose(\"datetime\", \"northing_m\", \"easting_m\")\nsomo_one_month = somo_one_month.rio.set_spatial_dims(\n    x_dim=\"easting_m\", y_dim=\"northing_m\"\n)\nsomo_one_month = somo_one_month.rio.write_crs(\"epsg:6933\")\nsomo_reprojected = somo_one_month.rio.reproject(\"EPSG:4326\")\nsomo_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 57, y: 1046, x: 2214)&gt;\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 -179.9 -179.8 -179.6 -179.4 ... 179.6 179.8 179.9\n  * y            (y) float64 84.96 84.8 84.64 84.48 ... -84.64 -84.8 -84.96\n  * datetime     (datetime) datetime64[ns] 2018-01-31 2018-02-28 ... 2022-09-30\n    spatial_ref  int64 0\nAttributes:\n    coordinates:  /Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Re...\n    long_name:    Representative DCA soil moisture measurement for the Earth ...\n    units:        cm**3/cm**3\n    valid_max:    0.5\n    valid_min:    0.019999999552965164xarray.DataArray'soil_moisture'datetime: 57y: 1046x: 2214nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.9 -179.8 ... 179.8 179.9axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.918425, -179.755825, -179.593224, ...,  179.591393,  179.753993,\n        179.916594])y(y)float6484.96 84.8 84.64 ... -84.8 -84.96axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.963262,  84.800655,  84.638047, ..., -84.636773, -84.79938 ,\n       -84.961988])datetime(datetime)datetime64[ns]2018-01-31 ... 2022-09-30array(['2018-01-31T00:00:00.000000000', '2018-02-28T00:00:00.000000000',\n       '2018-03-31T00:00:00.000000000', '2018-04-30T00:00:00.000000000',\n       '2018-05-31T00:00:00.000000000', '2018-06-30T00:00:00.000000000',\n       '2018-07-31T00:00:00.000000000', '2018-08-31T00:00:00.000000000',\n       '2018-09-30T00:00:00.000000000', '2018-10-31T00:00:00.000000000',\n       '2018-11-30T00:00:00.000000000', '2018-12-31T00:00:00.000000000',\n       '2019-01-31T00:00:00.000000000', '2019-02-28T00:00:00.000000000',\n       '2019-03-31T00:00:00.000000000', '2019-04-30T00:00:00.000000000',\n       '2019-05-31T00:00:00.000000000', '2019-06-30T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-31T00:00:00.000000000',\n       '2019-09-30T00:00:00.000000000', '2019-10-31T00:00:00.000000000',\n       '2019-11-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000',\n       '2020-01-31T00:00:00.000000000', '2020-02-29T00:00:00.000000000',\n       '2020-03-31T00:00:00.000000000', '2020-04-30T00:00:00.000000000',\n       '2020-05-31T00:00:00.000000000', '2020-06-30T00:00:00.000000000',\n       '2020-07-31T00:00:00.000000000', '2020-08-31T00:00:00.000000000',\n       '2020-09-30T00:00:00.000000000', '2020-10-31T00:00:00.000000000',\n       '2020-11-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000',\n       '2021-01-31T00:00:00.000000000', '2021-02-28T00:00:00.000000000',\n       '2021-03-31T00:00:00.000000000', '2021-04-30T00:00:00.000000000',\n       '2021-05-31T00:00:00.000000000', '2021-06-30T00:00:00.000000000',\n       '2021-07-31T00:00:00.000000000', '2021-08-31T00:00:00.000000000',\n       '2021-09-30T00:00:00.000000000', '2021-10-31T00:00:00.000000000',\n       '2021-11-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000',\n       '2022-01-31T00:00:00.000000000', '2022-02-28T00:00:00.000000000',\n       '2022-03-31T00:00:00.000000000', '2022-04-30T00:00:00.000000000',\n       '2022-05-31T00:00:00.000000000', '2022-06-30T00:00:00.000000000',\n       '2022-07-31T00:00:00.000000000', '2022-08-31T00:00:00.000000000',\n       '2022-09-30T00:00:00.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.1626005508861423 0.0 85.04456635019852 0.0 -0.16260789541619725array(0)Attributes: (5)coordinates :/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitudelong_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164\n\n\nNote that this is now a fully materialized data array - when we reproject we trigger an implicit compute.\nCreate a geoviews dataset and visualize the data on a map.\n\nkdims = [\"datetime\", \"x\", \"y\"]\nvdims = [\"soil_moisture\"]\nxr_dataset = gv.Dataset(somo_reprojected, kdims=kdims, vdims=vdims)\nimages = xr_dataset.to(gv.Image, [\"x\", \"y\"])\n\nrasterize(\n    images, precompute=True, aggregator=dsh.mean(\"soil_moisture\")\n) * gv.feature.coastline"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "href": "notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 2: Coarsen spatial resolution of the data",
    "text": "Strategy 2: Coarsen spatial resolution of the data\nBelow, we coarsen the spatial resolution of the data by a factor of 4 in the x and 2 in the y. These values were chosen because they can be used with the exact boundary argument as the dimensions size is a multiple of these values.\nYou can also coarsen by datetime, using the same strategy as below but replacing easting_m and northing_m with datetime. If {datetime: n} is the value give to the dim argument, this would create a mean of the soil moisture average for n days.\nOnce the data has been coarsned, again it is reprojected for map visualization and then visualized using Geoviews.\n\ncoarsened = soil_moisture.coarsen(dim={\"easting_m\": 4, \"northing_m\": 2}).mean()\n\ncoarsened = coarsened.transpose(\"datetime\", \"northing_m\", \"easting_m\")\ncoarsened = coarsened.rio.set_spatial_dims(x_dim=\"easting_m\", y_dim=\"northing_m\")\ncoarsened = coarsened.rio.write_crs(\"epsg:6933\")\ncoarsened_reprojected = coarsened.rio.reproject(\"EPSG:4326\")\ncoarsened_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 1679, y: 315, x: 667)&gt;\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 -179.7 -179.2 -178.7 -178.1 ... 178.6 179.2 179.7\n  * y            (y) float64 84.77 84.23 83.7 83.16 ... -83.64 -84.18 -84.72\n  * datetime     (datetime) datetime64[ns] 2018-01-01 2018-01-02 ... 2022-09-09\n    spatial_ref  int64 0\nAttributes:\n    coordinates:  /Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Re...\n    long_name:    Representative DCA soil moisture measurement for the Earth ...\n    units:        cm**3/cm**3\n    valid_max:    0.5\n    valid_min:    0.019999999552965164\n    _FillValue:   3.402823466e+38xarray.DataArray'soil_moisture'datetime: 1679y: 315x: 667nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.7 -179.2 ... 179.2 179.7axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.729872, -179.190164, -178.650456, ...,  178.636044,  179.175751,\n        179.715459])y(y)float6484.77 84.23 83.7 ... -84.18 -84.72axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.774672,  84.234883,  83.695095, ..., -83.639381, -84.17917 ,\n       -84.718958])datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.5397077035841558 0.0 85.04456635019838 0.0 -0.5397886314149526array(0)Attributes: (6)coordinates :/Soil_Moisture_Retrieval_Data_AM/latitude /Soil_Moisture_Retrieval_Data_AM/longitudelong_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164_FillValue :3.402823466e+38\n\n\n\nkdims = [\"datetime\", \"x\", \"y\"]\nvdims = [\"soil_moisture\"]\nxr_dataset = gv.Dataset(coarsened_reprojected, kdims=kdims, vdims=vdims)\nimages = xr_dataset.to(gv.Image, [\"x\", \"y\"])\n\n\nrasterize(\n    images, precompute=True, aggregator=dsh.mean(\"soil_moisture\")\n) * gv.feature.coastline"
  },
  {
    "objectID": "notebooks/quickstarts/downsample-zarr.html#cleanup",
    "href": "notebooks/quickstarts/downsample-zarr.html#cleanup",
    "title": "Downsample zarr",
    "section": "Cleanup",
    "text": "Cleanup\nWhen using a remote Dask cluster it is recommented to explicitly close the cluster.\nclient.shutdown()"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "title": "Calculate timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items cropped to AOI\nCalculate the mean for each timestep over the AOI\n\n\nfrom pystac_client import Client\nimport stackstac\nimport rioxarray  # noqa"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection = \"no2-monthly\""
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nchina_bbox = [\n    73.675,\n    18.198,\n    135.026,\n    53.459,\n]\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\n\nsearch = catalog.search(\n    bbox=china_bbox, datetime=datetime, collections=[collection], limit=1000\n)\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\nFound 73 items"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "title": "Calculate timeseries from COGs",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataSet using stackstac\n\n# This is a workaround that is planning to move up into stackstac itself\nimport rasterio as rio\nimport boto3\n\nsession = rio.session.AWSSession(boto3.Session())\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(session=rio.session.AWSSession(boto3.Session()))\n)\n\n\nda = stackstac.stack(search.get_all_items(), gdal_env=gdal_env)\nda = da.assign_coords({\"time\": da.start_datetime})\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-0683303cbb3f9689a8e6286f8a3ad3a0' (time: 73,\n                                                                band: 1,\n                                                                y: 1800, x: 3600)&gt;\ndask.array&lt;fetch_raster_window, shape=(73, 1, 1800, 3600), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/15)\n  * time            (time) &lt;U19 '2022-01-01T00:00:00' ... '2016-01-01T00:00:00'\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ... '...\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    ...              ...\n    end_datetime    (time) &lt;U19 '2022-01-31T00:00:00' ... '2016-01-31T00:00:00'\n    proj:epsg       float64 4.326e+03\n    proj:bbox       object {180.0, -180.0, 90.0, -90.0}\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    title           &lt;U17 'Default COG Layer'\n    epsg            float64 4.326e+03\nAttributes:\n    spec:        RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0),...\n    crs:         epsg:4326.0\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-0683303cbb3f9689a8e6286f8a3ad3a0'time: 73band: 1y: 1800x: 3600dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.52 GiB\n8.00 MiB\n\n\nShape\n(73, 1, 1800, 3600)\n(1, 1, 1024, 1024)\n\n\nCount\n730 Tasks\n584 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (15)time(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00', '2021-11-01T00:00:00',\n       '2021-10-01T00:00:00', '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00', '2021-05-01T00:00:00',\n       '2021-04-01T00:00:00', '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00', '2020-11-01T00:00:00',\n       '2020-10-01T00:00:00', '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00', '2020-05-01T00:00:00',\n       '2020-04-01T00:00:00', '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00', '2019-11-01T00:00:00',\n       '2019-10-01T00:00:00', '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00', '2019-05-01T00:00:00',\n       '2019-04-01T00:00:00', '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00', '2018-11-01T00:00:00',\n       '2018-10-01T00:00:00', '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00', '2018-05-01T00:00:00',\n       '2018-04-01T00:00:00', '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00', '2017-11-01T00:00:00',\n       '2017-10-01T00:00:00', '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00', '2017-05-01T00:00:00',\n       '2017-04-01T00:00:00', '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00', '2016-11-01T00:00:00',\n       '2016-10-01T00:00:00', '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00', '2016-05-01T00:00:00',\n       '2016-04-01T00:00:00', '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)proj:transform()object{0.1, 0.0, -0.1, 1.0, -180.0, 90.0}array({0.1, 0.0, -0.1, 1.0, -180.0, 90.0}, dtype=object)proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)start_datetime(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00',\n       '2019-05-01T00:00:00', '2019-04-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')end_datetime(time)&lt;U19'2022-01-31T00:00:00' ... '2016-...array(['2022-01-31T00:00:00', '2021-12-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-08-31T00:00:00',\n       '2021-07-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-03-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-01-31T00:00:00', '2020-12-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-08-31T00:00:00',\n       '2020-07-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-03-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-01-31T00:00:00', '2019-12-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-09-30T00:00:00', '2019-08-31T00:00:00',\n       '2019-07-31T00:00:00', '2019-06-30T00:00:00',\n       '2019-05-31T00:00:00', '2019-04-30T00:00:00',\n       '2019-03-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-01-31T00:00:00', '2018-12-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-08-31T00:00:00',\n       '2018-07-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-03-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-01-31T00:00:00', '2017-12-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-08-31T00:00:00',\n       '2017-07-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-03-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-01-31T00:00:00', '2016-12-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-08-31T00:00:00',\n       '2016-07-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-03-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-01-31T00:00:00'], dtype='&lt;U19')proj:epsg()float644.326e+03array(4326.)proj:bbox()object{180.0, -180.0, 90.0, -90.0}array({180.0, -180.0, 90.0, -90.0}, dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')epsg()float644.326e+03array(4326.)Attributes: (4)spec :RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326.0transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "title": "Calculate timeseries from COGs",
    "section": "Clip the data to the bounding box for China",
    "text": "Clip the data to the bounding box for China\n\n# Subset to Bounding Box for China\nsubset = da.rio.clip_box(*china_bbox)\nsubset\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/rasterio/windows.py:310: RasterioDeprecationWarning: The height, width, and precision parameters are unused, deprecated, and will be removed in 2.0.0.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-0683303cbb3f9689a8e6286f8a3ad3a0' (time: 73,\n                                                                band: 1,\n                                                                y: 354, x: 614)&gt;\ndask.array&lt;getitem, shape=(73, 1, 354, 614), dtype=float64, chunksize=(1, 1, 354, 535), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n  * time            (time) &lt;U19 '2022-01-01T00:00:00' ... '2016-01-01T00:00:00'\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ... '...\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 73.7 73.8 73.9 74.0 ... 134.7 134.8 134.9 135.0\n  * y               (y) float64 53.5 53.4 53.3 53.2 53.1 ... 18.5 18.4 18.3 18.2\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    ...              ...\n    proj:epsg       float64 4.326e+03\n    proj:bbox       object {90.0, 180.0, -90.0, -180.0}\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    title           &lt;U17 'Default COG Layer'\n    epsg            float64 4.326e+03\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0),...\n    resolution:  0.1xarray.DataArray'stackstac-0683303cbb3f9689a8e6286f8a3ad3a0'time: 73band: 1y: 354x: 614dask.array&lt;chunksize=(1, 1, 354, 535), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n121.06 MiB\n1.44 MiB\n\n\nShape\n(73, 1, 354, 614)\n(1, 1, 354, 535)\n\n\nCount\n876 Tasks\n146 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (16)time(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00', '2021-11-01T00:00:00',\n       '2021-10-01T00:00:00', '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00', '2021-05-01T00:00:00',\n       '2021-04-01T00:00:00', '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00', '2020-11-01T00:00:00',\n       '2020-10-01T00:00:00', '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00', '2020-05-01T00:00:00',\n       '2020-04-01T00:00:00', '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00', '2019-11-01T00:00:00',\n       '2019-10-01T00:00:00', '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00', '2019-05-01T00:00:00',\n       '2019-04-01T00:00:00', '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00', '2018-11-01T00:00:00',\n       '2018-10-01T00:00:00', '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00', '2018-05-01T00:00:00',\n       '2018-04-01T00:00:00', '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00', '2017-11-01T00:00:00',\n       '2017-10-01T00:00:00', '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00', '2017-05-01T00:00:00',\n       '2017-04-01T00:00:00', '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00', '2016-11-01T00:00:00',\n       '2016-10-01T00:00:00', '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00', '2016-05-01T00:00:00',\n       '2016-04-01T00:00:00', '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float6473.7 73.8 73.9 ... 134.9 135.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 73.7,  73.8,  73.9, ..., 134.8, 134.9, 135. ])y(y)float6453.5 53.4 53.3 ... 18.4 18.3 18.2axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([53.5, 53.4, 53.3, ..., 18.4, 18.3, 18.2])proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)start_datetime(time)&lt;U19'2022-01-01T00:00:00' ... '2016-...array(['2022-01-01T00:00:00', '2021-12-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-01-01T00:00:00', '2020-12-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-01-01T00:00:00', '2019-12-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-09-01T00:00:00', '2019-08-01T00:00:00',\n       '2019-07-01T00:00:00', '2019-06-01T00:00:00',\n       '2019-05-01T00:00:00', '2019-04-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-01-01T00:00:00', '2018-12-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-01-01T00:00:00', '2017-12-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-01-01T00:00:00', '2016-12-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-01-01T00:00:00'], dtype='&lt;U19')end_datetime(time)&lt;U19'2022-01-31T00:00:00' ... '2016-...array(['2022-01-31T00:00:00', '2021-12-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-08-31T00:00:00',\n       '2021-07-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-03-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-01-31T00:00:00', '2020-12-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-08-31T00:00:00',\n       '2020-07-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-03-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-01-31T00:00:00', '2019-12-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-09-30T00:00:00', '2019-08-31T00:00:00',\n       '2019-07-31T00:00:00', '2019-06-30T00:00:00',\n       '2019-05-31T00:00:00', '2019-04-30T00:00:00',\n       '2019-03-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-01-31T00:00:00', '2018-12-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-08-31T00:00:00',\n       '2018-07-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-03-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-01-31T00:00:00', '2017-12-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-08-31T00:00:00',\n       '2017-07-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-03-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-01-31T00:00:00', '2016-12-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-08-31T00:00:00',\n       '2016-07-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-03-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-01-31T00:00:00'], dtype='&lt;U19')proj:epsg()float644.326e+03array(4326.)proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')epsg()float644.326e+03array(4326.)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :73.65000000000002 0.09999999999999998 0.0 53.55 0.0 -0.09999999999999999array(0)Attributes: (2)spec :RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#select-a-band-of-data",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#select-a-band-of-data",
    "title": "Calculate timeseries from COGs",
    "section": "Select a band of data",
    "text": "Select a band of data\nThere is just one band in this case, cog_default.\n\n# select the band default\ndata_band = da.sel(band=\"cog_default\")"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "href": "notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "title": "Calculate timeseries from COGs",
    "section": "Aggregate the data",
    "text": "Aggregate the data\nCalculate the mean at each time across the whole dataset. Note this is the first time that the data is actually loaded.\n\n# Average over entire spatial bounding box for each month\nmeans = data_band.mean(dim=(\"x\", \"y\")).compute()\n\n\nmeans.plot()"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html",
    "href": "notebooks/quickstarts/wfs.html",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#run-this-notebook",
    "href": "notebooks/quickstarts/wfs.html#run-this-notebook",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#approach",
    "href": "notebooks/quickstarts/wfs.html#approach",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Approach",
    "text": "Approach\n\nUse OWSLib to determine what data is available and inspect the metadata\nUse OWSLib to filter and read the data\nUse geopandas and folium to analyze and plot the data\n\nNote that the default examples environment is missing one requirement: oswlib. We can pip install that before we move on.\n\n!pip install OWSLib==0.28.1\n\nRequirement already satisfied: OWSLib==0.28.1 in /opt/conda/lib/python3.7/site-packages (0.28.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (5.4.1)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (4.9.2)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2021.1)\nRequirement already satisfied: python-dateutil&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2.8.2)\nRequirement already satisfied: requests&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from OWSLib==0.28.1) (2.24.0)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=1.5-&gt;OWSLib==0.28.1) (1.15.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (2022.9.24)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (3.0.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests&gt;=1.0-&gt;OWSLib==0.28.1) (1.25.11)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.3; however, version 23.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\n\n\n\nfrom owslib.ogcapi.features import Features\nimport geopandas as gpd\nimport datetime as dt\nfrom datetime import datetime, timedelta"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#about-the-data",
    "href": "notebooks/quickstarts/wfs.html#about-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "About the Data",
    "text": "About the Data\nThe fire data shown is generated by the FEDs algorithm. The FEDs algorithm tracks fire movement and severity by ingesting observations from the VIIRS thermal sensors on the Suomi NPP and NOAA-20 satellites. This algorithm uses raw VIIRS observations to generate a polygon of the fire, locations of the active fire line, and estimates of fire mean Fire Radiative Power (FRP). The VIIRS sensors overpass at ~1:30 AM and PM local time, and provide estimates of fire evolution ~ every 12 hours. The data produced by this algorithm describe where fires are in space and how fires evolve through time. This CONUS-wide implementation of the FEDs algorithm is based on Chen et al 2020’s algorithm for California.\nThe data produced by this algorithm is considered experimental."
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "href": "notebooks/quickstarts/wfs.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Look at the data that is availible through the OGC API",
    "text": "Look at the data that is availible through the OGC API\nThe datasets that are distributed throught the OGC API are organized into collections. We can display the collections with the command:\n\nOGC_URL = \"https://firenrt.delta-backend.com\"\n\nw = Features(url=OGC_URL)\nw.feature_collections()\n\n['public.eis_fire_lf_nfplist_nrt',\n 'public.eis_fire_snapshot_newfirepix_nrt',\n 'public.eis_fire_newfirepix',\n 'public.eis_fire_lf_fireline_nrt',\n 'public.eis_fire_lf_perimeter_archive',\n 'public.eis_fire_lf_fireline_archive',\n 'public.eis_fire_fireline',\n 'public.eis_fire_lf_newfirepix_archive',\n 'public.eis_fire_perimeter',\n 'public.eis_fire_lf_nfplist_archive',\n 'public.eis_fire_snapshot_perimeter_nrt',\n 'public.eis_fire_snapshot_fireline_nrt',\n 'public.eis_fire_lf_perimeter_nrt',\n 'public.eis_fire_lf_newfirepix_nrt',\n 'public.st_squaregrid',\n 'public.st_hexagongrid',\n 'public.st_subdivide']\n\n\nWe will focus on the public.eis_fire_snapshot_fireline_nrt collection, the public.eis_fire_snapshot_perimeter_nrt collection, and the public.eis_fire_lf_perimeter_archive collection here.\n\nInspect the metatdata for public.eis_fire_snapshot_perimeter_nrt collection\nWe can access information that drescribes the \"public.eis_fire_snapshot_perimeter_nrt.\n\nperm = w.collection(\"public.eis_fire_snapshot_perimeter_nrt\")\n\nWe are particularly interested in the spatial and temporal extents of the data.\n\nperm[\"extent\"]\n\n{'spatial': {'bbox': [[-124.61687469482422,\n    24.069549560546875,\n    -63.63948059082031,\n    49.40034866333008]],\n  'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84'},\n 'temporal': {'interval': [['2023-05-03T00:00:00+00:00',\n    '2023-05-23T00:00:00+00:00']],\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian'}}\n\n\nIn addition to getting metadata about the data we can access the queryable fields. Each of these fields will represent a column in our dataframe.\n\nperm_q = w.collection_queryables(\"public.eis_fire_snapshot_perimeter_nrt\")\nperm_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'duration': {'name': 'duration', 'type': 'number'},\n 'farea': {'name': 'farea', 'type': 'number'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'flinelen': {'name': 'flinelen', 'type': 'number'},\n 'fperim': {'name': 'fperim', 'type': 'number'},\n 'isactive': {'name': 'isactive', 'type': 'number'},\n 'meanfrp': {'name': 'meanfrp', 'type': 'number'},\n 'n_newpixels': {'name': 'n_newpixels', 'type': 'number'},\n 'n_pixels': {'name': 'n_pixels', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 'pixden': {'name': 'pixden', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#filter-the-data",
    "href": "notebooks/quickstarts/wfs.html#filter-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Filter the data",
    "text": "Filter the data\nIt is always a good idea to do any data filtering as early as possible. In this example we know that we want the data for particular spatial and temporal extents. We can apply those and other filters using the OWSLib package.\nIn the below example we are:\n\nchoosing the public.eis_fire_snapshot_perimeter_nrt collection\nsubsetting it by space using the bbox parameter\nsubsetting it by time using the datetime parameter\nfiltering for fires over 5km^2 and over 2 days long using the filter parameter. The filter parameter lets us filter by the columns in ‘public.eis_fire_snapshot_perimeter_nrt’ using SQL-style queries.\n\nNOTE: The limit parameter desginates the maximum number of objects the query will return. The default limit is 10, so if we want to all of the fire perimeters within certain conditions, we need to make sure that the limit is large.\n\n## Get the most recent fire perimeters, and 7 days before most recent fire perimeter\nmost_recent_time = max(*perm[\"extent\"][\"temporal\"][\"interval\"])\nnow = dt.datetime.strptime(most_recent_time, \"%Y-%m-%dT%H:%M:%S+00:00\")\nlast_week = now - dt.timedelta(weeks=1)\nlast_week = dt.datetime.strftime(last_week, \"%Y-%m-%dT%H:%M:%S+00:00\")\nprint(\"Most Recent Time =\", most_recent_time)\nprint(\"Last week =\", last_week)\n\nMost Recent Time = 2023-05-23T00:00:00+00:00\nLast week = 2023-05-16T00:00:00+00:00\n\n\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",  # name of the dataset we want\n    bbox=[\"-106.8\", \"24.5\", \"-72.9\", \"37.3\"],  # coodrinates of bounding box,\n    datetime=[last_week + \"/\" + most_recent_time],  # date range\n    limit=1000,  # max number of items returned\n    filter=\"farea&gt;5 AND duration&gt;2\",  # additional filters based on queryable fields\n)\n\nThe result is a dictionary containing all of the data and some summary fields. We can look at the keys to see what all is in there.\n\nperm_results.keys()\n\ndict_keys(['type', 'id', 'title', 'description', 'numberMatched', 'numberReturned', 'links', 'features'])\n\n\nFor instance you can check the total number of matched items and make sure that it is equal to the number of returned items. This is how you know that the limit you defined above is high enough.\n\nperm_results[\"numberMatched\"] == perm_results[\"numberReturned\"]\n\nTrue\n\n\nYou can also access the data directly in the browser or in an HTTP GET call using the constructed link.\n\nperm_results[\"links\"][1][\"href\"]\n\n'https://firenrt.delta-backend.com/collections/public.eis_fire_snapshot_perimeter_nrt/items?bbox=-106.8%2C24.5%2C-72.9%2C37.3&datetime=2023-05-16T00%3A00%3A00%2B00%3A00%2F2023-05-23T00%3A00%3A00%2B00%3A00&limit=1000&filter=farea%3E5+AND+duration%3E2'"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#read-data",
    "href": "notebooks/quickstarts/wfs.html#read-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Read data",
    "text": "Read data\nIn addition to all the summary fields, the perm_results dict contains all the data. We can pass the data into geopandas to make it easier to interact with.\n\ndf = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\ndf\n\n\n\n\n\n\n\n\ngeometry\nduration\nfarea\nfireid\nflinelen\nfperim\nisactive\nmeanfrp\nn_newpixels\nn_pixels\nogc_fid\npixden\nt\n\n\n\n\n0\nPOLYGON ((-78.05380 25.07529, -78.05382 25.075...\n8.5\n6.787331\n63349\n0.0\n10.125732\n1\n0.0\n0\n32\n2\n4.714666\n2023-05-19T00:00:00\n\n\n1\nPOLYGON ((-81.06449 25.95495, -81.06448 25.954...\n16.5\n81.413491\n59462\n0.0\n37.958965\n1\n0.0\n0\n544\n21\n6.681939\n2023-05-18T12:00:00\n\n\n2\nPOLYGON ((-81.66818 29.24672, -81.66808 29.246...\n4.0\n7.789717\n65940\n0.0\n12.359742\n1\n0.0\n0\n66\n180\n8.472709\n2023-05-21T12:00:00\n\n\n3\nPOLYGON ((-85.52621 30.03404, -85.52414 30.037...\n8.0\n12.906472\n64561\n0.0\n17.488802\n1\n0.0\n0\n127\n585\n9.840024\n2023-05-22T12:00:00\n\n\n4\nMULTIPOLYGON (((-84.18103 30.73800, -84.18102 ...\n80.5\n594.630290\n23106\n0.0\n391.297316\n1\n0.0\n0\n1553\n2307\n2.611707\n2023-05-21T00:00:00\n\n\n5\nMULTIPOLYGON (((-103.52577 32.22440, -103.5257...\n17.0\n10.940001\n61561\n0.0\n19.953733\n1\n0.0\n0\n22\n6756\n2.010969\n2023-05-22T00:00:00\n\n\n6\nPOLYGON ((-104.08174 32.22901, -104.08175 32.2...\n11.0\n15.296388\n62322\n0.0\n17.629951\n1\n0.0\n0\n8\n6802\n0.522999\n2023-05-19T00:00:00\n\n\n7\nPOLYGON ((-106.30889 25.15346, -106.30888 25.1...\n6.0\n7.757181\n63955\n0.0\n11.384919\n1\n0.0\n0\n92\n7851\n11.859979\n2023-05-18T12:00:00\n\n\n8\nPOLYGON ((-106.30553 24.99012, -106.30554 24.9...\n8.5\n16.009259\n62262\n0.0\n19.050446\n0\n0.0\n0\n144\n7862\n8.994795\n2023-05-16T00:00:00"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#explore-data",
    "href": "notebooks/quickstarts/wfs.html#explore-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Explore data",
    "text": "Explore data\nWe can quickly explore the data by setting the coordinate reference system (crs) and using .explore()\n\ndf = df.set_crs(\"EPSG:4326\")\ndf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#visualize-most-recent-fire-perimeters-with-firelines",
    "href": "notebooks/quickstarts/wfs.html#visualize-most-recent-fire-perimeters-with-firelines",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize Most Recent Fire Perimeters with Firelines",
    "text": "Visualize Most Recent Fire Perimeters with Firelines\nIf we wanted to combine collections to make more informative analyses, we can use some of the same principles.\nFirst we’ll get the queryable fields, and the extents:\n\nfline_q = w.collection_queryables(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_collection = w.collection(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_q[\"properties\"]\n\n{'wkb_geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'mergeid': {'name': 'mergeid', 'type': 'number'},\n 'ogc_fid': {'name': 'ogc_fid', 'type': 'number'},\n 't': {'name': 't', 'type': 'string'}}\n\n\n\nRead\nThen we’ll use those fields to get most recent fire perimeters and fire lines.\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",\n    datetime=most_recent_time,\n    limit=1000,\n)\nperimeters = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\n\n## Get the most recent fire lines\nperimeter_ids = perimeters.fireid.unique()\nperimeter_ids = \",\".join(map(str, perimeter_ids))\n\nfline_results = w.collection_items(\n    \"public.eis_fire_snapshot_fireline_nrt\",\n    limit=1000,\n    filter=\"fireid IN (\"\n    + perimeter_ids\n    + \")\",  # only the fires from the fire perimeter query above\n)\nfline = gpd.GeoDataFrame.from_features(fline_results[\"features\"])\n\n\n\nVisualize\n\nperimeters = perimeters.set_crs(\"epsg:4326\")\nfline = fline.set_crs(\"epsg:4326\")\n\nm = perimeters.explore()\nm = fline.explore(m=m, color=\"orange\")\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#visualize-the-growth-of-the-camp-fire",
    "href": "notebooks/quickstarts/wfs.html#visualize-the-growth-of-the-camp-fire",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize the Growth of the Camp Fire",
    "text": "Visualize the Growth of the Camp Fire\nWe may be interested in understanding how a fire evolved through time. To do this, we can work with the “Large fire” or “lf” perimeter collections. The public.eis_fire_lf_perimeter_nrt colelction has the full spread history of fires from this year. public.eis_fire_lf_perimeter_archive has the full spread history of fires from 2018-2021 that were in the Western United States. The Camp Fire was in 2018, so we will work with the public.eis_fire_lf_perimeter_archive collection.\nWe can start by querying with information specific to the Camp Fire, like it’s genreal region (Northern California), and when it was active (November 2018).\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",  \n    bbox=[\"-124.52\", \"39.2\", \"-120\", \"42\"],  # North California bounding box,\n    datetime=[\"2018-11-01T00:00:00+00:00/2018-11-30T12:00:00+00:00\"], \n    limit=3000, \n    filter=\"duration&gt;2\",  # additional filters based on queryable fields\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by = \"t\", ascending = False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\n\nm = perimeters.explore(style_kwds = {'fillOpacity':0})\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#download-data",
    "href": "notebooks/quickstarts/wfs.html#download-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Download Data",
    "text": "Download Data\nDownloading pre-filtered data may be useful for working locally, or for working with the data in GIS software.\nWe can download the dataframe we made by writing it out into a shapefile or into a GeoJSON file.\nperimeters.to_file('perimeters.shp') \nperimeters.to_file('perimeters.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "notebooks/quickstarts/wfs.html#collection-information",
    "href": "notebooks/quickstarts/wfs.html#collection-information",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Collection Information",
    "text": "Collection Information\nThe API hosts 9 different collections. There are four different types of data, and three different time-scales availible for querying through the API. “*snapshot*” collections are useful for visualizing the most recent data. It contains the most recent fires perimeters, active firelines, or VIIRS observations within the last 20 days. “*lf*” collections (short for Large Fire), show every fire perimeter, active fire line, or VIIRS observations for fires over 5 km^2. Collections that end in *archive are for year 2018 - 2021 across the Western United States. Collections with the *nrt ending are for CONUS from this most recent year. FireIDs are consistent only between layers with the same timescale (snapshot, lf_*nrt, and lf_archive*).\npublic.eis_fire_snapshot_perimeter_nrt\nPerimeter of cumulative fire-area. Most recent perimeter from the last 20 days.\npublic.eis_fire_lf_perimeter_nrt\nPerimeter of cumulative fire-area, from fires over 5 km^2. Every fire perimeter from current year to date.\npublic.eis_fire_lf_perimeter_archive\nPerimeter of cumulative fire-area, from fires over 5 km^2 in the Western United States. Every fire perimeter from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nmeanfrp\nMean fire radiative power. The weighted sum of the fire radiative power detected at each new pixel, divided by the number of pixels. If no new pixels are detected, meanfrp is set to zero.\nMW/(pixel area)\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\npixden\nNumber of pixels divided by area of perimeter.\npixels/Km^2\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nKm\n\n\nfperim\nLength of fire perimeter.\nKm\n\n\nfarea\nArea within fire perimeter.\nKm^2\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\nn_pixels\nNumber of pixel-detections in history of fire.\npixels\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\ngeometry\nThe shape of the perimeter.\nGeometry\n\n\n\npublic.eis_fire_snapshot_fireline_nrt\nActive fire line as estimated by new VIIRS detections. Most fire line from the last 20 days.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2. Every fire line from current year to date.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2 in the Western United States. Every fire line from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID\n\n\n\npublic.eis_fire_snapshot_newfirepix_nrt\nNew pixel detections that inform the most recent time-step’s perimeter and fireline calculation from the last 20 days.\npublic.eis_fire_lf_newfirepix_nrt\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible from start of current year to date.\npublic.eis_fire_lf_newfirepix_archive\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible for Western United States from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID"
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html",
    "href": "notebooks/quickstarts/list-collections.html",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#run-this-notebook",
    "href": "notebooks/quickstarts/list-collections.html#run-this-notebook",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#approach",
    "href": "notebooks/quickstarts/list-collections.html#approach",
    "title": "List STAC collections",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog\nIterate over collections and print the title of each collection"
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "href": "notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "title": "List STAC collections",
    "section": "Open STAC catalog",
    "text": "Open STAC catalog\n\nfrom pystac_client import Client\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncatalog = Client.open(STAC_API_URL)"
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#list-collections",
    "href": "notebooks/quickstarts/list-collections.html#list-collections",
    "title": "List STAC collections",
    "section": "List collections",
    "text": "List collections\n\ncollections = list(catalog.get_collections())\nfor collection in sorted(collections, key=lambda x: x.title):\n    print(collection.title)\n\n\n\n0-100 cm Volumetric Soil Moisture (%)\nActive Fire Line\nAerosol Optical Depth (AOD)\nAnnual LAI maps for 2003 and 2021 (Bangladesh)\nAnnual land cover maps for 2001 and 2020\nBlack Marble 500m Nightlights Daily Dataset\nBlack Marble High Definition Nightlights 1 band Dataset\nBlack Marble High Definition Nightlights Monthly Dataset\nBurn Area Reflectance Classification for Thomas Fire\nCO₂ (Avg)\nCO₂ (Diff)\nCaldor Fire Behavior\nCaldor Fire Burn Severity\nChange in ET for 2020 fires using LIS outputs\nChange in transpiration for 2020 fires using LIS outputs\nECCO sea-surface height change from 1992 to 2017\nEvapotranspiration - LIS 10km Global DA\nFire Perimeters\nGEOGLAM Crop Monitor\nGRDI BUILT Constituent Raster\nGRDI CDR Constituent Raster\nGRDI Filled Missing Values Count\nGRDI IMR Constituent Raster\nGRDI SHDI Constituent Raster\nGRDI V1 raster\nGRDI VNL Constituent Raster\nGRDI VNL Slope Constituent Raster\nGlobal TWS Non-Stationarity Index\nGridded 2012 EPA Methane Emissions - Abandoned Coal Mines\nGridded 2012 EPA Methane Emissions - Composting\nGridded 2012 EPA Methane Emissions - Domestic Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Enteric Fermentation\nGridded 2012 EPA Methane Emissions - Ferroalloy Production\nGridded 2012 EPA Methane Emissions - Field Burning\nGridded 2012 EPA Methane Emissions - Field Burning (monthly)\nGridded 2012 EPA Methane Emissions - Forest Fires\nGridded 2012 EPA Methane Emissions - Forest Fires (daily)\nGridded 2012 EPA Methane Emissions - Industrial Landfills\nGridded 2012 EPA Methane Emissions - Industrial Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Manure Management\nGridded 2012 EPA Methane Emissions - Manure Management (monthly)\nGridded 2012 EPA Methane Emissions - Mobile Combustion\nGridded 2012 EPA Methane Emissions - Municipal Landfills\nGridded 2012 EPA Methane Emissions - Natural Gas Distribution\nGridded 2012 EPA Methane Emissions - Natural Gas Processing\nGridded 2012 EPA Methane Emissions - Natural Gas Production\nGridded 2012 EPA Methane Emissions - Natural Gas Production (monthly)\nGridded 2012 EPA Methane Emissions - Natural Gas Transmission\nGridded 2012 EPA Methane Emissions - Petrochemical Production\nGridded 2012 EPA Methane Emissions - Petroleum\nGridded 2012 EPA Methane Emissions - Petroleum (monthly)\nGridded 2012 EPA Methane Emissions - Rice Cultivation\nGridded 2012 EPA Methane Emissions - Rice Cultivation (monthly)\nGridded 2012 EPA Methane Emissions - Stationary Combustion\nGridded 2012 EPA Methane Emissions - Stationary Combustion (monthly)\nGridded 2012 EPA Methane Emissions - Surface Coal Mines\nGridded 2012 EPA Methane Emissions - Underground Coal Mines\nGross Primary Productivity - LIS 10km Global DA\nGross Primary Productivity Trend - LIS 10km Global DA\nGroundwater Storage - LIS 10km Global DA\nHLSL30.002 Environmental Justice Events\nHLSS30.002 Environmental Justice Events\nHouston LST (Diff)\nHouston Land Cover\nHouston NDVI: decadal average\nHouston land surface temperature at night time - decadal average\nHouston land surface temperature during daytime - decadal average\nHurricane Ida - Blue Tarps PlanetScope Image\nHurricane Ida - Detected Blue Tarps\nICESat-2 L4 Monthly Gridded Sea Ice Thickness (COGs)\nMTBS Burn Severity\nMaximum Fire Radiative Power for Thomas Fire\nNCEO Africa Aboveground Woody Biomass 2017\nNO₂\nNO₂ (Diff)\nOMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)\nOMI_trno2 - 0.10 x 0.10 Annual as Cloud-Optimized GeoTIFFs (COGs)\nPopulation Density Maps using satellite imagery built by Meta\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjections of Snow Water Equivalent (SWE)\nProjections of Snow Water Equivalent (SWE) - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) - SSP5-8.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP5-8.5\nProjections of Snow Water Equivalent (SWE) losses\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Pine Island Glacier\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Thwaites Glacier\nSnow Water Equivalent - LIS 10km Global DA\nSocial Vulnerability Index (Household)\nSocial Vulnerability Index (Household) (Masked)\nSocial Vulnerability Index (Housing)\nSocial Vulnerability Index (Housing) (Masked)\nSocial Vulnerability Index (Minority)\nSocial Vulnerability Index (Minority) (Masked)\nSocial Vulnerability Index (Overall)\nSocial Vulnerability Index (Overall) (Masked)\nSocial Vulnerability Index (SocioEconomic)\nSocial Vulnerability Index (SocioEconomic) (Masked)\nStream network across the Contiguous United States\nStreamflow - LIS 10km Global DA\nSubsurface Runoff - LIS 10km Global DA\nSurface runoff - LIS 10km Global DA\nTerrestrial Water Storage (TWS) Anomalies\nTerrestrial Water Storage - LIS 10km Global DA\nTerrestrial Water Storage Trend - LIS 10km Global DA\nTotal Precipitation - LIS 10km Global DA\nTrend in Terrestrial Water Storage (TWS) Anomalies\nVIIRs Fire Detections\ndisalexi-etsuppression"
  },
  {
    "objectID": "notebooks/quickstarts/list-collections.html#alternate-approachs",
    "href": "notebooks/quickstarts/list-collections.html#alternate-approachs",
    "title": "List STAC collections",
    "section": "Alternate approachs",
    "text": "Alternate approachs\nInstead of exploring STAC catalog programatically, you can discover available collections the following ways:\n\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com"
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html",
    "href": "notebooks/quickstarts/hls-visualization.html",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "href": "notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#approach",
    "href": "notebooks/quickstarts/hls-visualization.html#approach",
    "title": "Get tiles from COGs",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates within a bounding box, which is also an area of interest (AOI) in this example, for a given collection\nRegister a dynamic tiler search for an AOI and specific date range for a given collection\nExplore different options for displaying multi-band Harmonized Landsat and Sentinel (HLS) assets with the Raster API."
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#about-the-data",
    "href": "notebooks/quickstarts/hls-visualization.html#about-the-data",
    "title": "Get tiles from COGs",
    "section": "About the Data",
    "text": "About the Data\nA small subset of HLS data has been ingested to the VEDA datastore to visually explore data using the Raster API, which is a VEDA instance of (pgstac-titiler). This limited subset includes a two granules for dates before and after Hurricane Maria in 2017 and Hurricane Ida in 2021.\nNote about HLS datasets: The Sentinel and Landsat assets have been “harmonized” in the sense that these products have been generated to use the same spatial resolution and grid system. Thus these 2 HLS S30 and L30 productscan be used interchangeably in algorithms. However, the individual band assets are specific to each provider. This notebook focuses on displaying HLS data with a dynamic tiler so separate examples are provided for rendering the unique band assets of each collection.\nAdditional Resources\n\nHLSL30 Dataset Landing Page\nLandsat 8 Bands and Combinations Blog\nHLSS30 Dataset Landing Page\nSentinel 2 Bands and Combinations Blog\nCQL2 STAC-API Examples\n\n\nimport json\nimport requests\n\nfrom folium import Map, TileLayer"
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "href": "notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "title": "Get tiles from COGs",
    "section": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations",
    "text": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations\nIn this notebook we will focus on HLS S30 data for Hurricane Ida, but Hurricane Maria and L30 parameters are provided below for further exploration.\n\n# Endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Harmonized Sentinel collection id and configuration info\ns30_collection_id = \"hls-s30-002-ej-reprocessed\"\ns30_swir_assets = [\"B12\", \"B8A\", \"B04\"]\ns30_vegetation_index_assets = [\"B08\", \"B04\"]\ns30_vegetation_index_expression = \"(B08-B04)/(B08+B04)\"\ns30_vegetation_index_rescaling = \"0,1\"\ns30_vegetation_index_colormap = \"rdylgn\"\n\n# Harmonized Landsat collection id and map configuration info\nl30_collection_id = \"hls-l30-002-ej-reprocessed\"\nl30_swir_assets = [\"B07\", \"B05\", \"B04\"]\nl30_ndwi_expression = \"(B03-B05)/(B03+B05)\"\nl30_ndwi_assets = [\"B03\", \"B05\"]\nl30_ndwi_rescaling = \"0,1\"\nl30_ndwi_colormap = \"spectral\"\n\n# Search criteria for events in both HLS Events collections\nmaria_bbox = [-66.167596, 17.961538, -65.110098, 18.96772]\nmaria_temporal_range = [\"2017-06-06T00:00:00Z\", \"2017-11-30T00:00:00Z\"]\n\nida_bbox = [-90.932637, 29.705366, -89.766437, 30.71627]\nida_temporal_range = [\"2021-07-01T00:00:00Z\", \"2021-10-28T00:00:00Z\"]\n\n\nFirst, search the STAC API to find the specific dates available within timeframe of interest (Hurricane Ida)\nTo focus on a specific point in time, we will restrict the temporal range when defining the item search in the example below.\n\ncollections_filter = {\n    \"op\": \"=\",\n    \"args\": [{\"property\": \"collection\"}, s30_collection_id],\n}\n\nspatial_filter = {\"op\": \"s_intersects\", \"args\": [{\"property\": \"bbox\"}, ida_bbox]}\n\ntemporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [{\"property\": \"datetime\"}, {\"interval\": ida_temporal_range}],\n}\n\n# Additional filters can be applied for other search criteria like &lt;= maximum eo:cloud_cover in item properties\ncloud_filter = {\"op\": \"&lt;=\", \"args\": [{\"property\": \"eo:cloud_cover\"}, 80]}\n\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"limit\": 100,\n    \"sortby\": [{\"direction\": \"asc\", \"field\": \"properties.datetime\"}],\n    \"context\": \"on\",  # add context for a summary of matched results\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, temporal_filter, cloud_filter],\n    },\n}\n\n# Note this search body can also be used for a stac item search\nstac_items_response = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json=search_body,\n).json()\n\n# Check how many items were matched in search\nprint(\"search context:\", stac_items_response[\"context\"])\n\n# Iterate over search results to get an array of item datetimes\n[item[\"properties\"][\"datetime\"] for item in stac_items_response[\"features\"]]\n\nsearch context: {'limit': 100, 'matched': 14, 'returned': 14}\n\n\n['2021-07-14T16:55:15.122720Z',\n '2021-07-24T16:55:15.112940Z',\n '2021-07-29T16:55:16.405890Z',\n '2021-08-08T16:55:15.798510Z',\n '2021-08-13T16:55:13.394950Z',\n '2021-08-23T16:55:11.785040Z',\n '2021-09-02T16:55:09.568600Z',\n '2021-09-07T16:55:13.430530Z',\n '2021-09-22T16:55:10.763010Z',\n '2021-09-27T16:55:17.027350Z',\n '2021-10-07T16:55:18.213640Z',\n '2021-10-12T16:55:14.209080Z',\n '2021-10-17T16:55:18.517600Z',\n '2021-10-22T16:55:14.670710Z']"
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "href": "notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "title": "Get tiles from COGs",
    "section": "Visualizing the data on a map",
    "text": "Visualizing the data on a map\nThe VEDA backend is based on eoAPI, an application for searching and tiling earth observation STAC records. The application uses titiler-pgstac for dynamically mosaicing cloud optimized data from a registerd STAC API search.\nTo use the dynamic tiler, register a STAC item search and then use the registered search ID to dynamically mosaic the search results on the map.\n\nUpdate the temporal range in search body and register that search with the Raster API\nThe registered search id can be reused for alternate map layer visualizations.\n\n# Restricted date range\nrestricted_temporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        {\"property\": \"datetime\"},\n        {\"interval\": [\"2021-10-16T00:00:00Z\", \"2021-10-18T00:00:00Z\"]},\n    ],\n}\n\n# Specify cql2-json filter language in search body\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, restricted_temporal_filter],\n    },\n}\n\nmosaic_response = requests.post(\n    f\"{RASTER_API_URL}/mosaic/register\",\n    json=search_body,\n).json()\nprint(json.dumps(mosaic_response, indent=2))\n\n{\n  \"searchid\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"links\": [\n    {\n      \"rel\": \"metadata\",\n      \"type\": \"application/json\",\n      \"href\": \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/info\"\n    },\n    {\n      \"rel\": \"tilejson\",\n      \"type\": \"application/json\",\n      \"href\": \"https://staging-raster.delta-backend.com/mosaic/7743bcb31bff7151aff7e5508785fce1/tilejson.json\"\n    }\n  ]\n}\n\n\n\n# Get base url for tiler from the register mosaic request\ntiles_href = next(\n    link[\"href\"] for link in mosaic_response[\"links\"] if link[\"rel\"] == \"tilejson\"\n)\n\n\n\nConfigure map formatting parameters\nSee the raster-api/docs for more formatting options"
  },
  {
    "objectID": "notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "href": "notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "title": "Get tiles from COGs",
    "section": "Use the built-in SWIR post processing algorithm",
    "text": "Use the built-in SWIR post processing algorithm\nNote in the example below the band assets for HLS S30 are selected. The equivalent SWIR band assets for L30 are provided at the top of this notebook.\n\n# Add additional map formatting parameters to tiles url\ntilejson_response = requests.get(\n    tiles_href,\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"post_process\": \"swir\",\n        \"assets\": s30_swir_assets,\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://staging-raster.delta-backend.com/mosaic/tiles/7743bcb31bff7151aff7e5508785fce1/WebMercatorQuad/{z}/{x}/{y}@1x?post_process=swir&assets=B12&assets=B8A&assets=B04\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.0,\n    85.0511287798066\n  ],\n  \"center\": [\n    0.0,\n    0.0,\n    6\n  ]\n}\n\n\n\nDisplay the data on a map\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFormat and render tiles using custom formatting\nThe titiler/raster-api supports user defined band combinations, band math expressions, rescaling, band index, resampling and more.\n\n# Add additional map formatting parameters to tiles url\ntilejson_response = requests.get(\n    tiles_href,\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"assets\": s30_vegetation_index_assets,\n        \"expression\": s30_vegetation_index_expression,\n        \"rescale\": s30_vegetation_index_rescaling,\n        \"colormap_name\": s30_vegetation_index_colormap,\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://staging-raster.delta-backend.com/mosaic/tiles/7743bcb31bff7151aff7e5508785fce1/WebMercatorQuad/{z}/{x}/{y}@1x?assets=B08&assets=B04&expression=%28B08-B04%29%2F%28B08%2BB04%29&rescale=0%2C1&colormap_name=rdylgn\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.0,\n    85.0511287798066\n  ],\n  \"center\": [\n    0.0,\n    0.0,\n    6\n  ]\n}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html",
    "href": "notebooks/quickstarts/open-and-plot.html",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "href": "notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#approach",
    "href": "notebooks/quickstarts/open-and-plot.html#approach",
    "title": "Open and plot COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nOpen the first item in the collection with xarray\nPlot the data using hvplot"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#about-the-data",
    "href": "notebooks/quickstarts/open-and-plot.html#about-the-data",
    "title": "Open and plot COGs",
    "section": "About the data",
    "text": "About the data\nCDC’s Social Vulnerability Index (SVI) uses 15 variables at the census tract level. The data comes from the U.S. decennial census for the years 2000 & 2010, and the American Community Survey (ACS) for the years 2014, 2016, and 2018. It is a hierarchical additive index (Tate, 2013), with the component elements of CDC’s SVI including the following for 4 themes: Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation.\nSVI indicates the relative vulnerability of every U.S. Census tract–subdivisions of counties for which the Census collects statistical data. SVI ranks the tracts on 15 social factors, including unemployment, minority status, and disability, and further groups them into four related themes. Thus, each tract receives a ranking for each Census variable and for each of the four themes, as well as an overall ranking.\n\nScientific research\nThe SVI Overall Score provides the overall, summed social vulnerability score for a given tract. The Overall Score SVI Grid is part of the U.S. Census Grids collection, and displays the Center for Disease Control & Prevention (CDC) SVI score. Funding for the final development, processing and dissemination of this data set by the Socioeconomic Data and Applications Center (SEDAC) was provided under the U.S. National Aeronautics and Space Administration (NASA)¹.\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nInterpreting the data\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nCredits\n\nCenter for International Earth Science Information Network, (CIESIN), Columbia University. 2021. Documentation for the U.S. Social Vulnerability Index Grids. Palisades, NY: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/fjr9-a973. Accessed 13 May 2022.\nCenters for Disease Control and Prevention/ Agency for Toxic Substances and Disease Registry/ Geospatial Research, Analysis, and Services Program. CDC/ATSDR Social Vulnerability Index Database. https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation_01192022_1.pdf\n\n\nfrom pystac_client import Client\nimport xarray as xr\n\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "title": "Open and plot COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection = \"social-vulnerability-index-overall-nopop\""
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "href": "notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "title": "Open and plot COGs",
    "section": "Find items in collection",
    "text": "Find items in collection\nUse pystac_client to search the STAC collection.\n\nsearch = Client.open(STAC_API_URL).search(collections=[collection])\nitems = list(search.items())\n\nThere are 5 items representing the 5 years of data in the collection (2000, 2010, 2014, 2016, and 2018)."
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#read-data",
    "href": "notebooks/quickstarts/open-and-plot.html#read-data",
    "title": "Open and plot COGs",
    "section": "Read data",
    "text": "Read data\nRead in data using xarray with the rasterio engine\n\ns3_link = items[0].assets[\"cog_default\"].href\nda = xr.open_dataset(s3_link, engine=\"rasterio\")\nda = da.squeeze(\"band\", drop=True)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (x: 13353, y: 6297)\nCoordinates:\n  * x            (x) float64 -178.2 -178.2 -178.2 ... -66.98 -66.97 -66.96\n  * y            (y) float64 71.38 71.37 71.36 71.35 ... 18.94 18.93 18.92 18.91\n    spatial_ref  int64 0\nData variables:\n    band_data    (y, x) float32 ...xarray.DatasetDimensions:x: 13353y: 6297Coordinates: (3)x(x)float64-178.2 -178.2 ... -66.97 -66.96array([-178.229167, -178.220833, -178.2125  , ...,  -66.979167,  -66.970834,\n        -66.9625  ])y(y)float6471.38 71.37 71.36 ... 18.92 18.91array([71.379166, 71.370833, 71.362499, ..., 18.929166, 18.920833, 18.9125  ])spatial_ref()int64...crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-178.23333334 0.00833333330000749 0.0 71.383332688 0.0 -0.00833333329998412array(0)Data variables: (1)band_data(y, x)float32...[84083841 values with dtype=float32]Attributes: (0)"
  },
  {
    "objectID": "notebooks/quickstarts/open-and-plot.html#plot-data",
    "href": "notebooks/quickstarts/open-and-plot.html#plot-data",
    "title": "Open and plot COGs",
    "section": "Plot data",
    "text": "Plot data\nPlot data using hvplot. By using rasterize=True we tell hvplot to use datashader behind the scenes to make the plot render more quickly and re-render on zoom.\n\n%%time\nda.hvplot(x=\"x\", y=\"y\", rasterize=True, clim=(0, 1), coastline=True, cmap=\"viridis\")\n\nWARNING:param.main: Calling the .opts method with options broken down by options group (i.e. separate plot, style and norm groups) is deprecated. Use the .options method converting to the simplified format instead or use hv.opts.apply_groups for backward compatibility.\n\n\nCPU times: user 5.37 s, sys: 249 ms, total: 5.62 s\nWall time: 5.66 s"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html",
    "href": "notebooks/quickstarts/intake.html",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#run-this-notebook",
    "href": "notebooks/quickstarts/intake.html#run-this-notebook",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#approach",
    "href": "notebooks/quickstarts/intake.html#approach",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve items\nRead collection using intake\n\n\nimport intake\nfrom pystac_client import Client\nfrom pystac import ItemCollection\nimport rasterio as rio\nfrom rasterio.plot import show\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\""
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#discover-items-for-time-of-interest",
    "href": "notebooks/quickstarts/intake.html#discover-items-for-time-of-interest",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Discover items for time of interest",
    "text": "Discover items for time of interest\nUse pystac_client to search the STAC for a particular time of interest.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(\n    max_items=100,  # this is max number of records that will be returned\n    limit=25,  # this is the page size of results per page (so based on `max_items` above we will have four pages of results)\n    datetime=\"2001-01-01/2003-01-01\",\n    sortby=\"datetime\",\n)\nitems = list(search.items())\nlen(items)\n\n100\n\n\nintake requires data be casted to pystac.ItemCollection(s) before we can inspect the catalog:\n\nitem_collection = ItemCollection(items=items)\ncollection = intake.open_stac_item_collection(item_collection)"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#basic-inspection-of-catalogcollections-with-instake-stac",
    "href": "notebooks/quickstarts/intake.html#basic-inspection-of-catalogcollections-with-instake-stac",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Basic Inspection of Catalog/Collection(s) with instake-stac",
    "text": "Basic Inspection of Catalog/Collection(s) with instake-stac\n\n# list the `features` of a STAC `FeatureCollection`\nlist(collection)[:25]\n\n['SSH_2017_minus_1992.cog',\n 'TREND_MEAN_TWS.cog',\n 'MODIS_LC_2001_BD_v2.cog',\n 'Anomaly_TWS_20020901.cog',\n 'Anomaly_TWS_20020902.cog',\n 'Anomaly_TWS_20020903.cog',\n 'Anomaly_TWS_20020904.cog',\n 'Anomaly_TWS_20020905.cog',\n 'Anomaly_TWS_20020906.cog',\n 'Anomaly_TWS_20020907.cog',\n 'Anomaly_TWS_20020908.cog',\n 'Anomaly_TWS_20020909.cog',\n 'Anomaly_TWS_20020910.cog',\n 'Anomaly_TWS_20020911.cog',\n 'Anomaly_TWS_20020912.cog',\n 'Anomaly_TWS_20020913.cog',\n 'Anomaly_TWS_20020914.cog',\n 'Anomaly_TWS_20020915.cog',\n 'Anomaly_TWS_20020916.cog',\n 'Anomaly_TWS_20020917.cog',\n 'Anomaly_TWS_20020918.cog',\n 'Anomaly_TWS_20020919.cog',\n 'Anomaly_TWS_20020920.cog',\n 'Anomaly_TWS_20020921.cog',\n 'Anomaly_TWS_20020922.cog']\n\n\n\n# list the `assets` of a particular `FeatureCollection.&lt;feature_id&gt;`\nlist(collection[\"TREND_MEAN_TWS.cog\"])\n\n['cog_default']\n\n\n\n# if the entries count is too high to `list` all of them in a notebook session you can \"search\" children of the `FeatureCollection`\nfor id, entry in collection.search(\"MEAN_TWS\").items():\n    print(id)\n\nTREND_MEAN_TWS.cog.cog_default\nTREND_MEAN_TWS.cog\n\n\n\n# or inspect the metadata for a `Feature` outright as `yaml`\ncollection[\"TREND_MEAN_TWS.cog\"]\n\ncog_default:\n  args:\n    chunks: {}\n    urlpath: s3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif\n  description: Default COG Layer\n  direct_access: allow\n  driver: intake_xarray.raster.RasterIOSource\n  metadata:\n    description: Cloud optimized default layer to display on map\n    href: s3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif\n    plots:\n      geotiff:\n        cmap: viridis\n        data_aspect: 1\n        dynamic: true\n        frame_width: 500\n        kind: image\n        rasterize: true\n        x: x\n        y: y\n    raster:bands:\n    - data_type: float64\n      histogram:\n        buckets:\n        - 343348.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 0.0\n        - 93900.0\n        count: 11.0\n        max: 8.917984008789062\n        min: -9999.0\n      nodata: 0.0\n      offset: 0.0\n      sampling: area\n      scale: 1.0\n      statistics:\n        maximum: 8.917984008789062\n        mean: -7851.75869965132\n        minimum: -9999.0\n        stddev: 4105.965757426157\n        valid_percent: 100.0\n    roles:\n    - data\n    - layer\n    title: Default COG Layer\n    type: image/tiff; application=geotiff; profile=cloud-optimized\n\n\n\n# now walk from the `FeatureCollection` to a specific asset URL\ncollection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].metadata[\"href\"]\n\n's3://veda-data-store-staging/EIS/COG/LIS_TWS_TREND/TREND_MEAN_TWS.cog.tif'\n\n\n\n# view this single cloud optimized geotiff source\ncog = rio.open(collection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].metadata[\"href\"])\nshow(cog)\n\n\n\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#loading-data-items-with-intake-stac",
    "href": "notebooks/quickstarts/intake.html#loading-data-items-with-intake-stac",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Loading data items with intake-stac",
    "text": "Loading data items with intake-stac\nAfter identifying an asset you want you can then load it into an xarray.DataArray using the .to_dask() function\n\nimport hvplot.xarray\n\nda = collection[\"TREND_MEAN_TWS.cog\"][\"cog_default\"].to_dask().squeeze()\nda = da.where(da != -9999)\nda.hvplot.image()"
  },
  {
    "objectID": "notebooks/quickstarts/intake.html#loading-many-data-items",
    "href": "notebooks/quickstarts/intake.html#loading-many-data-items",
    "title": "Searching VEDA STAC API and inspecting catalogs with intake",
    "section": "Loading many data items",
    "text": "Loading many data items\nAlternatively we can get all the asset hrefs in one fell swoop and load all.\nWe’ll use a specific collection for this example. In this case the Leaf Area Index in Bangladesh for 2003 and 2020.\n\nsearch = catalog.search(collections=[\"modis-annual-lai-2003-2020\"])\n\n\nsources = [item.assets[\"cog_default\"].href for item in search.items()]\nda_sources = intake.open_rasterio(\n    sources, chunks={}, concat_dim=\"year\", path_as_pattern=\"_LAI_{year}_BD\"\n)\nda = da_sources.to_dask().sel(band=1).squeeze()\nda = da.where(da != da.nodatavals[0])\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (year: 2, y: 1312, x: 1037)&gt;\ndask.array&lt;where, shape=(2, 1312, 1037), dtype=float32, chunksize=(1, 1312, 1037), chunktype=numpy.ndarray&gt;\nCoordinates:\n    band     int64 1\n  * y        (y) float64 26.63 26.63 26.62 26.62 ... 20.76 20.75 20.75 20.74\n  * x        (x) float64 88.03 88.03 88.04 88.04 ... 92.67 92.67 92.68 92.68\n  * year     (year) &lt;U4 '2020' '2003'\nAttributes:\n    transform:      (0.004491576420597609, 0.0, 88.02591469087191, 0.0, -0.00...\n    crs:            +init=epsg:4326\n    res:            (0.004491576420597609, 0.004491576420597609)\n    is_tiled:       1\n    nodatavals:     (-3.4028234663852886e+38,)\n    scales:         (1.0,)\n    offsets:        (0.0,)\n    AREA_OR_POINT:  Areaxarray.DataArrayyear: 2y: 1312x: 1037dask.array&lt;chunksize=(1, 1312, 1037), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.38 MiB\n5.19 MiB\n\n\nShape\n(2, 1312, 1037)\n(1, 1312, 1037)\n\n\nCount\n14 Tasks\n2 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (4)band()int641array(1)y(y)float6426.63 26.63 26.62 ... 20.75 20.74array([26.632802, 26.628311, 26.623819, ..., 20.753329, 20.748837, 20.744346])x(x)float6488.03 88.03 88.04 ... 92.68 92.68array([88.02816 , 88.032652, 88.037144, ..., 92.67245 , 92.676942, 92.681434])year(year)&lt;U4'2020' '2003'array(['2020', '2003'], dtype='&lt;U4')Attributes: (8)transform :(0.004491576420597609, 0.0, 88.02591469087191, 0.0, -0.004491576420597609, 26.63504817414382)crs :+init=epsg:4326res :(0.004491576420597609, 0.004491576420597609)is_tiled :1nodatavals :(-3.4028234663852886e+38,)scales :(1.0,)offsets :(0.0,)AREA_OR_POINT :Area\n\n\nTake the difference between the LAI for the two years and plot that.\n\ndiff = (da.sel(year=\"2020\") - da.sel(year=\"2003\")).compute()\n\n# get the 2% and 98% percentiles for color limits in the plot\nvmin, vmax = diff.quantile([0.02, 0.98]).values\n\n# get the full collection name for the title\ncollection_name = catalog.get_collection(\"modis-annual-lai-2003-2020\").title\n\ndiff.hvplot.image(\n    tiles=True,\n    crs=da.crs,\n    alpha=0.8,\n    cmap=\"turbo_r\",\n    clim=(vmin, vmax),\n    frame_height=600,\n    title=f\"Difference between {collection_name}\",\n)\n\n/srv/conda/envs/notebook/lib/python3.9/site-packages/pyproj/crs/crs.py:130: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)"
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html",
    "href": "notebooks/quickstarts/no2-map-plot.html",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "href": "notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#approach",
    "href": "notebooks/quickstarts/no2-map-plot.html#approach",
    "title": "Get map from COGs - NO2",
    "section": "Approach",
    "text": "Approach\n\nFetch STAC item for a particular date and collection - NO2\nPass STAC item in to the raster API /stac/tilejson.json endpoint\nVisualize tiles using folium\n\n\nimport requests\nimport folium"
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "title": "Get map from COGs - NO2",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_name = \"no2-monthly\""
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "href": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2015-12-31 17:00:00-07',\n     '2022-04-30 18:00:00-06']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2022-05-01T00:00:00Z'],\n  'cog_default': {'max': 50064805976866816, 'min': -6618294421291008}},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\nrescale_values\n\n{'max': 50064805976866816, 'min': -6618294421291008}"
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "href": "notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_name],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\nlen(items)\n\n1\n\n\nLet’s take a look at that one item.\n\nitem = items[0]\nitem\n\n{'id': 'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly/OMI_trno2_0.10x0.10_202101_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 35781585143857150,\n      'min': -4107596126486528.0,\n      'count': 11.0,\n      'buckets': [7437.0,\n       432387.0,\n       2866.0,\n       699.0,\n       356.0,\n       207.0,\n       76.0,\n       27.0,\n       7.0,\n       1.0]},\n     'statistics': {'mean': 367152773066762.6,\n      'stddev': 961254458662885.4,\n      'maximum': 35781585143857150,\n      'minimum': -4107596126486528.0,\n      'valid_percent': 84.69829559326172}}]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1800.0, 3600.0],\n  'end_datetime': '2021-01-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'start_datetime': '2021-01-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}"
  },
  {
    "objectID": "notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "href": "notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "title": "Get map from COGs - NO2",
    "section": "Use /stac/tilejson.json to get tiles",
    "text": "Use /stac/tilejson.json to get tiles\nWe pass the, item id, collection name, and the rescale_values in to the RASTER API endpoint and get back a tile.\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={item['collection']}&item={item['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\ntiles\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202101_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=rdbu_r&rescale=-6618294421291008%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    attr=\"VEDA\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html",
    "href": "notebooks/quickstarts/visualize-multiple-times.html",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#approach",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#approach",
    "title": "Open and visualize COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items\nUse rioxarray to crop data to AOI\nUse hvplot to render the COG at every timestep\n\n\nimport requests\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\nimport rioxarray  # noqa\nimport hvplot.xarray  # noqa\nfrom cartopy import crs as ccrs"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "title": "Open and visualize COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\ncollection_id = \"no2-monthly\""
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Open and visualize COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection_id], sortby=\"start_datetime\")\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\nFound 84 items"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "title": "Open and visualize COGs",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "title": "Open and visualize COGs",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataSet using stackstac\n\n# This is a workaround that is planning to move up into stackstac itself\nimport rasterio as rio\nimport boto3\n\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(session=rio.session.AWSSession(boto3.Session()))\n)\n\n\nda = stackstac.stack(search.item_collection(), gdal_env=gdal_env)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)})\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-325375d6140281389457035ea71ca0fb' (time: 84,\n                                                                band: 1,\n                                                                y: 1800, x: 3600)&gt;\ndask.array&lt;fetch_raster_window, shape=(84, 1, 1800, 3600), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/15)\n  * time            (time) datetime64[ns] 2016-01-01 2016-02-01 ... 2022-12-01\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ... '...\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 90.0 89.9 89.8 89.7 ... -89.6 -89.7 -89.8 -89.9\n    start_datetime  (time) &lt;U19 '2016-01-01T00:00:00' ... '2022-12-01T00:00:00'\n    ...              ...\n    proj:shape      object {1800.0, 3600.0}\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    proj:bbox       object {180.0, -180.0, 90.0, -90.0}\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    title           &lt;U17 'Default COG Layer'\n    epsg            float64 4.326e+03\nAttributes:\n    spec:        RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0),...\n    crs:         epsg:4326.0\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-325375d6140281389457035ea71ca0fb'time: 84band: 1y: 1800x: 3600dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.06 GiB\n8.00 MiB\n\n\nShape\n(84, 1, 1800, 3600)\n(1, 1, 1024, 1024)\n\n\nDask graph\n672 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (15)time(time)datetime64[ns]2016-01-01 ... 2022-12-01array(['2016-01-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2022-01-01T00:00:00.000000000', '2022-02-01T00:00:00.000000000',\n       '2022-03-01T00:00:00.000000000', '2022-04-01T00:00:00.000000000',\n       '2022-05-01T00:00:00.000000000', '2022-06-01T00:00:00.000000000',\n       '2022-07-01T00:00:00.000000000', '2022-08-01T00:00:00.000000000',\n       '2022-09-01T00:00:00.000000000', '2022-10-01T00:00:00.000000000',\n       '2022-11-01T00:00:00.000000000', '2022-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc'], dtype='&lt;U37')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])start_datetime(time)&lt;U19'2016-01-01T00:00:00' ... '2022-...array(['2016-01-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-12-01T00:00:00',\n       '2017-01-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-12-01T00:00:00',\n       '2018-01-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-12-01T00:00:00',\n       '2019-01-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-04-01T00:00:00',\n...\n       '2019-09-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-12-01T00:00:00',\n       '2020-01-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-12-01T00:00:00',\n       '2021-01-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-12-01T00:00:00',\n       '2022-01-01T00:00:00', '2022-02-01T00:00:00',\n       '2022-03-01T00:00:00', '2022-04-01T00:00:00',\n       '2022-05-01T00:00:00', '2022-06-01T00:00:00',\n       '2022-07-01T00:00:00', '2022-08-01T00:00:00',\n       '2022-09-01T00:00:00', '2022-10-01T00:00:00',\n       '2022-11-01T00:00:00', '2022-12-01T00:00:00'], dtype='&lt;U19')proj:transform()object{0.1, 0.0, -0.1, 1.0, -180.0, 90.0}array({0.1, 0.0, -0.1, 1.0, -180.0, 90.0}, dtype=object)proj:epsg()float644.326e+03array(4326.)end_datetime(time)&lt;U19'2016-01-31T00:00:00' ... '2022-...array(['2016-01-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-03-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-07-31T00:00:00', '2016-08-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-12-31T00:00:00',\n       '2017-01-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-03-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-07-31T00:00:00', '2017-08-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-12-31T00:00:00',\n       '2018-01-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-03-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-07-31T00:00:00', '2018-08-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-12-31T00:00:00',\n       '2019-01-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-03-31T00:00:00', '2019-04-30T00:00:00',\n...\n       '2019-09-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-12-31T00:00:00',\n       '2020-01-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-03-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-07-31T00:00:00', '2020-08-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-12-31T00:00:00',\n       '2021-01-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-03-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-07-31T00:00:00', '2021-08-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-12-31T00:00:00',\n       '2022-01-31T00:00:00', '2022-02-28T00:00:00',\n       '2022-03-31T00:00:00', '2022-04-30T00:00:00',\n       '2022-05-31T00:00:00', '2022-06-30T00:00:00',\n       '2022-07-31T00:00:00', '2022-08-31T00:00:00',\n       '2022-09-30T00:00:00', '2022-10-31T00:00:00',\n       '2022-11-30T00:00:00', '2022-12-31T00:00:00'], dtype='&lt;U19')proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)proj:bbox()object{180.0, -180.0, 90.0, -90.0}array({180.0, -180.0, 90.0, -90.0}, dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')epsg()float644.326e+03array(4326.)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01', '2016-04-01',\n               '2016-05-01', '2016-06-01', '2016-07-01', '2016-08-01',\n               '2016-09-01', '2016-10-01', '2016-11-01', '2016-12-01',\n               '2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01',\n               '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01',\n               '2017-09-01', '2017-10-01', '2017-11-01', '2017-12-01',\n               '2018-01-01', '2018-02-01', '2018-03-01', '2018-04-01',\n               '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n               '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n               '2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-06-01', '2019-07-01', '2019-08-01',\n               '2019-09-01', '2019-10-01', '2019-11-01', '2019-12-01',\n               '2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',\n               '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',\n               '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01',\n               '2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01',\n               '2021-05-01', '2021-06-01', '2021-07-01', '2021-08-01',\n               '2021-09-01', '2021-10-01', '2021-11-01', '2021-12-01',\n               '2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01',\n               '2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01',\n               '2022-09-01', '2022-10-01', '2022-11-01', '2022-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([            -180.0,             -179.9,             -179.8,\n                          -179.7,             -179.6,             -179.5,\n                          -179.4,             -179.3,             -179.2,\n                          -179.1,\n              ...\n                           179.0, 179.10000000000002, 179.20000000000005,\n                           179.3, 179.40000000000003,              179.5,\n              179.60000000000002, 179.70000000000005,              179.8,\n              179.90000000000003],\n             dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Float64Index([              90.0,               89.9,               89.8,\n                            89.7,               89.6,               89.5,\n                            89.4,               89.3,               89.2,\n                            89.1,\n              ...\n                           -89.0, -89.10000000000002, -89.20000000000002,\n              -89.30000000000001,              -89.4,              -89.5,\n              -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                           -89.9],\n             dtype='float64', name='y', length=1800))Attributes: (4)spec :RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326.0transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "title": "Open and visualize COGs",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.rio.clip([france_aoi[\"geometry\"]])\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-325375d6140281389457035ea71ca0fb' (time: 84,\n                                                                band: 1, y: 97,\n                                                                x: 143)&gt;\ndask.array&lt;getitem, shape=(84, 1, 97, 143), dtype=float64, chunksize=(1, 1, 97, 143), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n  * time            (time) datetime64[ns] 2016-01-01 2016-02-01 ... 2022-12-01\n    id              (time) &lt;U37 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ... '...\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -4.7 -4.6 -4.5 -4.4 -4.3 ... 9.1 9.2 9.3 9.4 9.5\n  * y               (y) float64 51.0 50.9 50.8 50.7 50.6 ... 41.7 41.6 41.5 41.4\n    start_datetime  (time) &lt;U19 '2016-01-01T00:00:00' ... '2022-12-01T00:00:00'\n    ...              ...\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-180.0, -90....\n    proj:bbox       object {90.0, 180.0, -90.0, -180.0}\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    title           &lt;U17 'Default COG Layer'\n    epsg            float64 4.326e+03\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0),...\n    resolution:  0.1xarray.DataArray'stackstac-325375d6140281389457035ea71ca0fb'time: 84band: 1y: 97x: 143dask.array&lt;chunksize=(1, 1, 97, 143), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n8.89 MiB\n108.37 kiB\n\n\nShape\n(84, 1, 97, 143)\n(1, 1, 97, 143)\n\n\nDask graph\n84 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (16)time(time)datetime64[ns]2016-01-01 ... 2022-12-01array(['2016-01-01T00:00:00.000000000', '2016-02-01T00:00:00.000000000',\n       '2016-03-01T00:00:00.000000000', '2016-04-01T00:00:00.000000000',\n       '2016-05-01T00:00:00.000000000', '2016-06-01T00:00:00.000000000',\n       '2016-07-01T00:00:00.000000000', '2016-08-01T00:00:00.000000000',\n       '2016-09-01T00:00:00.000000000', '2016-10-01T00:00:00.000000000',\n       '2016-11-01T00:00:00.000000000', '2016-12-01T00:00:00.000000000',\n       '2017-01-01T00:00:00.000000000', '2017-02-01T00:00:00.000000000',\n       '2017-03-01T00:00:00.000000000', '2017-04-01T00:00:00.000000000',\n       '2017-05-01T00:00:00.000000000', '2017-06-01T00:00:00.000000000',\n       '2017-07-01T00:00:00.000000000', '2017-08-01T00:00:00.000000000',\n       '2017-09-01T00:00:00.000000000', '2017-10-01T00:00:00.000000000',\n       '2017-11-01T00:00:00.000000000', '2017-12-01T00:00:00.000000000',\n       '2018-01-01T00:00:00.000000000', '2018-02-01T00:00:00.000000000',\n       '2018-03-01T00:00:00.000000000', '2018-04-01T00:00:00.000000000',\n       '2018-05-01T00:00:00.000000000', '2018-06-01T00:00:00.000000000',\n       '2018-07-01T00:00:00.000000000', '2018-08-01T00:00:00.000000000',\n       '2018-09-01T00:00:00.000000000', '2018-10-01T00:00:00.000000000',\n       '2018-11-01T00:00:00.000000000', '2018-12-01T00:00:00.000000000',\n       '2019-01-01T00:00:00.000000000', '2019-02-01T00:00:00.000000000',\n       '2019-03-01T00:00:00.000000000', '2019-04-01T00:00:00.000000000',\n       '2019-05-01T00:00:00.000000000', '2019-06-01T00:00:00.000000000',\n       '2019-07-01T00:00:00.000000000', '2019-08-01T00:00:00.000000000',\n       '2019-09-01T00:00:00.000000000', '2019-10-01T00:00:00.000000000',\n       '2019-11-01T00:00:00.000000000', '2019-12-01T00:00:00.000000000',\n       '2020-01-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000',\n       '2021-01-01T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-03-01T00:00:00.000000000', '2021-04-01T00:00:00.000000000',\n       '2021-05-01T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n       '2021-07-01T00:00:00.000000000', '2021-08-01T00:00:00.000000000',\n       '2021-09-01T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n       '2021-11-01T00:00:00.000000000', '2021-12-01T00:00:00.000000000',\n       '2022-01-01T00:00:00.000000000', '2022-02-01T00:00:00.000000000',\n       '2022-03-01T00:00:00.000000000', '2022-04-01T00:00:00.000000000',\n       '2022-05-01T00:00:00.000000000', '2022-06-01T00:00:00.000000000',\n       '2022-07-01T00:00:00.000000000', '2022-08-01T00:00:00.000000000',\n       '2022-09-01T00:00:00.000000000', '2022-10-01T00:00:00.000000000',\n       '2022-11-01T00:00:00.000000000', '2022-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc'], dtype='&lt;U37')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-4.7 -4.6 -4.5 -4.4 ... 9.3 9.4 9.5axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4. , -3.9, -3.8, -3.7, -3.6,\n       -3.5, -3.4, -3.3, -3.2, -3.1, -3. , -2.9, -2.8, -2.7, -2.6, -2.5, -2.4,\n       -2.3, -2.2, -2.1, -2. , -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2,\n       -1.1, -1. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,\n        1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,\n        2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,\n        3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,\n        4.9,  5. ,  5.1,  5.2,  5.3,  5.4,  5.5,  5.6,  5.7,  5.8,  5.9,  6. ,\n        6.1,  6.2,  6.3,  6.4,  6.5,  6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,\n        7.3,  7.4,  7.5,  7.6,  7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,\n        8.5,  8.6,  8.7,  8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5])y(y)float6451.0 50.9 50.8 ... 41.6 41.5 41.4axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([51. , 50.9, 50.8, 50.7, 50.6, 50.5, 50.4, 50.3, 50.2, 50.1, 50. , 49.9,\n       49.8, 49.7, 49.6, 49.5, 49.4, 49.3, 49.2, 49.1, 49. , 48.9, 48.8, 48.7,\n       48.6, 48.5, 48.4, 48.3, 48.2, 48.1, 48. , 47.9, 47.8, 47.7, 47.6, 47.5,\n       47.4, 47.3, 47.2, 47.1, 47. , 46.9, 46.8, 46.7, 46.6, 46.5, 46.4, 46.3,\n       46.2, 46.1, 46. , 45.9, 45.8, 45.7, 45.6, 45.5, 45.4, 45.3, 45.2, 45.1,\n       45. , 44.9, 44.8, 44.7, 44.6, 44.5, 44.4, 44.3, 44.2, 44.1, 44. , 43.9,\n       43.8, 43.7, 43.6, 43.5, 43.4, 43.3, 43.2, 43.1, 43. , 42.9, 42.8, 42.7,\n       42.6, 42.5, 42.4, 42.3, 42.2, 42.1, 42. , 41.9, 41.8, 41.7, 41.6, 41.5,\n       41.4])start_datetime(time)&lt;U19'2016-01-01T00:00:00' ... '2022-...array(['2016-01-01T00:00:00', '2016-02-01T00:00:00',\n       '2016-03-01T00:00:00', '2016-04-01T00:00:00',\n       '2016-05-01T00:00:00', '2016-06-01T00:00:00',\n       '2016-07-01T00:00:00', '2016-08-01T00:00:00',\n       '2016-09-01T00:00:00', '2016-10-01T00:00:00',\n       '2016-11-01T00:00:00', '2016-12-01T00:00:00',\n       '2017-01-01T00:00:00', '2017-02-01T00:00:00',\n       '2017-03-01T00:00:00', '2017-04-01T00:00:00',\n       '2017-05-01T00:00:00', '2017-06-01T00:00:00',\n       '2017-07-01T00:00:00', '2017-08-01T00:00:00',\n       '2017-09-01T00:00:00', '2017-10-01T00:00:00',\n       '2017-11-01T00:00:00', '2017-12-01T00:00:00',\n       '2018-01-01T00:00:00', '2018-02-01T00:00:00',\n       '2018-03-01T00:00:00', '2018-04-01T00:00:00',\n       '2018-05-01T00:00:00', '2018-06-01T00:00:00',\n       '2018-07-01T00:00:00', '2018-08-01T00:00:00',\n       '2018-09-01T00:00:00', '2018-10-01T00:00:00',\n       '2018-11-01T00:00:00', '2018-12-01T00:00:00',\n       '2019-01-01T00:00:00', '2019-02-01T00:00:00',\n       '2019-03-01T00:00:00', '2019-04-01T00:00:00',\n...\n       '2019-09-01T00:00:00', '2019-10-01T00:00:00',\n       '2019-11-01T00:00:00', '2019-12-01T00:00:00',\n       '2020-01-01T00:00:00', '2020-02-01T00:00:00',\n       '2020-03-01T00:00:00', '2020-04-01T00:00:00',\n       '2020-05-01T00:00:00', '2020-06-01T00:00:00',\n       '2020-07-01T00:00:00', '2020-08-01T00:00:00',\n       '2020-09-01T00:00:00', '2020-10-01T00:00:00',\n       '2020-11-01T00:00:00', '2020-12-01T00:00:00',\n       '2021-01-01T00:00:00', '2021-02-01T00:00:00',\n       '2021-03-01T00:00:00', '2021-04-01T00:00:00',\n       '2021-05-01T00:00:00', '2021-06-01T00:00:00',\n       '2021-07-01T00:00:00', '2021-08-01T00:00:00',\n       '2021-09-01T00:00:00', '2021-10-01T00:00:00',\n       '2021-11-01T00:00:00', '2021-12-01T00:00:00',\n       '2022-01-01T00:00:00', '2022-02-01T00:00:00',\n       '2022-03-01T00:00:00', '2022-04-01T00:00:00',\n       '2022-05-01T00:00:00', '2022-06-01T00:00:00',\n       '2022-07-01T00:00:00', '2022-08-01T00:00:00',\n       '2022-09-01T00:00:00', '2022-10-01T00:00:00',\n       '2022-11-01T00:00:00', '2022-12-01T00:00:00'], dtype='&lt;U19')proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:epsg()float644.326e+03array(4326.)end_datetime(time)&lt;U19'2016-01-31T00:00:00' ... '2022-...array(['2016-01-31T00:00:00', '2016-02-29T00:00:00',\n       '2016-03-31T00:00:00', '2016-04-30T00:00:00',\n       '2016-05-31T00:00:00', '2016-06-30T00:00:00',\n       '2016-07-31T00:00:00', '2016-08-31T00:00:00',\n       '2016-09-30T00:00:00', '2016-10-31T00:00:00',\n       '2016-11-30T00:00:00', '2016-12-31T00:00:00',\n       '2017-01-31T00:00:00', '2017-02-28T00:00:00',\n       '2017-03-31T00:00:00', '2017-04-30T00:00:00',\n       '2017-05-31T00:00:00', '2017-06-30T00:00:00',\n       '2017-07-31T00:00:00', '2017-08-31T00:00:00',\n       '2017-09-30T00:00:00', '2017-10-31T00:00:00',\n       '2017-11-30T00:00:00', '2017-12-31T00:00:00',\n       '2018-01-31T00:00:00', '2018-02-28T00:00:00',\n       '2018-03-31T00:00:00', '2018-04-30T00:00:00',\n       '2018-05-31T00:00:00', '2018-06-30T00:00:00',\n       '2018-07-31T00:00:00', '2018-08-31T00:00:00',\n       '2018-09-30T00:00:00', '2018-10-31T00:00:00',\n       '2018-11-30T00:00:00', '2018-12-31T00:00:00',\n       '2019-01-31T00:00:00', '2019-02-28T00:00:00',\n       '2019-03-31T00:00:00', '2019-04-30T00:00:00',\n...\n       '2019-09-30T00:00:00', '2019-10-31T00:00:00',\n       '2019-11-30T00:00:00', '2019-12-31T00:00:00',\n       '2020-01-31T00:00:00', '2020-02-29T00:00:00',\n       '2020-03-31T00:00:00', '2020-04-30T00:00:00',\n       '2020-05-31T00:00:00', '2020-06-30T00:00:00',\n       '2020-07-31T00:00:00', '2020-08-31T00:00:00',\n       '2020-09-30T00:00:00', '2020-10-31T00:00:00',\n       '2020-11-30T00:00:00', '2020-12-31T00:00:00',\n       '2021-01-31T00:00:00', '2021-02-28T00:00:00',\n       '2021-03-31T00:00:00', '2021-04-30T00:00:00',\n       '2021-05-31T00:00:00', '2021-06-30T00:00:00',\n       '2021-07-31T00:00:00', '2021-08-31T00:00:00',\n       '2021-09-30T00:00:00', '2021-10-31T00:00:00',\n       '2021-11-30T00:00:00', '2021-12-31T00:00:00',\n       '2022-01-31T00:00:00', '2022-02-28T00:00:00',\n       '2022-03-31T00:00:00', '2022-04-30T00:00:00',\n       '2022-05-31T00:00:00', '2022-06-30T00:00:00',\n       '2022-07-31T00:00:00', '2022-08-31T00:00:00',\n       '2022-09-30T00:00:00', '2022-10-31T00:00:00',\n       '2022-11-30T00:00:00', '2022-12-31T00:00:00'], dtype='&lt;U19')proj:shape()object{1800.0, 3600.0}array({1800.0, 3600.0}, dtype=object)proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')epsg()float644.326e+03array(4326.)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-4.7499999999999885 0.09999999999999992 0.0 51.05 0.0 -0.10000000000000002array(0)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['2016-01-01', '2016-02-01', '2016-03-01', '2016-04-01',\n               '2016-05-01', '2016-06-01', '2016-07-01', '2016-08-01',\n               '2016-09-01', '2016-10-01', '2016-11-01', '2016-12-01',\n               '2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01',\n               '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01',\n               '2017-09-01', '2017-10-01', '2017-11-01', '2017-12-01',\n               '2018-01-01', '2018-02-01', '2018-03-01', '2018-04-01',\n               '2018-05-01', '2018-06-01', '2018-07-01', '2018-08-01',\n               '2018-09-01', '2018-10-01', '2018-11-01', '2018-12-01',\n               '2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-06-01', '2019-07-01', '2019-08-01',\n               '2019-09-01', '2019-10-01', '2019-11-01', '2019-12-01',\n               '2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',\n               '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',\n               '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01',\n               '2021-01-01', '2021-02-01', '2021-03-01', '2021-04-01',\n               '2021-05-01', '2021-06-01', '2021-07-01', '2021-08-01',\n               '2021-09-01', '2021-10-01', '2021-11-01', '2021-12-01',\n               '2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01',\n               '2022-05-01', '2022-06-01', '2022-07-01', '2022-08-01',\n               '2022-09-01', '2022-10-01', '2022-11-01', '2022-12-01'],\n              dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([ -4.699999999999989,  -4.599999999999994,                -4.5,\n               -4.399999999999977,  -4.299999999999983,  -4.199999999999989,\n               -4.099999999999994,                -4.0, -3.8999999999999773,\n               -3.799999999999983,\n              ...\n                8.600000000000023,   8.700000000000017,   8.800000000000011,\n                8.900000000000006,                 9.0,   9.100000000000023,\n                9.200000000000017,   9.300000000000011,   9.400000000000006,\n                              9.5],\n             dtype='float64', name='x', length=143))yPandasIndexPandasIndex(Float64Index([              51.0,               50.9,               50.8,\n              50.699999999999996, 50.599999999999994,               50.5,\n                            50.4,               50.3, 50.199999999999996,\n              50.099999999999994,               50.0,               49.9,\n                            49.8, 49.699999999999996, 49.599999999999994,\n                            49.5,               49.4,               49.3,\n              49.199999999999996, 49.099999999999994,               49.0,\n                            48.9,               48.8, 48.699999999999996,\n              48.599999999999994,               48.5,               48.4,\n                            48.3, 48.199999999999996, 48.099999999999994,\n                            48.0,               47.9,               47.8,\n              47.699999999999996, 47.599999999999994,               47.5,\n                            47.4,               47.3, 47.199999999999996,\n              47.099999999999994,               47.0,               46.9,\n                            46.8, 46.699999999999996, 46.599999999999994,\n                            46.5,               46.4,               46.3,\n              46.199999999999996, 46.099999999999994,               46.0,\n                            45.9,               45.8, 45.699999999999996,\n              45.599999999999994,               45.5,               45.4,\n                            45.3, 45.199999999999996, 45.099999999999994,\n                            45.0,               44.9,               44.8,\n              44.699999999999996, 44.599999999999994,               44.5,\n                            44.4,               44.3, 44.199999999999996,\n              44.099999999999994,               44.0,               43.9,\n                            43.8, 43.699999999999996, 43.599999999999994,\n                            43.5,               43.4,               43.3,\n              43.199999999999996, 43.099999999999994,               43.0,\n                            42.9,               42.8, 42.699999999999996,\n              42.599999999999994,               42.5,               42.4,\n                            42.3, 42.199999999999996, 42.099999999999994,\n                            42.0,               41.9,               41.8,\n              41.699999999999996, 41.599999999999994,               41.5,\n                            41.4],\n             dtype='float64', name='y'))Attributes: (2)spec :RasterSpec(epsg=4326.0, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#select-a-band-of-data",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#select-a-band-of-data",
    "title": "Open and visualize COGs",
    "section": "Select a band of data",
    "text": "Select a band of data\nThere is just one band in this case, cog_default.\n\ndata_band = subset.sel(band=\"cog_default\")"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "href": "notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "title": "Open and visualize COGs",
    "section": "Compute and plot",
    "text": "Compute and plot\nSo far we have just been setting up a calculation lazily in Dask. Now we can trigger computation using .compute().\n\n%%time\n\nimage_stack = data_band.compute()\n\nCPU times: user 3.22 s, sys: 865 ms, total: 4.09 s\nWall time: 9.73 s\n\n\n\n# get the 2% and 98% percentiles for min and max bounds of color\nvmin, vmax = image_stack.quantile(0.02).item(), image_stack.quantile(0.98).item()\n\n# decode the data defined crs - this should be easier in `hvplot`\ncrs = str(ccrs.CRS(da.crs).to_epsg())\n\nimage_stack.hvplot(\n    groupby=\"time\",\n    crs=crs,\n    tiles=True,\n    colorbar=False,\n    clim=(vmin, vmax),\n    cmap=\"viridis\",\n    alpha=0.8,\n    frame_height=512,\n    widget_location=\"bottom\",\n)"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html",
    "href": "notebooks/quickstarts/visualize-zarr.html",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "href": "notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#approach",
    "href": "notebooks/quickstarts/visualize-zarr.html#approach",
    "title": "Visualize zarr",
    "section": "Approach",
    "text": "Approach\n\nUse intake to open a STAC collection using with xarray and dask\nPlot the data using hvplot"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "href": "notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "title": "Visualize zarr",
    "section": "About the data",
    "text": "About the data\nThis is the Gridded Daily OCO-2 Carbon Dioxide assimilated dataset. More information can be found at: OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2 V10r (OCO2_GEOS_L3CO2_DAY)\nThe data has been converted to zarr format and published to the development version of the VEDA STAC Catalog.\n\nimport intake\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "title": "Visualize zarr",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://dev-stac.delta-backend.com/collections\nSTAC Browser: http://veda-dev-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://dev-stac.delta-backend.com/\"\ncollection_id = \"oco2-geos-l3-daily\""
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "href": "notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "title": "Visualize zarr",
    "section": "Get STAC collection",
    "text": "Get STAC collection\nUse intake to get the entire STAC collection.\n\ncollection = intake.open_stac_collection(f\"{STAC_API_URL}/collections/{collection_id}\")\ncollection\n\noco2-geos-l3-daily:\n  args:\n    stac_obj: https://dev-stac.delta-backend.com//collections/oco2-geos-l3-daily\n  description: ''\n  driver: intake_stac.catalog.StacCollection\n  metadata:\n    assets:\n      zarr:\n        description: ''\n        href: s3://veda-data-store-staging/EIS/zarr/OCO2_GEOS_L3CO2_day.zarr\n        roles:\n        - data\n        - zarr\n        title: oco2-geos-l3-daily Zarr root\n        type: application/vnd+zarr\n        xarray:open_kwargs:\n          chunks: {}\n          consolidated: true\n          engine: zarr\n    cube:dimensions:\n      lat:\n        axis: y\n        description: latitude\n        extent:\n        - -90.0\n        - 90.0\n        reference_system: 4326\n        type: spatial\n      lon:\n        axis: x\n        description: longitude\n        extent:\n        - -180.0\n        - 179.375\n        reference_system: 4326\n        type: spatial\n      time:\n        description: time\n        extent:\n        - '2015-01-01T12:00:00Z'\n        - '2021-11-04T12:00:00Z'\n        step: P1DT0H0M0S\n        type: temporal\n    cube:variables:\n      XCO2:\n        attrs:\n          long_name: Assimilated dry-air column average CO2 daily mean\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Assimilated dry-air column average CO2 daily mean\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n      XCO2PREC:\n        attrs:\n          long_name: Precision of dry-air column average CO2 daily mean from Desroziers\n            et al. (2005) diagnostic\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Precision of dry-air column average CO2 daily mean from Desroziers\n          et al. (2005) diagnostic\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n    dashboard:is_periodic: true\n    dashboard:time_density: day\n    description: \"The OCO-2 mission provides the highest quality space-based XCO2\\\n      \\ retrievals to date. However, the instrument data are characterized by large\\\n      \\ gaps in coverage due to OCO-2\\u2019s narrow 10-km ground track and an inability\\\n      \\ to see through clouds and thick aerosols. This global gridded dataset is produced\\\n      \\ using a data assimilation technique commonly referred to as state estimation\\\n      \\ within the geophysical literature. Data assimilation synthesizes simulations\\\n      \\ and observations, adjusting the state of atmospheric constituents like CO2\\\n      \\ to reflect observed values, thus gap-filling observations when and where they\\\n      \\ are unavailable based on previous observations and short transport simulations\\\n      \\ by GEOS. Compared to other methods, data assimilation has the advantage that\\\n      \\ it makes estimates based on our collective scientific understanding, notably\\\n      \\ of the Earth\\u2019s carbon cycle and atmospheric transport. OCO-2 GEOS (Goddard\\\n      \\ Earth Observing System) Level 3 data are produced by ingesting OCO-2 L2 retrievals\\\n      \\ every 6 hours with GEOS CoDAS, a modeling and data assimilation system maintained\\\n      \\ by NASA\\u2019s Global Modeling and Assimilation Office (GMAO). GEOS CoDAS\\\n      \\ uses a high-performance computing implementation of the Gridpoint Statistical\\\n      \\ Interpolation approach for solving the state estimation problem. GSI finds\\\n      \\ the analyzed state that minimizes the three-dimensional variational (3D-Var)\\\n      \\ cost function formulation of the state estimation problem.\"\n    extent:\n      spatial:\n        bbox:\n        - - -180.0\n          - -90.0\n          - 180.0\n          - 90.0\n      temporal:\n        interval:\n        - - '2015-01-01T12:00:00Z'\n          - '2021-11-04T12:00:00Z'\n    id: oco2-geos-l3-daily\n    license: CC0-1.0\n    stac_extensions:\n    - https://stac-extensions.github.io/datacube/v2.0.0/schema.json\n    stac_version: 1.0.0\n    title: Gridded Daily OCO-2 Carbon Dioxide assimilated dataset\n    type: Collection"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "href": "notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "title": "Visualize zarr",
    "section": "Read from zarr to xarray",
    "text": "Read from zarr to xarray\nIntake lets you go straight from the asset to an xarray dataset backed by a dask array.\n\nsource = collection.get_asset(\"zarr\")\n\nds = source.to_dask()\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:   (time: 2500, lat: 361, lon: 576)\nCoordinates:\n  * lat       (lat) float64 -90.0 -89.5 -89.0 -88.5 ... 88.5 89.0 89.5 90.0\n  * lon       (lon) float64 -180.0 -179.4 -178.8 -178.1 ... 178.1 178.8 179.4\n  * time      (time) datetime64[ns] 2015-01-01T12:00:00 ... 2021-11-04T12:00:00\nData variables:\n    XCO2      (time, lat, lon) float64 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    XCO2PREC  (time, lat, lon) float64 dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\nAttributes: (12/25)\n    BuildId:                        B10.2.06\n    Contact:                        Brad Weir (brad.weir@nasa.gov)\n    Conventions:                    CF-1\n    DataResolution:                 0.5x0.625\n    EastBoundingCoordinate:         179.375\n    Format:                         NetCDF-4/HDF-5\n    ...                             ...\n    ShortName:                      OCO2_GEOS_L3CO2_DAY_10r\n    SouthBoundingCoordinate:        -90.0\n    SpatialCoverage:                global\n    Title:                          OCO-2 GEOS Level 3 daily, 0.5x0.625 assim...\n    VersionID:                      V10r\n    WestBoundingCoordinate:         -180.0xarray.DatasetDimensions:time: 2500lat: 361lon: 576Coordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2015-01-01T12:00:00 ... 2021-11-...begin_date :20170801begin_time :120000long_name :timearray(['2015-01-01T12:00:00.000000000', '2015-01-02T12:00:00.000000000',\n       '2015-01-03T12:00:00.000000000', ..., '2021-11-02T12:00:00.000000000',\n       '2021-11-03T12:00:00.000000000', '2021-11-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)XCO2(time, lat, lon)float64dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Assimilated dry-air column average CO2 daily meanunits :mol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nCount\n601 Tasks\n600 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n\nXCO2PREC\n\n\n(time, lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\nlong_name :\n\nPrecision of dry-air column average CO2 daily mean from Desroziers et al. (2005) diagnostic\n\nunits :\n\nmol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nCount\n601 Tasks\n600 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n\nAttributes: (25)BuildId :B10.2.06Contact :Brad Weir (brad.weir@nasa.gov)Conventions :CF-1DataResolution :0.5x0.625EastBoundingCoordinate :179.375Format :NetCDF-4/HDF-5History :Original file generated: Tue Mar 15 12:02:48 2022 GMTIdentifierProductDOI :10.5067/Y9M4NM9MPCGHIdentifierProductDOIAuthority :http://doi.org/Institution :NASA GSFC Global Modeling and Assimilation Office and OCO-2 Project, Jet Propulsion LaboratoryLatitudeResolution :0.5LongName :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2LongitudeResolution :0.625NorthBoundingCoordinate :90.0ProductionDateTime :2022-03-15T12:02:48ZRangeBeginningDate :2017-08-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2017-08-01RangeEndingTime :23:59:99.999999ShortName :OCO2_GEOS_L3CO2_DAY_10rSouthBoundingCoordinate :-90.0SpatialCoverage :globalTitle :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2VersionID :V10rWestBoundingCoordinate :-180.0\n\n\nIn xarray you can inspect just one data variable using dot notation:\n\nds.XCO2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'XCO2' (time: 2500, lat: 361, lon: 576)&gt;\ndask.array&lt;open_dataset-93b4c29ffd858875ace137bcbf8bc157XCO2, shape=(2500, 361, 576), dtype=float64, chunksize=(100, 100, 100), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 -90.0 -89.5 -89.0 -88.5 -88.0 ... 88.5 89.0 89.5 90.0\n  * lon      (lon) float64 -180.0 -179.4 -178.8 -178.1 ... 178.1 178.8 179.4\n  * time     (time) datetime64[ns] 2015-01-01T12:00:00 ... 2021-11-04T12:00:00\nAttributes:\n    long_name:  Assimilated dry-air column average CO2 daily mean\n    units:      mol CO2/mol dryxarray.DataArray'XCO2'time: 2500lat: 361lon: 576dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nCount\n601 Tasks\n600 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2015-01-01T12:00:00 ... 2021-11-...begin_date :20170801begin_time :120000long_name :timearray(['2015-01-01T12:00:00.000000000', '2015-01-02T12:00:00.000000000',\n       '2015-01-03T12:00:00.000000000', ..., '2021-11-02T12:00:00.000000000',\n       '2021-11-03T12:00:00.000000000', '2021-11-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')Attributes: (2)long_name :Assimilated dry-air column average CO2 daily meanunits :mol CO2/mol dry"
  },
  {
    "objectID": "notebooks/quickstarts/visualize-zarr.html#plot-data",
    "href": "notebooks/quickstarts/visualize-zarr.html#plot-data",
    "title": "Visualize zarr",
    "section": "Plot data",
    "text": "Plot data\nWe can plot the XCO2 variable as an interactive map (with date slider) using hvplot.\n\nds.XCO2.hvplot(\n    x=\"lon\",\n    y=\"lat\",\n    groupby=\"time\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)\n\nWARNING:param.main: Calling the .opts method with options broken down by options group (i.e. separate plot, style and norm groups) is deprecated. Use the .options method converting to the simplified format instead or use hv.opts.apply_groups for backward compatibility."
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html",
    "href": "notebooks/quickstarts/timeseries-stac-api.html",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#approach",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#approach",
    "title": "Get timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUsing a list of STAC items and a bouding box fetch stats from /cog/statistics endpoint\nGenerate a timeseries plot using statistics from each time step\nSpeed up workflow using Dask\n\n\nimport requests\nimport folium\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_name = \"no2-monthly\""
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2022-12-01 00:00:00+00']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2022-12-01T00:00:00Z'],\n  'cog_default': {'max': 50064805976866816, 'min': -10183824872833024}},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\nDescribe the periodic nature of the data\nIn the collection above we will pay particular attention to the fields that define the periodicity of the data.\n\ncollection[\"dashboard:is_periodic\"]\n\nTrue\n\n\n\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\ncollection[\"summaries\"]\n\n{'datetime': ['2016-01-01T00:00:00Z', '2022-12-01T00:00:00Z'],\n 'cog_default': {'max': 50064805976866816, 'min': -10183824872833024}}"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC items",
    "text": "Fetch STAC items\nGet the list of all the STAC items within this collection.\n\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nlen(items)\n\n84\n\n\nWe can inspect one of these items to get a sense of what metadata is available.\n\nitems[0]\n\n{'id': 'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202212_Col3_V4.nc'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 38843752944500740,\n      'min': -6707683428139008.0,\n      'count': 11.0,\n      'buckets': [58.0,\n       423787.0,\n       8532.0,\n       1008.0,\n       339.0,\n       199.0,\n       79.0,\n       4.0,\n       0.0,\n       2.0]},\n     'statistics': {'mean': 423085978017950.7,\n      'stddev': 971153921053730.1,\n      'maximum': 38843752944500740,\n      'minimum': -6707683428139008.0,\n      'valid_percent': 82.78045654296875}}]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1800.0, 3600.0],\n  'end_datetime': '2022-12-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'start_datetime': '2022-12-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Define an area of interest",
    "text": "Define an area of interest\nWe will be using a bounding box over metropolitan france. We’ll use that bounding box to subset the data when calculating the timeseries.\n\nfrance_bounding_box = {\n    \"type\": \"Feature\",\n    \"properties\": {\"ADMIN\": \"France\", \"ISO_A3\": \"FRA\"},\n    \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n            [\n                [-5.183429, 42.332925],\n                [8.233998, 42.332925],\n                [8.233998, 51.066135],\n                [-5.183429, 51.066135],\n                [-5.183429, 42.332925],\n            ]\n        ],\n    },\n}\n\nLet’s take a look at that box.\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=3,\n)\n\nfolium.GeoJson(france_bounding_box, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "title": "Get timeseries from COGs",
    "section": "Use /cog/statistics to get data for the AOI",
    "text": "Use /cog/statistics to get data for the AOI\nFirst, we create a generate_stats function and then we call it with the bounding box defined for France.\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][\"cog_default\"][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\nGenerate stats\nThis may take some time due to the complexity of the shape we’re requesting. See the end of this notebook for tips on how to speed this up.\n\n%%time\nstats = [generate_stats(item, france_bounding_box) for item in items]\n\nCPU times: user 969 ms, sys: 54 ms, total: 1.02 s\nWall time: 21.8 s\n\n\n\n\nInspect one result\n\nstats[0]\n\n{'statistics': {'1': {'min': -3811088227368960.0,\n   'max': 2.110830182347571e+16,\n   'mean': 2198312557605593.8,\n   'count': 10941.0,\n   'sum': 2.40517376927628e+19,\n   'std': 2015876566313865.5,\n   'median': 1698433945567232.0,\n   'majority': 1910535939424256.0,\n   'minority': -3811088227368960.0,\n   'unique': 10805.0,\n   'histogram': [[44.0,\n     3253.0,\n     6020.0,\n     1061.0,\n     357.0,\n     145.0,\n     43.0,\n     12.0,\n     5.0,\n     1.0],\n    [-3811088227368960.0,\n     -1319149275971584.0,\n     1172789809643520.0,\n     3664728895258624.0,\n     6156667578220544.0,\n     8648607066488832.0,\n     1.1140546017886208e+16,\n     1.3632484969283584e+16,\n     1.612442392068096e+16,\n     1.861636179833651e+16,\n     2.110830182347571e+16]],\n   'valid_percent': 93.85,\n   'masked_pixels': 717.0,\n   'valid_pixels': 10941.0,\n   'percentile_2': -286377359756492.6,\n   'percentile_98': 8518530438581425.0}},\n 'ISO_A3': 'FRA',\n 'ADMIN': 'France',\n 'start_datetime': '2022-12-01T00:00:00'}"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "title": "Get timeseries from COGs",
    "section": "Plot timeseries",
    "text": "Plot timeseries\nIt is easier to interact with these results as a pandas dataframe. The following function takes the json, passes it to pandas, cleans up the column names and parses the date column.\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\n\n\nConstruct the plot\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly NO2 values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NO2 values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NO2 values\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\nIn this graph we can see the yearly cycles in NO2 values due to seasonal variations, as well as a slight downward slope in maximum NO2 values"
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "title": "Get timeseries from COGs",
    "section": "Complex AOI",
    "text": "Complex AOI\nThe values plotted above don’t correspond exactly to Fance, since the bounding box excludes Corsica and overseas territories such as Mayotte and French Polynesia, and covers parts of neighboring countries including Spain, Italy, Germany and the entirety of Luxembourg. We can fetch GeoJSON from an authoritative online source (https://gadm.org/download_country.html).\nWhile the NO2 values above correspond more or less to those of in France, we can be much more precise by using a complex geojson that represents the boundaries of France exactly, including overseas territories in the Carribean and Indian Oceans, and South America.\nNote: In this notebook we write out the whole perimeter as a MultiPolygon in geojson. In practice you will often be reading this kind of shape from a file (usually with the help of geopandas).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe can now request the NO2 values for this complex AOI the same way as for the bounding box.\nNotice, however, that due to the complexity of the shape, it takes much longer to gather the requested data about 4 times as long as for the bounding box example above.\n\n%%time\naoi_df = clean_stats([generate_stats(item, france_aoi) for item in items])\n\nCPU times: user 3.4 s, sys: 63.5 ms, total: 3.47 s\nWall time: 2min 50s\n\n\nWe can compare the mean monthly NO2 values calculated when using the bounding box and when using the country’s exact borders\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"],\n    df[\"mean\"],\n    color=\"blue\",\n    label=\"Mean monthly NO2 values using bounding box\",\n)\nplt.plot(\n    aoi_df[\"date\"],\n    aoi_df[\"mean\"],\n    color=\"red\",\n    label=\"Mean monthly NO2 values using complex AOI\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\nWhile the difference is small, it is very interesting to note that the NO2 values calculated using the exact borders are systematically less than when using the bounding box. This may be due to the fact that the bounding box includes parts of western Germany and northern Italy that have a lot industrial activity, whereas the areas included when using the exact borders that are not included in the bounding box case, are overseas territories much less industrial activity."
  },
  {
    "objectID": "notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "href": "notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "title": "Get timeseries from COGs",
    "section": "Speed things up: parallelize computation with Dask",
    "text": "Speed things up: parallelize computation with Dask\nWe can drastically reduce the time it takes to generate the timeseries, even with the complex AOI above, by parallelizing our code. The cogs/statistics API is powered by AWS Lambda which executes each request in a separate instance. This means the requests are highly scalable. Since each statistics request is for a single timestamp, we can request statistics for multiple timesteps concurrently, and greatly reduce the amount of time needed. We will demonstrate this by using the Dask.\n\nCreate a Dask client\nFirst we will create a Dask client. In this case we will use the threads on the same server that is running this jupyter notebook.\n\nimport dask.distributed\n\nclient = dask.distributed.Client()\n\n\n\nSubmit work\nWe will submit the generate_stats function for each item in our list and collect a list of futures. This will immediately kick off work in dask. We can then gather all the results.\n\n%%time\nfutures = [client.submit(generate_stats, item, france_aoi) for item in items]\nstats = client.gather(futures)\n\n2023-03-31 13:36:46,044 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:46,361 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:46,747 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:47,046 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:47,431 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:47,718 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:48,050 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:48,343 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:48,632 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:48,956 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:49,267 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:49,623 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:49,932 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:50,290 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:50,578 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:50,908 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:51,205 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:51,524 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n2023-03-31 13:36:51,857 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n2023-03-31 13:36:52,160 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n2023-03-31 13:36:52,485 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n2023-03-31 13:36:52,793 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:53,103 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:53,399 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:53,736 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:54,033 - distributed.utils_perf - WARNING - full garbage collections took 29% CPU time recently (threshold: 10%)\n2023-03-31 13:36:54,318 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n2023-03-31 13:36:54,657 - distributed.utils_perf - WARNING - full garbage collections took 30% CPU time recently (threshold: 10%)\n\n\nCPU times: user 19.2 s, sys: 465 ms, total: 19.7 s\nWall time: 47.6 s\n\n\n\n\nClose the Dask client\nIt is good practice to close the client when you are done.\nclient.shutdown()\n\n\nAlternate approach\nIf you are familiar with the concurrent.futures library you can use that instead of Dask."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html",
    "href": "notebooks/templates/template-accessing-the-data-directly.html",
    "title": "Run this notebook",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that access the data directly. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook.\nYou can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#approach",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#approach",
    "title": "Run this notebook",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests\nimport stackstac\n\nfrom pystac_client import Client"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "title": "Run this notebook",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "title": "Run this notebook",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://dev-stac.delta-backend.com/collections\nSTAC Browser: http://veda-dev-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. We use pystac-client to do a search. Here is an some example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Run this notebook",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nbbox = []\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\n\nsearch = catalog.search(\n    bbox=bbox, datetime=datetime, collections=[collection_id], limit=1000\n)\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "title": "Run this notebook",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON from an authoritative online source for instance: https://gadm.org/download_country.html\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\n\nNext some notebooks read in the data. If you are using the raster API to trigger computation server side skip this section. Here is an example of reading the data in using stackstac and clipping using rasterio.\n\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "title": "Run this notebook",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataSet using stackstac\n\n# This is a workaround that is planning to move up into stackstac itself\nimport rasterio as rio\nimport boto3\n\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(AWS_NO_SIGN_REQUEST=True, session=rio.session.AWSSession(boto3.Session())\n)\n\n\nda = stackstac.stack(search.item_collection(), gdal_env=gdal_env)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)})\nda"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "title": "Run this notebook",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.rio.clip([aoi[\"geometry\"]])\nsubset\n\n\nWith the STAC object, and optionally the AOI and/or the data in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is an idea though :)"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "title": "Run this notebook",
    "section": "Select a band of data",
    "text": "Select a band of data\nThere is just one band in this case, cog_default.\n\ndata_band = da.sel(band=\"cog_default\")"
  },
  {
    "objectID": "notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "href": "notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "title": "Run this notebook",
    "section": "Compute and plot",
    "text": "Compute and plot\nCalculate the mean at each time across the whole dataset. Note this is the first time that the data is actually loaded.\n\n# Average over entire AOI for each month\nmeans = data_band.mean(dim=(\"x\", \"y\")).compute()\n\n\nmeans.plot()"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html",
    "href": "notebooks/templates/template-using-the-raster-api.html",
    "title": "Run this notebook",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that use the raster API. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook.\nYou can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#approach",
    "href": "notebooks/templates/template-using-the-raster-api.html#approach",
    "title": "Run this notebook",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "href": "notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "title": "Run this notebook",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "href": "notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "title": "Run this notebook",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://dev-stac.delta-backend.com/collections\nSTAC Browser: http://veda-dev-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. In some notebooks we get the collection and use all the items, and in others we search for specific items."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "href": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "title": "Run this notebook",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_id}\").json()\ncollection"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "href": "notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "title": "Run this notebook",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "href": "notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "title": "Run this notebook",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm\n\n\nWith the STAC object and optionally the AOI in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is one idea though :)"
  },
  {
    "objectID": "notebooks/templates/template-using-the-raster-api.html#use-stactilejson.json-to-get-tiles",
    "href": "notebooks/templates/template-using-the-raster-api.html#use-stactilejson.json-to-get-tiles",
    "title": "Run this notebook",
    "section": "Use /stac/tilejson.json to get tiles",
    "text": "Use /stac/tilejson.json to get tiles\nWe pass the item_id, collection id, and the rescale_values in to the RASTER API endpoint and get back a tile.\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={collection_id}&item={item['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\ntiles\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    attr=\"VEDA\",\n)"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html",
    "href": "notebooks/datasets/nceo-biomass-statistics.html",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#approach",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#approach",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Approach",
    "text": "Approach\n\nQuery STAC API and explore item contents for a given collection\nRead and access the data\nVisualize the collection with hvplot\nRun zonal statistics on collection using rasterstats\nVisualize resultant zonal statistics on a choropleth map"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "About the Data",
    "text": "About the Data\nThe NCEO Aboveground Woody Biomass 2017 dataset is a map for Africa at 100 m spatial resolution which was developed using a combination of LiDAR, Synthetic Aperture Radar (SAR) and optical based data. Aboveground woody biomass (AGB) plays an key role in the study of the Earth’s carbon cycle and response to climate change. Estimation based on Earth Observation measurements is an effective method for regional scale studies and the results are expressed as dry matter in Mg ha-1.\nImportant Note: Users of this dataset should keep in mind that the map is a continental-scale dataset, generated using a combination of different remote sensing data types, with a single method for the whole study area produced in 2017. Users, therefore, should understand that accuracy may vary for different regions and vegetation types."
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "The Case Study - Guinea",
    "text": "The Case Study - Guinea\nMapping and understanding the spatial distribution of AGB is key to understanding carbon dioxide emissions from tropical deforestation through the loss of woody carbon stocks. The resulting carbon fluxes from these land-use changes and vegetation degradation can have negative impacts on the global carbon cycle. Change analysis between AGB maps overtime can display losses in high biomass forests, due to suspected deforestation and forest degredation.\nThe forests of southern Guinea are reported to have some of the highest density AGB of any forest in the world and are one of the most threatened ecoregions in Africa. Importantly, this area was also the epicenter of the 2014 Ebola outbreak, which had a lasting impact on the region. There is more and more evidence that human deforestation activities in this area may have accelerated the spread of the deadly virus as a result of increasing human-bat interactions in the region.\nIn this example we explore the NCEO AGB dataset for 2017, running zonal statistics at the district (administrative 2) level to understand those areas in Guinea that need greatest prioritization for protection and conservation."
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Setting up the Environment",
    "text": "Setting up the Environment\nTo run zonal statistics we’ll need to import a python package called rasterstats into our environment. You can uncomment the following line for installation. This cell needs only needs to be run once.\n\n!pip install rasterstats\n\nRequirement already satisfied: rasterstats in /srv/conda/envs/notebook/lib/python3.10/site-packages (0.18.0)\nRequirement already satisfied: fiona&lt;1.9 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.8.22)\nRequirement already satisfied: shapely in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.8.5.post1)\nRequirement already satisfied: cligj&gt;=0.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (0.7.2)\nRequirement already satisfied: simplejson in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (3.19.1)\nRequirement already satisfied: affine&lt;3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (2.3.1)\nRequirement already satisfied: click&gt;7.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (8.1.3)\nRequirement already satisfied: numpy&gt;=1.9 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.23.5)\nRequirement already satisfied: rasterio&gt;=1.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterstats) (1.3.4)\nRequirement already satisfied: attrs&gt;=17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (22.2.0)\nRequirement already satisfied: six&gt;=1.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (1.16.0)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (65.6.3)\nRequirement already satisfied: certifi in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (2022.12.7)\nRequirement already satisfied: click-plugins&gt;=1.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (1.1.1)\nRequirement already satisfied: munch in /srv/conda/envs/notebook/lib/python3.10/site-packages (from fiona&lt;1.9-&gt;rasterstats) (2.5.0)\nRequirement already satisfied: snuggs&gt;=1.4.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from rasterio&gt;=1.0-&gt;rasterstats) (1.4.7)\nRequirement already satisfied: pyparsing&gt;=2.1.6 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from snuggs&gt;=1.4.1-&gt;rasterio&gt;=1.0-&gt;rasterstats) (3.0.9)"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nfrom pystac_client import Client\n\n\n# Provide STAC API endpoint\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\n\n# Declare collection of interest - NCEO Biomass\ncollection = \"nceo_africa_2017\"\n\nNow let’s check how many total items are available.\n\nsearch = Client.open(STAC_API_URL).search(collections=[collection])\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\nFound 1 items\n\n\nThis makes sense as there is only one item available: a map for 2017.\n\n# Explore the \"cog_default\" asset of one item to see what it contains\nitems[0].assets[\"cog_default\"].to_dict()\n\n{'href': 's3://nasa-maap-data-store/file-staging/nasa-map/nceo-africa-2017/AGB_map_2017v0m_COG.tif',\n 'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n 'title': 'Default COG Layer',\n 'description': 'Cloud optimized default layer to display on map',\n 'raster:bands': [{'scale': 1.0,\n   'nodata': 'inf',\n   'offset': 0.0,\n   'sampling': 'area',\n   'data_type': 'uint16',\n   'histogram': {'max': 429.0,\n    'min': 0.0,\n    'count': 11.0,\n    'buckets': [405348.0,\n     44948.0,\n     18365.0,\n     6377.0,\n     3675.0,\n     3388.0,\n     3785.0,\n     9453.0,\n     13108.0,\n     1186.0]},\n   'statistics': {'mean': 37.58407913145342,\n    'stddev': 81.36678677343947,\n    'maximum': 429.0,\n    'minimum': 0.0,\n    'valid_percent': 50.42436439336373}}],\n 'roles': ['data', 'layer']}\n\n\nExplore through the item’s assets. We can see from the data’s statistics values that the min and max values for the observed values range from 0 to 429 Mg ha-1.\nExploring the properties [\"proj:epsg\"] we also notice something strange, as the value is float and should be integer. We’ll revise this using the code below, but it will be revised upstream in the future.\n\nitems[0].properties[\"proj:epsg\"] = int(items[0].properties[\"proj:epsg\"])"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Reading and accessing the data",
    "text": "Reading and accessing the data\nNow that we’ve explored the dataset through the STAC API, let’s read and access the dataset itself.\n\n# This is a workaround that is planning to move up into stackstac itself\n\nimport boto3\nimport stackstac\nimport rasterio as rio\nimport rioxarray\n\ngdal_env = stackstac.DEFAULT_GDAL_ENV.updated(\n    always=dict(\n        AWS_NO_SIGN_REQUEST=True, session=rio.session.AWSSession(boto3.Session())\n    )\n)\n\n\nda = stackstac.stack(items[0], gdal_env=gdal_env)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                band: 1,\n                                                                y: 81025,\n                                                                x: 78078)&gt;\ndask.array&lt;fetch_raster_window, shape=(1, 1, 81025, 78078), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -18.27 -18.27 -18.27 ... 51.86 51.86 51.86\n  * y               (y) float64 37.73 37.73 37.73 37.73 ... -35.05 -35.05 -35.05\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:geometry   object {'type': 'Polygon', 'coordinates': [[[-18.27352950...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    crs:         epsg:4326\n    transform:   | 0.00, 0.00,-18.27|\\n| 0.00,-0.00, 37.73|\\n| 0.00, 0.00, 1.00|\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1band: 1y: 81025x: 78078dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n47.13 GiB\n8.00 MiB\n\n\nShape\n(1, 1, 81025, 78078)\n(1, 1, 1024, 1024)\n\n\nDask graph\n6160 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (16)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-18.27 -18.27 ... 51.86 51.86array([-18.274428, -18.27353 , -18.272631, ...,  51.861538,  51.862436,\n        51.863335])y(y)float6437.73 37.73 37.73 ... -35.05 -35.05array([ 37.731937,  37.731039,  37.73014 , ..., -35.051364, -35.052262,\n       -35.053161])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{37.73103856358817, 51.864232928...array({37.73103856358817, 51.86423292864056, -35.054059016911935, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([-18.274427824843425, -18.273529509559307, -18.272631194275185,\n              -18.271732878991067,  -18.27083456370695, -18.269936248422827,\n               -18.26903793313871, -18.268139617854587,  -18.26724130257047,\n               -18.26634298728635,\n              ...\n               51.855249775799365,   51.85614809108348,    51.8570464063676,\n                51.85794472165172,   51.85884303693584,  51.859741352219956,\n               51.860639667504074,   51.86153798278819,  51.862436298072325,\n                51.86333461335644],\n             dtype='float64', name='x', length=78078))yPandasIndexPandasIndex(Float64Index([  37.73193687887226,   37.73103856358814,  37.730140248304025,\n                 37.7292419330199,   37.72834361773578,   37.72744530245166,\n               37.726546987167545,   37.72564867188343,   37.72475035659931,\n                37.72385204131518,\n              ...\n               -35.04507586407077, -35.045974179354886, -35.046872494639004,\n               -35.04777080992312,  -35.04866912520724,  -35.04956744049136,\n               -35.05046575577549,  -35.05136407105961,  -35.05226238634373,\n              -35.053160701627846],\n             dtype='float64', name='y', length=81025))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))crs :epsg:4326transform :| 0.00, 0.00,-18.27|\n| 0.00,-0.00, 37.73|\n| 0.00, 0.00, 1.00|resolution :0.0008983152841195214\n\n\n\n# Create an AOI for our study area\n\n# Guinea\nguinea_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                [-15.519958756713947, 12.732440363049193],\n                [-15.519958756713947, 6.771426493209475],\n                [-7.078554695621165, 6.771426493209475],\n                [-7.078554695621165, 12.732440363049193],\n                [-15.519958756713947, 12.732440363049193],\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\n\n# Subset to bounding box of Guinea\nsubset = da.rio.clip([guinea_aoi[\"geometry\"]])\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                band: 1,\n                                                                y: 6636, x: 9397)&gt;\ndask.array&lt;getitem, shape=(1, 1, 6636, 9397), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n  * band            (band) &lt;U11 'cog_default'\n  * x               (x) float64 -15.52 -15.52 -15.52 ... -7.081 -7.08 -7.079\n  * y               (y) float64 12.73 12.73 12.73 12.73 ... 6.773 6.772 6.772\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1band: 1y: 6636x: 9397dask.array&lt;chunksize=(1, 1, 842, 5), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n475.76 MiB\n8.00 MiB\n\n\nShape\n(1, 1, 6636, 9397)\n(1, 1, 1024, 1024)\n\n\nDask graph\n77 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (17)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-15.52 -15.52 ... -7.08 -7.079axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-15.519295, -15.518397, -15.517498, ...,  -7.080521,  -7.079623,\n        -7.078724])y(y)float6412.73 12.73 12.73 ... 6.772 6.772axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([12.731823, 12.730924, 12.730026, ...,  6.773297,  6.772399,  6.771501])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{-35.054059016911935, 51.8642329...array({-35.054059016911935, 51.86423292864056, 37.73103856358817, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-15.519744006090912 0.0008983152841195214 0.0 12.732271679468038 0.0 -0.0008983152841195215array(0)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Float64Index([-15.519294848448853, -15.518396533164733, -15.517498217880615,\n              -15.516599902596495, -15.515701587312375, -15.514803272028256,\n              -15.513904956744136, -15.513006641460017, -15.512108326175897,\n              -15.511210010891777,\n              ...\n               -7.086809276418906,  -7.085910961134786,  -7.085012645850668,\n               -7.084114330566548,  -7.083216015282428,  -7.082317699998308,\n                -7.08141938471419,   -7.08052106943007,   -7.07962275414595,\n                -7.07872443886183],\n             dtype='float64', name='x', length=9397))yPandasIndexPandasIndex(Float64Index([12.731822521825979,  12.73092420654186, 12.730025891257743,\n               12.72912757597362, 12.728229260689503, 12.727330945405381,\n              12.726432630121263, 12.725534314837144, 12.724635999553023,\n              12.723737684268905,\n              ...\n               6.779585449250032,   6.77868713396591,  6.777788818681792,\n               6.776890503397674,  6.775992188113552,  6.775093872829434,\n               6.774195557545315,  6.773297242261194, 6.7723989269770755,\n               6.771500611692954],\n             dtype='float64', name='y', length=6636))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))resolution :0.0008983152841195214\n\n\n\n# select the band of interest, as there is only one in this dataset we'll select the default\ndata_band = subset.sel(band=\"cog_default\")\ndata_band\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-486a109067eae0fbd2de23580e0f93f3' (time: 1,\n                                                                y: 6636, x: 9397)&gt;\ndask.array&lt;getitem, shape=(1, 6636, 9397), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n  * time            (time) datetime64[ns] NaT\n    id              (time) &lt;U19 'AGB_map_2017v0m_COG'\n    band            &lt;U11 'cog_default'\n  * x               (x) float64 -15.52 -15.52 -15.52 ... -7.081 -7.08 -7.079\n  * y               (y) float64 12.73 12.73 12.73 12.73 ... 6.773 6.772 6.772\n    proj:transform  object {0.0, 1.0, 37.73103856358817, 0.000898315284119521...\n    ...              ...\n    proj:shape      object {81024.0, 78077.0}\n    title           &lt;U17 'Default COG Layer'\n    description     &lt;U47 'Cloud optimized default layer to display on map'\n    raster:bands    object {'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sa...\n    epsg            int64 4326\n    spatial_ref     int64 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-486a109067eae0fbd2de23580e0f93f3'time: 1y: 6636x: 9397dask.array&lt;chunksize=(1, 842, 5), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n475.76 MiB\n8.00 MiB\n\n\nShape\n(1, 6636, 9397)\n(1, 1024, 1024)\n\n\nDask graph\n77 chunks in 8 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\nCoordinates: (17)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-15.52 -15.52 ... -7.08 -7.079axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-15.519295, -15.518397, -15.517498, ...,  -7.080521,  -7.079623,\n        -7.078724])y(y)float6412.73 12.73 12.73 ... 6.772 6.772axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([12.731823, 12.730924, 12.730026, ...,  6.773297,  6.772399,  6.771501])proj:transform()object{0.0, 1.0, 37.73103856358817, 0....array({0.0, 1.0, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:bbox()object{-35.054059016911935, 51.8642329...array({-35.054059016911935, 51.86423292864056, 37.73103856358817, -18.273529509559307},\n      dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:shape()object{81024.0, 78077.0}array({81024.0, 78077.0}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')raster:bands()object{'scale': 1.0, 'nodata': 'inf', ...array({'scale': 1.0, 'nodata': 'inf', 'offset': 0.0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429.0, 'min': 0.0, 'count': 11.0, 'buckets': [405348.0, 44948.0, 18365.0, 6377.0, 3675.0, 3388.0, 3785.0, 9453.0, 13108.0, 1186.0]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429.0, 'minimum': 0.0, 'valid_percent': 50.42436439336373}},\n      dtype=object)epsg()int644326array(4326)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-15.519744006090912 0.0008983152841195214 0.0 12.732271679468038 0.0 -0.0008983152841195215array(0)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))xPandasIndexPandasIndex(Float64Index([-15.519294848448853, -15.518396533164733, -15.517498217880615,\n              -15.516599902596495, -15.515701587312375, -15.514803272028256,\n              -15.513904956744136, -15.513006641460017, -15.512108326175897,\n              -15.511210010891777,\n              ...\n               -7.086809276418906,  -7.085910961134786,  -7.085012645850668,\n               -7.084114330566548,  -7.083216015282428,  -7.082317699998308,\n                -7.08141938471419,   -7.08052106943007,   -7.07962275414595,\n                -7.07872443886183],\n             dtype='float64', name='x', length=9397))yPandasIndexPandasIndex(Float64Index([12.731822521825979,  12.73092420654186, 12.730025891257743,\n               12.72912757597362, 12.728229260689503, 12.727330945405381,\n              12.726432630121263, 12.725534314837144, 12.724635999553023,\n              12.723737684268905,\n              ...\n               6.779585449250032,   6.77868713396591,  6.777788818681792,\n               6.776890503397674,  6.775992188113552,  6.775093872829434,\n               6.774195557545315,  6.773297242261194, 6.7723989269770755,\n               6.771500611692954],\n             dtype='float64', name='y', length=6636))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))resolution :0.0008983152841195214"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-nceo-biomass-2017-layer-for-our-study-area-in-guinea",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-nceo-biomass-2017-layer-for-our-study-area-in-guinea",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Visualizing the NCEO Biomass 2017 layer for our study area in Guinea",
    "text": "Visualizing the NCEO Biomass 2017 layer for our study area in Guinea\nNow that we’ve got the NCEO data layer subsetted for Guinea, let’s visualize it using hvplot.\n\nimport hvplot.xarray\n\nbiomass = data_band.squeeze()\nbiomass\n\nbiomass.hvplot(\n    x=\"x\",\n    y=\"y\",\n    coastline=True,\n    rasterize=True,\n    cmap=\"viridis\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Zonal Statistics",
    "text": "Zonal Statistics\nThis map we created above is great, but let’s focus on which districts (administrative level 2 boundaries) should be prioritized for forest conservation.\nZonal statistics is an operation that calculates statistics on the cell values of a raster layer (e.g., the NCEO AGB dataset) within the zones (i.e., polygons) of another dataset. It is an analytical tool that can calculate the mean, median, sum, minimum, maximum, or range in each zone. The zonal extent, often polygons, can be in the form of objects like administrative boundaries, water catchment areas, or field boundaries.\nIn this example, we’ll explore the data contained in the NCEO AGB collection and analyze it for each of the districts in Guinea. To do this we will need to import district (administrative level 2) boundary layers from below. We will use the Humanitarian Data Exchange (HDX) site to retrieve subnational administrative boundaries for Guinea. Specifically, we will use the geoBoundaries-GIN-ADM2_simplified.geojson which can be accessed here and read them in directly using geopandas.\n\nimport geopandas as gpd\n\nadmin2_gdf = gpd.read_file(\n    \"https://raw.githubusercontent.com/wmgeolab/geoBoundaries/0f0b6f5fb638e7faf115f876da4e77d8f7fa319f/releaseData/gbOpen/GIN/ADM2/geoBoundaries-GIN-ADM2_simplified.geojson\"\n)\n\n\n# check the CRS\nprint(admin2_gdf.crs)\n\nepsg:4326\n\n\n\nimport pandas as pd\nfrom rasterstats import zonal_stats\n\n\nadmin2_biomass = pd.DataFrame(\n    zonal_stats(\n        admin2_gdf,\n        biomass.values,\n        affine=biomass.rio.transform(),\n        nodata=biomass.rio.nodata,\n        band=1\n        # geojson_out=True\n    ),\n    index=admin2_gdf.index,\n)\nadmin2_biomass\n\n/srv/conda/envs/notebook/lib/python3.10/site-packages/rasterstats/io.py:335: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n0.0\n546.0\n46.846892\n527269\n\n\n2\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n0.0\n345.0\n31.315987\n41660\n\n\n4\n0.0\n494.0\n47.809879\n116515\n\n\n5\n0.0\n422.0\n57.583787\n530195\n\n\n6\n0.0\n548.0\n48.555337\n317191\n\n\n7\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n0.0\n448.0\n44.744092\n403399\n\n\n9\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n0.0\n422.0\n46.973538\n426420\n\n\n11\n0.0\n452.0\n50.016374\n161593\n\n\n12\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n0.0\n563.0\n89.595375\n429232\n\n\n14\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n0.0\n558.0\n65.559085\n955743\n\n\n16\n0.0\n486.0\n48.230132\n897380\n\n\n17\n0.0\n590.0\n90.110284\n630818\n\n\n18\n0.0\n413.0\n47.668705\n364092\n\n\n19\n0.0\n339.0\n37.304653\n544541\n\n\n20\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n0.0\n470.0\n46.776737\n216368\n\n\n22\n0.0\n469.0\n57.435206\n278955\n\n\n23\n0.0\n592.0\n71.918267\n461576\n\n\n24\n0.0\n623.0\n123.937151\n814877\n\n\n25\n0.0\n451.0\n45.058698\n859223\n\n\n26\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n0.0\n592.0\n86.547049\n413435\n\n\n29\n0.0\n560.0\n57.591189\n460766\n\n\n30\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n0.0\n554.0\n57.991285\n771431\n\n\n32\n0.0\n389.0\n49.663329\n615108\n\n\n33\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nNow we’ll join the administrative level 2 boundaries to the zonal statistics results, so that we can map the districts on a choropleth map.\n\nconcat_df = admin2_gdf.join(admin2_biomass)\nconcat_df\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n0\n1\nGN-BE\nBeyla\nADM2\nGIN-ADM2-49546643B63767081\nGIN\nADM2\nPOLYGON ((-8.24559 8.44255, -8.24158 8.45044, ...\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n2\nGN-BF\nBoffa\nADM2\nGIN-ADM2-49546643B69790359\nGIN\nADM2\nMULTIPOLYGON (((-13.77147 9.84445, -13.76994 9...\n0.0\n546.0\n46.846892\n527269\n\n\n2\n3\nGN-BK\nBoke\nADM2\nGIN-ADM2-49546643B67680147\nGIN\nADM2\nMULTIPOLYGON (((-14.57512 10.76872, -14.57633 ...\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n4\nGN-C\nConakry\nADM2\nGIN-ADM2-49546643B26553537\nGIN\nADM2\nMULTIPOLYGON (((-13.78686 9.46592, -13.79013 9...\n0.0\n345.0\n31.315987\n41660\n\n\n4\n5\nGN-CO\nCoyah\nADM2\nGIN-ADM2-49546643B29309121\nGIN\nADM2\nPOLYGON ((-13.49399 9.53945, -13.48050 9.55304...\n0.0\n494.0\n47.809879\n116515\n\n\n5\n6\nGN-DB\nDabola\nADM2\nGIN-ADM2-49546643B70320134\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46752 10.545...\n0.0\n422.0\n57.583787\n530195\n\n\n6\n7\nGN-DL\nDalaba\nADM2\nGIN-ADM2-49546643B47404564\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -12.03171 11.288...\n0.0\n548.0\n48.555337\n317191\n\n\n7\n8\nGN-DI\nDinguiraye\nADM2\nGIN-ADM2-49546643B47728803\nGIN\nADM2\nPOLYGON ((-10.72063 11.13326, -10.72092 11.144...\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n9\nGN-DU\nDubreka\nADM2\nGIN-ADM2-49546643B78750611\nGIN\nADM2\nMULTIPOLYGON (((-13.76504 9.82404, -13.75194 9...\n0.0\n448.0\n44.744092\n403399\n\n\n9\n10\nGN-FA\nFaranah\nADM2\nGIN-ADM2-49546643B99428691\nGIN\nADM2\nPOLYGON ((-11.38731 10.39356, -11.38273 10.350...\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n11\nGN-FO\nForecariah\nADM2\nGIN-ADM2-49546643B32851960\nGIN\nADM2\nMULTIPOLYGON (((-13.32015 9.14776, -13.32062 9...\n0.0\n422.0\n46.973538\n426420\n\n\n11\n12\nGN-FR\nFria\nADM2\nGIN-ADM2-49546643B75641357\nGIN\nADM2\nPOLYGON ((-13.76799 10.27884, -13.73119 10.276...\n0.0\n452.0\n50.016374\n161593\n\n\n12\n13\nGN-GA\nGaoual\nADM2\nGIN-ADM2-49546643B44796554\nGIN\nADM2\nPOLYGON ((-13.84293 11.29667, -13.83242 11.291...\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n14\n15\nGN-KA\nKankan\nADM2\nGIN-ADM2-49546643B19447005\nGIN\nADM2\nPOLYGON ((-8.14727 9.58395, -8.15293 9.58911, ...\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n16\nGN-KE\nKerouane\nADM2\nGIN-ADM2-49546643B28981869\nGIN\nADM2\nPOLYGON ((-8.61661 9.50260, -8.60868 9.51354, ...\n0.0\n558.0\n65.559085\n955743\n\n\n16\n17\nGN-KD\nKindia\nADM2\nGIN-ADM2-49546643B38105311\nGIN\nADM2\nPOLYGON ((-13.11475 9.58669, -13.10890 9.58190...\n0.0\n486.0\n48.230132\n897380\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n18\n19\nGN-KB\nKoubia\nADM2\nGIN-ADM2-49546643B329053\nGIN\nADM2\nPOLYGON ((-11.30453 12.01713, -11.31240 12.021...\n0.0\n413.0\n47.668705\n364092\n\n\n19\n20\nGN-KN\nKoundara\nADM2\nGIN-ADM2-49546643B74925550\nGIN\nADM2\nPOLYGON ((-12.82676 12.14425, -12.76880 12.221...\n0.0\n339.0\n37.304653\n544541\n\n\n20\n21\nGN-KO\nKouroussa\nADM2\nGIN-ADM2-49546643B81289084\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46733 10.531...\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n22\nGN-LA\nLabe\nADM2\nGIN-ADM2-49546643B47788034\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -11.98685 11.320...\n0.0\n470.0\n46.776737\n216368\n\n\n22\n23\nGN-LE\nLelouma\nADM2\nGIN-ADM2-49546643B80531036\nGIN\nADM2\nPOLYGON ((-12.99636 11.18952, -12.98648 11.187...\n0.0\n469.0\n57.435206\n278955\n\n\n23\n24\nGN-LO\nLola\nADM2\nGIN-ADM2-49546643B51651521\nGIN\nADM2\nPOLYGON ((-8.46455 8.27185, -8.44429 8.25379, ...\n0.0\n592.0\n71.918267\n461576\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n25\n26\nGN-ML\nMali\nADM2\nGIN-ADM2-49546643B68291102\nGIN\nADM2\nPOLYGON ((-12.76304 11.85482, -12.74823 11.857...\n0.0\n451.0\n45.058698\n859223\n\n\n26\n27\nGN-MM\nMamou\nADM2\nGIN-ADM2-49546643B49157402\nGIN\nADM2\nPOLYGON ((-11.15547 11.05524, -11.13717 11.074...\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n28\nGN-MD\nMandiana\nADM2\nGIN-ADM2-49546643B49348937\nGIN\nADM2\nPOLYGON ((-8.13614 10.00000, -8.13498 10.00774...\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435\n\n\n29\n30\nGN-PI\nPita\nADM2\nGIN-ADM2-49546643B22597757\nGIN\nADM2\nPOLYGON ((-12.20899 11.16225, -12.21822 11.152...\n0.0\n560.0\n57.591189\n460766\n\n\n30\n31\nGN-SI\nSiguiri\nADM2\nGIN-ADM2-49546643B98837050\nGIN\nADM2\nPOLYGON ((-10.00475 11.40696, -10.00285 11.401...\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n32\nGN-TE\nTelimele\nADM2\nGIN-ADM2-49546643B10795278\nGIN\nADM2\nPOLYGON ((-13.65247 10.66825, -13.59967 10.711...\n0.0\n554.0\n57.991285\n771431\n\n\n32\n33\nGN-TO\nTougue\nADM2\nGIN-ADM2-49546643B67909893\nGIN\nADM2\nPOLYGON ((-11.74293 10.98745, -11.70851 11.019...\n0.0\n389.0\n49.663329\n615108\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nBy sorting the results, we can identify those top districts with the highest mean AGB.\n\nconcat_df_sorted = concat_df.sort_values(by=\"mean\", ascending=False)\nconcat_df_sorted.head()\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435"
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Visualizing the results with a choropleth map",
    "text": "Visualizing the results with a choropleth map\nNow, let’s visualize the results!\n\nimport hvplot.pandas\n\n# renaming the shapeName to District for improved legend\nconcat_df.rename(columns={\"shapeName\": \"District\"}, inplace=True)\n\n\nagb = concat_df.hvplot(\n    c=\"mean\",\n    width=900,\n    height=500,\n    geo=True,\n    hover_cols=[\"mean\", \"District\"],\n    cmap=\"viridis\",\n    hover_fill_color=\"white\",\n    line_width=1,\n    title=\"Mean Aboveground Woody Biomass per Guinean District (Mg ha-1)\",\n    tiles=\"CartoLight\",\n)\n\nagb\n\n/srv/conda/envs/notebook/lib/python3.10/site-packages/geoviews/operation/projection.py:79: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  polys = [g for g in geom if g.area &gt; 1e-15]\n\n\n\n\n\n\n  \n\n\n\n\nBy hovering over the map, we can identify the names and mean AGB per district."
  },
  {
    "objectID": "notebooks/datasets/nceo-biomass-statistics.html#summary",
    "href": "notebooks/datasets/nceo-biomass-statistics.html#summary",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully performed zonal statistics on the NCEO AGB dataset in Guinea and displayed the results on a choropleth map. The results of this analysis can dispaly those districts which contain the greatest average amount of AGB and should be prioritized for forest protection efforts."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html",
    "href": "notebooks/datasets/volcano-so2-monitoring.html",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#approach",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#approach",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - SO2\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize tiles for each of the time steps of interest using folium"
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "About the Data",
    "text": "About the Data\nCollecting measurements of Sulfur Dioxide (SO2) plumes from space is a valuable way to monitor changes in emissions. The SO2 index product is used by NASA to monitor volcanic clouds and pre-eruptive volcanic gas emissions activity. Additionally, this information is used in advisories to airlines for operational decisions.\nIn this notebook, we will explore the Sulfur Dioxide dataset and how it was used in this VEDA Discovery article to monitor air pollution across the globe."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Sulfur Dioxide\ncollection_name = \"OMSO2PCA-COG\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'OMSO2PCA-COG',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/OMSO2PCA-COG/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/OMSO2PCA-COG'}],\n 'title': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2005-01-01T00:00:00Z',\n     '2021-01-01T00:00:00Z']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2005-01-01T00:00:00Z', '2021-01-01T00:00:00Z'],\n  'cog_default': {'max': 28.743701934814453, 'min': -4.941379070281982}},\n 'description': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'year'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from 2005 to 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is yearly.\nWe can verify this by checking the total items returned from our STAC API requests.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 17 items\n\n\nThis makes sense as there are 17 years between 2005 - 2021."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API",
    "text": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API\nWe’ll explore three different time steps to show how NASA has observed volcanic activity in the Galápagos islands (2005), detected large scale emissions on the Kamchatka Peninsula (2009), and monitored the eruptions of Fagradalsfjall in Iceland (2021). We’ll then visualize the outputs on a map using folium.\nTo start, we’ll identify which item value corresponds to each year of interest and setting a rescaling_factor for the SO2 index, so that values range from 0 to 1.\n\n# to access the year value from each item more easily\nitems = {item[\"properties\"][\"start_datetime\"][:4]: item for item in items}\n\n\nrescaling_factor = \"0,1\"\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this three times, one for each time step of interest, so that we can visualize each event independently.\n\ntile_2005 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2005']['collection']}&item={items['2005']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2005\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2005&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2009 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2009']['collection']}&item={items['2009']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2009\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2009&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2021 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2021']['collection']}&item={items['2021']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2021\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2021&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWe will then use the tile URL prepared above to create a simple visualization for each time step using folium. In each of these visualizations you can zoom in and out of the map’s focus area to explore the data layer for that year."
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Galápagos islands (2005)",
    "text": "Visualizing Galápagos islands (2005)\n\n# Set initial zoom and map for Galápagos islands\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -0.915435,\n        -89.57216,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2005[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Kamchatka Peninsula (2009)",
    "text": "Visualizing Kamchatka Peninsula (2009)\n\n# Set initial zoom and map for Kamchatka Peninsula\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        53.018234,\n        158.67016,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2009[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Fagradalsfjall, Iceland (2021)",
    "text": "Visualizing Fagradalsfjall, Iceland (2021)\n\n# Set initial zoom and map for Fagradalsfjall, Iceland\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        65.0294256,\n        -18.393870,\n    ],\n    zoom_start=6,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2021[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/datasets/volcano-so2-monitoring.html#summary",
    "href": "notebooks/datasets/volcano-so2-monitoring.html#summary",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors sulfur dioxide emissions from space, by showcasing three different examples across the globe: volcanic activity in the Galápagos islands (2005), large scale emissions on the Kamchatka Peninsula (2009), and eruptions of Fagradalsfjall in Iceland (2021)."
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html",
    "href": "notebooks/datasets/air-quality-covid.html",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "href": "notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#approach",
    "href": "notebooks/datasets/air-quality-covid.html#approach",
    "title": "Air Quality and COVID-19",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - NO₂\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap"
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#about-the-data",
    "href": "notebooks/datasets/air-quality-covid.html#about-the-data",
    "title": "Air Quality and COVID-19",
    "section": "About the Data",
    "text": "About the Data\nThis dataset is of monthly nitrogen dioxide (NO₂) levels values across the globe. Darker colors indicate higher NO₂ levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow."
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "href": "notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "title": "Air Quality and COVID-19",
    "section": "The Case Study - Air Quality and COVID-19",
    "text": "The Case Study - Air Quality and COVID-19\nIn this notebook, we’ll walk through the development of side-by-side comparisons of NO₂ levels before and after government lockdowns as demonstrated Seeing Rebounds in NO₂ in this VEDA Discovery story: Air Quality and COVID-19 available on the VEDA Dashboard."
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "href": "notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "title": "Air Quality and COVID-19",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Nitrogen Oxide\ncollection_name = \"no2-monthly\"\n\n\n#Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2022-12-01 00:00:00+00']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2022-12-01T00:00:00Z'],\n  'cog_default': {'max': 50064805976866816, 'min': -10183824872833024}},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from January 2015 to December 2022. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Check total number of items available\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 84 items\n\n\nThis makes sense as there are 7 years between 2016 - 2022, with 12 months per year, meaning 84 records in total.\nBelow, we’ll use the cog_default values to provide our upper and lower bounds in rescale_values.\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\nrescale_values\n\n{'max': 50064805976866816, 'min': -10183824872833024}"
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "href": "notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "title": "Air Quality and COVID-19",
    "section": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API",
    "text": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API\nWe will explore changes in air quality due to changes in human behaviour resulting from the COVID-19 pandemic. With people largely confined to their homes to reduce the spread of the novel coronavirus, scientists were anticipated there were likely to be fewer cars, planes, and ships emitting fossil fuel pollution. In this notebook, we’ll explore the impacts these government lockdowns had on specific air pollutants (i.e., NO₂) and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice times, once for February 2020 and again for February 2022, so that we can visualize each event independently.\n\nfebruary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-02']['collection']}&item={items['2020-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2020_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202002_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-10183824872833024%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\nfebruary_2022_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2022-02']['collection']}&item={items['2022-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2022_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202202_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-10183824872833024%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}"
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "href": "notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "title": "Air Quality and COVID-19",
    "section": "Seeing Rebounds in NO₂",
    "text": "Seeing Rebounds in NO₂\nAir pollutants with short lifespans, like NO₂, decreased dramatically with COVID-related shutdowns in the spring of 2020 (see lefthand side map). As the world began to re-open and mobility restrictions eased, travel increased and alongside it NO₂ pollutants. Air quality levels are now returning to pre-pandemic levels (see righthand side map).\nScroll and zoom within the maps below, the side-by-side comparison will follow wherever you explore. Darker purples indicate higher NO₂ levels and more activity. Lighter blues indicate lower levels of NO₂ and less activity.\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and map for NO2 Layer\nm = folium.plugins.DualMap(location=(33.6901, 118.9325), zoom_start=5)\n\n# February 2020\nmap_layer_2020 = TileLayer(\n    tiles=february_2020_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2020.add_to(m.m1)\n\n# February 2022\nmap_layer_2022 = TileLayer(\n    tiles=february_2022_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2022.add_to(m.m2)\n\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/datasets/air-quality-covid.html#summary",
    "href": "notebooks/datasets/air-quality-covid.html#summary",
    "title": "Air Quality and COVID-19",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors NO₂ emissions from space. By showcasing lockdown (February 2020) and post-lockdown (February 2022) snapshots of air quality side-by-side, we demonstrate how quickly atmospheric NO₂ responds to reductions in emissions and human behavior."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection\nPass STAC item into raster API /stac/tilejson.json endpoint\nGet time series statistics over available time period to identify seasonal trends\nVisualize peak by displaying the tile in folium\nVisualize time series of raster images"
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "About the Data",
    "text": "About the Data\nOcean Net Primary Production (NPP) is the result of CO2 fixation, through photosynthesis, by marine phytoplankton which contain chlorophyll. It is the proportion of phytoplankton-sequestered carbon that enters the oceanic food web and supports a variety of marine life."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "The Case Study - Walvis Bay, Namibia",
    "text": "The Case Study - Walvis Bay, Namibia\nWalvis Bay is home to Namibia’s largest marine farming center and a well established commercial fishing industry. It’s location in the nutrient-rich Benguela upwelling system of the Atlantic Ocean, means producers can rely on this area to cultivate an abundance of shellfish including oysters, mussels, and scallops.\nOccasionally the nutrient-rich waters of the Atlantic produce higher than normal NPP levels, resulting in short-lived harmful algal blooms. This is often a result of both favorable temperatures and abundance of sufficient nutrients. The resulting algal blooms can have severe consequences causing massive fish kills, contaminating seafood with toxins and creating an unsafe environment for humans and marine life. Toxins accumulated in the shellfish organs can be subsequently transmitted to humans through consumption and resulting in serious health threats.\nIn this example we explore the Ocean NPP dataset over the year 2020 to identify spatial and temporal patterns of Ocean NPP in the Walvis Bay area."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provife STAC and RASTER API endpoints\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\n# Declare collection of interest - Ocean NPP\ncollection_name = \"MO_NPP_npp_vgpm\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'MO_NPP_npp_vgpm',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'}],\n 'title': '',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2020-01-01T00:00:00Z',\n     '2020-12-12T23:59:59Z']]}},\n 'license': 'MIT',\n 'summaries': {'datetime': ['2020-01-01T00:00:00Z', '2020-12-01T00:00:00Z'],\n  'cog_default': {'max': 34561.35546875, 'min': 14.516647338867188}},\n 'description': 'Ocean Net Primary Production (NPP): https://oceancolor.gsfc.nasa.gov/atbd/npp/',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\n# Verify frequency of data available\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\n# Get collection summary\ncollection[\"summaries\"]\n\n{'datetime': ['2020-01-01T00:00:00Z', '2020-12-01T00:00:00Z'],\n 'cog_default': {'max': 34561.35546875, 'min': 14.516647338867188}}\n\n\nGreat, we can explore the year 2020 time series. Let’s create a bounding box to explore the Walvis Bay area of interest (AOI) in Namibia\n\n# Walvis Bay, Namibia\nwalvis_bay_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                [13.686159004559698, -21.700046934333145],\n                [13.686159004559698, -23.241974326585833],\n                [14.753560168039911, -23.241974326585833],\n                [14.753560168039911, -21.700046934333145],\n                [13.686159004559698, -21.700046934333145],\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\nLet’s visualize the AOI we have just created using folium\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nfolium.GeoJson(walvis_bay_aoi, name=\"Walvis Bay\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nReturning back to our STAC API requests, let’s check how many total items are available.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 12 items\n\n\nThis makes sense is our collection is monthly, so we should have 12 total items.\n\n# Explore one item to see what it contains\nitems[0]\n\n{'id': 'A_202012.L3m_MO_NPP_npp_vgpm_4km',\n 'bbox': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging-stac.delta-backend.com/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging-stac.delta-backend.com/collections/MO_NPP_npp_vgpm/items/A_202012.L3m_MO_NPP_npp_vgpm_4km'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/MO_NPP_npp_vgpm/A_202012.L3m_MO_NPP_npp_vgpm_4km.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -32767.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 18305.302734375,\n      'min': 20.69771957397461,\n      'count': 11.0,\n      'buckets': [223827.0,\n       2620.0,\n       363.0,\n       105.0,\n       45.0,\n       20.0,\n       13.0,\n       4.0,\n       1.0,\n       4.0]},\n     'statistics': {'mean': 448.69620531977694,\n      'stddev': 450.1186820854004,\n      'maximum': 18305.302734375,\n      'minimum': 20.69771957397461,\n      'valid_percent': 43.29719543457031}}]}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180.0000050868518, -90.00000508655744],\n    [180.0000050868518, -90.00000508655744],\n    [180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, -90.00000508655744]]]},\n 'collection': 'MO_NPP_npp_vgpm',\n 'properties': {'proj:bbox': [-180.0000050868518,\n   -90.00000508655744,\n   180.0000050868518,\n   89.9999974571629],\n  'proj:epsg': 4326.0,\n  'proj:shape': [4320.0, 8640.0],\n  'end_datetime': '2020-12-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0000050868518, -90.00000508655744],\n     [180.0000050868518, -90.00000508655744],\n     [180.0000050868518, 89.9999974571629],\n     [-180.0000050868518, 89.9999974571629],\n     [-180.0000050868518, -90.00000508655744]]]},\n  'proj:transform': [0.041666667844178655,\n   0.0,\n   -180.0000050868518,\n   0.0,\n   -0.04166666725549082,\n   89.9999974571629,\n   0.0,\n   0.0,\n   1.0],\n  'start_datetime': '2020-12-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}\n\n\nNow that we have explored the collection metadata by querying the STAC API, we can use the RASTER API to access the data itself.\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][\"cog_default\"][\"href\"]},\n        json=geojson,\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\n%%time\nstats = [generate_stats(item, walvis_bay_aoi) for item in items]\n\nCPU times: user 813 ms, sys: 42.7 ms, total: 856 ms\nWall time: 13.2 s\n\n\nWith the function provided above, we can generate statistics for our AOI. In the example below, we’ll explore sample statistics available from one of the tiles.\n\nstats[0]\n\n{'statistics': {'1': {'min': 2288.525146484375,\n   'max': 19800.787109375,\n   'mean': 4803.135537190083,\n   'count': 605.0,\n   'sum': 2905897.0,\n   'std': 2175.5772113693283,\n   'median': 4171.0107421875,\n   'majority': 4321.17041015625,\n   'minority': 2288.525146484375,\n   'unique': 595.0,\n   'histogram': [[282.0, 207.0, 73.0, 22.0, 11.0, 1.0, 3.0, 1.0, 0.0, 5.0],\n    [2288.525146484375,\n     4039.75146484375,\n     5790.9775390625,\n     7542.20361328125,\n     9293.4296875,\n     11044.65625,\n     12795.8828125,\n     14547.1083984375,\n     16298.3349609375,\n     18049.560546875,\n     19800.787109375]],\n   'valid_percent': 62.89,\n   'masked_pixels': 357.0,\n   'valid_pixels': 605.0,\n   'percentile_98': 10588.860703124998,\n   'percentile_2': 2742.223876953125}},\n 'start_datetime': '2020-12-01T00:00:00'}\n\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)"
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the full Ocean NPP time series available (January-December 2020) for the Walvis Bay area of Namibia. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly Ocean NPP values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NPP values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NPP values\",\n)\n\nplt.legend()\nplt.title(\"Ocean NPP Values for Walvis Bay, Namibia (2020)\")\n\nText(0.5, 1.0, 'Ocean NPP Values for Walvis Bay, Namibia (2020)')\n\n\n\n\n\nHere, we observe the seasonal variability in Ocean NPP for the Walvis Bay area. The larger peaks in the max values suggests the intensity of these events may vary spatially. Let’s explore one of the time steps (e.g., October) where there are higher maximum monthly NPP values to see if this is the case.\nImportant note: Keep in mind that the size and extent of your AOI will influence the ‘signal’ of your time series. If the phenomena you are investigating displays greater spatial variability a larger AOI will provide more ‘noise’ making it more difficult to detect."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Raster Imagery",
    "text": "Visualizing the Raster Imagery\nLet’s first explore a single tile during one of the relative peaks in October, where we observe an increased sustained peak in NPP values.\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2020-10-01T00:00:00\n\n\n\nrescale_values = collection[\"summaries\"][\"cog_default\"]\nrescale_values\n\n{'max': 34561.35546875, 'min': 14.516647338867188}\n\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'center': [0.0, -3.814697265625e-06, 0]}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFrom the image above, we see higher Ocean NPP values (displayed in teal) located in and around Walvis Bay and the surrounding shorelines - highlighting areas of concern for the local shellfish industry."
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the raster time series",
    "text": "Visualizing the raster time series\nNow we will look at each of the raster tiles that make up this time series to explore the spatial and temporal patterns of Ocean NPP observed in Walvis Bay throughout 2020.\nWe used the code below to examine the tiles and the order in which they are presented.\n\nimport matplotlib.pyplot as plt\n\nfor item in items:\n    tiles = requests.get(\n        f\"{RASTER_API_URL}/stac/tilejson.json?collection={item['collection']}&item={item['id']}\"\n        \"&assets=cog_default\"\n        \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n        f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n    ).json()\n    print(tiles[\"tiles\"])\n\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202011.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202009.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202008.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202007.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202006.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202005.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202004.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202003.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202002.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://staging-raster.delta-backend.com/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202001.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n\n\nSince we found the tiles to be presented in reverse time order, we’ll revise this in the code below. We’ll use reversed() to do so.\n\nimport tempfile\nfrom datetime import datetime\nfrom IPython.display import display, Image\n\nCOG_DEFAULT = [\n    x\n    for x in requests.get(f\"{STAC_API_URL}/collections\").json()[\"collections\"]\n    if x[\"id\"] == \"MO_NPP_npp_vgpm\"\n][0][\"summaries\"][\"cog_default\"]\n\n\nfor item in reversed(items):\n    image_bytes = requests.post(\n        f\"{RASTER_API_URL}/cog/crop\",\n        params={\n            \"format\": \"png\",\n            \"height\": 512,\n            \"width\": 512,\n            \"url\": item[\"assets\"][\"cog_default\"][\"href\"],\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n        json=walvis_bay_aoi,\n    ).content\n\n    # formating the datetime to make for easier reading\n    datetime_str = item[\"properties\"][\"start_datetime\"]\n    datetime_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n    print(datetime_object.strftime(\"%B %Y\"))\n\n    display(Image(image_bytes, height=512, width=512))\n\nJanuary 2020\nFebruary 2020\nMarch 2020\nApril 2020\nMay 2020\nJune 2020\nJuly 2020\nAugust 2020\nSeptember 2020\nOctober 2020\nNovember 2020\nDecember 2020"
  },
  {
    "objectID": "notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "href": "notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized the spatial and temporal variability of Ocean NPP values in the Benguela Current, which displays a seasonal pattern of peaking in the October, November, December, and January months when favorable temperatures and nutrient conditions are present."
  },
  {
    "objectID": "services/index.html",
    "href": "services/index.html",
    "title": "Services",
    "section": "",
    "text": "The VEDA ecosystem of services provides a platform data products hosted in the VEDA data store and exposed via the VEDA Spatio-Temporal Asset Catalog (STAC).\nThe docs (this site) provide information about best practice in cloud-based Earth data science and platform-specific details about how to use and contribute information on VEDA."
  },
  {
    "objectID": "services/dashboard.html",
    "href": "services/dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "The VEDA Dashboard is a publishing platform for results of scientific analysis of Earth data at NASA, in the form of Dataset information pages (“Datasets”) and stories (“Discoveries”)."
  },
  {
    "objectID": "services/dashboard.html#contributing",
    "href": "services/dashboard.html#contributing",
    "title": "Dashboard",
    "section": "Contributing",
    "text": "Contributing\nPlease see our docs on Dashboard Configuration."
  },
  {
    "objectID": "services/apis.html",
    "href": "services/apis.html",
    "title": "APIs",
    "section": "",
    "text": "Most of the VEDA APIs are hosted out of a single project (veda-backend) that combines multiple standalone services."
  },
  {
    "objectID": "services/apis.html#environments",
    "href": "services/apis.html#environments",
    "title": "APIs",
    "section": "Environments",
    "text": "Environments\nWhile some of our services are already very mature, VEDA is currently in the build-up phase. Therefore, we do not yet have a production environment for users. Maintenance on the staging environment will be announced internally and selected known stakeholders will be informed of any larger changes.\n\nProduction (stable): Coming soon\n\n\nStaging (maintenance will be announced):\n\nSTAC browser: veda-staging-stac-browser\nSTAC API (metadata): https://staging-stac.delta-backend.com/docs\nList collections: https://staging-stac.delta-backend.com/collections\nRaster API (tiling): https://staging-raster.delta-backend.com/docs\nSTAC viewer (experimental): https://staging-stac.delta-backend.com/index.html\n\n\n\nDevelopment, aka Dev (experimental work, expected downtime)\n\nSTAC browser: veda-dev-stac-browser\nSTAC API (metadata): https://dev-stac.delta-backend.com/docs\nList collections: https://dev-stac.delta-backend.com/collections\nRaster API (tiling): https://dev-raster.delta-backend.com/docs\nSTAC viewer (experimental): https://dev-stac.delta-backend.com/index.html"
  },
  {
    "objectID": "services/jupyterhub.html",
    "href": "services/jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "VEDA promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with VEDA can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: nasa-veda.2i2c.cloud. Please find instructions for requesting access below.\nIf you are a scientist affiliated with NASA projects such as EIS and MAAP, you can also keep using the resources provided by these projects.\nThrough the use of open-source technology, we make sure our services are interoperable and exchangeable."
  },
  {
    "objectID": "services/jupyterhub.html#getting-access-to-vedas-jupyterhub-environment",
    "href": "services/jupyterhub.html#getting-access-to-vedas-jupyterhub-environment",
    "title": "JupyterHub",
    "section": "Getting access to VEDA’s JupyterHub environment",
    "text": "Getting access to VEDA’s JupyterHub environment\nAccess to the VEDA notebook environment is currently on an as-need basis. If you are a user afficiliated with VEDA, you can gain access by following these steps:\n\nMake sure you have a Github Account. Take note of your Github username\nSend an email to the VEDA team (veda@uah.edu) asking for access to the VEDA notebook environment. Please include your Github username. They will invite you through Github to join the VEDA Analytics Github Team. Please watch your email for the invite.\nOnce you accepted the invitation, you should be able to go to https://nasa-veda.2i2c.cloud/ and login via your Github credentials."
  },
  {
    "objectID": "services/jupyterhub.html#instructory-notebooks",
    "href": "services/jupyterhub.html#instructory-notebooks",
    "title": "JupyterHub",
    "section": "Instructory notebooks",
    "text": "Instructory notebooks\nThis documentation site provides Jupyter notebooks on how to load and analyze Earth data an interactive cloud computing environment."
  },
  {
    "objectID": "services/data-store.html",
    "href": "services/data-store.html",
    "title": "VEDA Data Store",
    "section": "",
    "text": "The VEDA Data Store consists of cloud object storage (AWS S3 in us-west-2) and a central Spatio-Temporal Asset Catalog (STAC) that exposes the datasets."
  },
  {
    "objectID": "services/data-store.html#dataset-selection",
    "href": "services/data-store.html#dataset-selection",
    "title": "VEDA Data Store",
    "section": "Dataset selection",
    "text": "Dataset selection\nThe VEDA Data Store is meant for\n\nNovel datasets produced by NASA Earth data scientists to be presented on the VEDA Dashboard or shared with other science users on VEDA, but not (yet) suited for publication in one of the EOSDIS Distributed Active Archive Centers (DAAC)\nNon-authoritative cloud-optimized versions of Datasets from a DAAC\nOther datasets that do not have an authoritative, cloud-optimized source, to be published or used within the VEDA platform"
  },
  {
    "objectID": "services/data-store.html#browsing-the-data-store",
    "href": "services/data-store.html#browsing-the-data-store",
    "title": "VEDA Data Store",
    "section": "Browsing the Data Store",
    "text": "Browsing the Data Store\nThe main public interface is the STAC browser and many APIs are exposed that provide access to the data using various protocols.\n\nSTAC browser: veda-staging-stac-browser"
  },
  {
    "objectID": "services/data-store.html#contributing-data",
    "href": "services/data-store.html#contributing-data",
    "title": "VEDA Data Store",
    "section": "Contributing data",
    "text": "Contributing data\nThe process of data ingestion into the VEDA Data Store is under active development.\nPlease see our docs on dataset ingestion."
  },
  {
    "objectID": "external-resources.html",
    "href": "external-resources.html",
    "title": "External resources",
    "section": "",
    "text": "This list is intended for scientists who want to get started with cloud-based, collaborative, open geospatial data analysis, or those looking to refresh their knowledge. It is by no means complete, but contains pointers to more elaborate resources. We anticipate this list to evolve as our platform and use cases evolve. Suggestions for additional resources or topics are highly welcome."
  },
  {
    "objectID": "external-resources.html#master-the-basic-workflow-tools",
    "href": "external-resources.html#master-the-basic-workflow-tools",
    "title": "External resources",
    "section": "Master the basic workflow tools",
    "text": "Master the basic workflow tools\n\nGit - for managing code versions\nConda - for managing dependencies\nJupyter - for running code"
  },
  {
    "objectID": "external-resources.html#take-a-course-or-read-a-book-on-geospatial-data-analysis",
    "href": "external-resources.html#take-a-course-or-read-a-book-on-geospatial-data-analysis",
    "title": "External resources",
    "section": "Take a course or read a book on geospatial data analysis",
    "text": "Take a course or read a book on geospatial data analysis\n\nData Carpentry geospatial course\nUW course on Geospatial Data Analysis with Python\nGeographic data book - emphasis on vector data"
  },
  {
    "objectID": "external-resources.html#look-through-nicely-curated-lists-of-tools-and-resources",
    "href": "external-resources.html#look-through-nicely-curated-lists-of-tools-and-resources",
    "title": "External resources",
    "section": "Look through nicely curated lists of tools and resources",
    "text": "Look through nicely curated lists of tools and resources\n\nNASA Openscapes Earthdata Cloud Cookbook\nCryoCloud JupyterBook\nProject Pythia - Pangeo’s education hub\nPlanetary Computer\nGeospatial Computing Platform library of training resources (by Python package)"
  },
  {
    "objectID": "external-resources.html#find-out-what-a-hack-week-is-and-see-how-they-teach-best-practice",
    "href": "external-resources.html#find-out-what-a-hack-week-is-and-see-how-they-teach-best-practice",
    "title": "External resources",
    "section": "Find out what a Hack Week is and see how they teach best practice",
    "text": "Find out what a Hack Week is and see how they teach best practice\n\nUW GeoHackWeek\nUW OceanHackWeek"
  },
  {
    "objectID": "external-resources.html#stay-up-to-date-through-blogs",
    "href": "external-resources.html#stay-up-to-date-through-blogs",
    "title": "External resources",
    "section": "Stay up-to-date through blogs",
    "text": "Stay up-to-date through blogs\n\nBlog series on latest trends and resources in geospatial - entry-level"
  }
]