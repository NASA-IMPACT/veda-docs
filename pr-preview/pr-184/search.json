[
  {
    "objectID": "AGU2024.html",
    "href": "AGU2024.html",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "AGU Searchable Schedule\nThis repo is for high-level planning for AGU December 9-13, 2024, in Washington DC\nBelow is information workshops, talks, posters, and demos by the NASA Openscapes Mentors and collaborators. Expanding a talk or poster item below reveals an AGU link for full author list, abstract, time, and location.\nAbstracts doc link\nPoster Requirements\n\n\nWhen are people there? Please add yourself!\n\n\nName: Days\n\n\n\n\nTalks\n\n\nTalk: Advancing Open Science: Lessons Learned from NASA Projects. 08:40 - 08:50 EST. Point of Contact: Aimee Barciauskas\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1607534\nSession: U11A: Accelerating Scientific Discovery and Interdisciplinary Collaboration Through Cloud Computing Hubs and Tools I Oral\nMonday, 9 December 2024, 08:40 - 08:50 EST Ballroom A (Convention Center)\n\n\n\nTalk: From Global Models to Local Impacts: Interactive “Explor-anatory” Visualization of CMIP Data for Museumgoers (Invited). 16:00 - 16:10 EST. Point of Contact: Alex Gurvich\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1616551\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:00 - 16:10 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: Enhancing Geospatial Data Interoperability with pyarc2stac (Invited). 16:40 - 16:50 EST. Points of Contact: Slesa Adhikari, Abdelhak Marouane, Brian Freitag\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1625370\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:40 - 16:50 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: On-the-fly raster visualizations leveraging STAC metadata standards. 16:50 - 17:00 EST. Points of Contact: Saadiq Mohiuddin, Hanbyul Jo, Aimee Barciauskas, Brian Freitag\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1733434\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:50 - 17:00 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: Dynamic Tiling for Earth Data Visualization. 17:00 - 17:10 EST. Points of Contact: Aimee Barciauskas, Manil Maskey, Vncent Sarago, Henry Rodman, Max Jones, Sean Harkins\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1624796\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 17:00 - 17:10 EST Marquis 12-13 (Marriott Marquis)\n\nSessions\n\n\nIN14B - Emerging Technologies for Earth Science Data Access and Visualization II (Oral). 16:00 - 17:30 EST. Points of Contact: Ryan A Boller, James G Acker, Lorraine Tighe, Aimee Barciauskas, Elizabeth Joynerv\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Session/240646\nMonday, 9 December 2024, 16:00 - 17:30 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nIN11B - Emerging Technologies for Earth Science Data Access and Visualization II (Poster). 08:30 - 12:20 EST. Points of Contact: Ryan A Boller, James G Acker, Lorraine Tighe, Aimee Barciauskas, Elizabeth Joynerv\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Session/240535\nMonday, 9 December 2024, 16:00 - 17:30 EST Hall B-C (Poster Hall) (Convention Center)\n\n\n\n\nPosters\n\n\nTalk: Beyond Boundaries: Remote Sensing in Highlighting and Bridging Gaps for Environmental Justice. 13:40 - 17:30 EST. Points of Contact: Maheshwari Neelam, Yaítza Luna-Cruz, Antarpreet Singh Jutla, Brian Freitag\n\nSession: GH23A: Poster Session\nTuesday, 10 December 2024, 17:00 - 17:10 EST Hall B-C (Poster Hall) (Convention Center)\n\n\n\n\nPosters\n\n\nTalk: Human-Induced Drought: Escalating Mental Health Crises and Environmental Inequities. 08:30 - 12:20 EST. Points of Contact: Maheshwari Neelam, Brian Freitag\n\nSession: GH31B-2402: Poster Session\nWednesday, 11 December 2024, 08:30 - 12:20 EST Hall B-C (Poster Hall) (Convention Center)\n\n\n\n\nPosters\n\n\nTalk: STAC And Virtualizarr Pipelines For Optimized NODD Data Accessibility. 08:30 - 12:20 EST. Points of Contact: Sean Harkins, Aimee Barciauskas, Henry Rodman\n\nSession: SY41G-2637: Poster Session\nThursday, 12 December 2024, 08:30 - 12:20 EST Hall B-C (Poster Hall) (Convention Center)\n\n\n\n\n\n\n\nHackday with Pangeo: We’ll join Pangeo hackday to meet and work with other awesome open science/cloud/geospatial folks!\nhttps://discourse.pangeo.io/t/post-agu-pangeo-working-meeting-december-14-2024-in-washington-dc/4440"
  },
  {
    "objectID": "AGU2024.html#monday-dec-09",
    "href": "AGU2024.html#monday-dec-09",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "Talks\n\n\nTalk: Advancing Open Science: Lessons Learned from NASA Projects. 08:40 - 08:50 EST. Point of Contact: Aimee Barciauskas\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1607534\nSession: U11A: Accelerating Scientific Discovery and Interdisciplinary Collaboration Through Cloud Computing Hubs and Tools I Oral\nMonday, 9 December 2024, 08:40 - 08:50 EST Ballroom A (Convention Center)\n\n\n\nTalk: From Global Models to Local Impacts: Interactive “Explor-anatory” Visualization of CMIP Data for Museumgoers (Invited). 16:00 - 16:10 EST. Point of Contact: Alex Gurvich\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1616551\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:00 - 16:10 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: Enhancing Geospatial Data Interoperability with pyarc2stac (Invited). 16:40 - 16:50 EST. Points of Contact: Slesa Adhikari, Abdelhak Marouane, Brian Freitag\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1625370\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:40 - 16:50 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: On-the-fly raster visualizations leveraging STAC metadata standards. 16:50 - 17:00 EST. Points of Contact: Saadiq Mohiuddin, Hanbyul Jo, Aimee Barciauskas, Brian Freitag\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1733434\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 16:50 - 17:00 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nTalk: Dynamic Tiling for Earth Data Visualization. 17:00 - 17:10 EST. Points of Contact: Aimee Barciauskas, Manil Maskey, Vncent Sarago, Henry Rodman, Max Jones, Sean Harkins\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1624796\nSession: IN14B: Emerging Technologies for Earth Science Data Access and Visualization II Oral\nMonday, 9 December 2024, 17:00 - 17:10 EST Marquis 12-13 (Marriott Marquis)\n\nSessions\n\n\nIN14B - Emerging Technologies for Earth Science Data Access and Visualization II (Oral). 16:00 - 17:30 EST. Points of Contact: Ryan A Boller, James G Acker, Lorraine Tighe, Aimee Barciauskas, Elizabeth Joynerv\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Session/240646\nMonday, 9 December 2024, 16:00 - 17:30 EST Marquis 12-13 (Marriott Marquis)\n\n\n\nIN11B - Emerging Technologies for Earth Science Data Access and Visualization II (Poster). 08:30 - 12:20 EST. Points of Contact: Ryan A Boller, James G Acker, Lorraine Tighe, Aimee Barciauskas, Elizabeth Joynerv\n\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Session/240535\nMonday, 9 December 2024, 16:00 - 17:30 EST Hall B-C (Poster Hall) (Convention Center)"
  },
  {
    "objectID": "AGU2024.html#tuesday-dec-10",
    "href": "AGU2024.html#tuesday-dec-10",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "Posters\n\n\nTalk: Beyond Boundaries: Remote Sensing in Highlighting and Bridging Gaps for Environmental Justice. 13:40 - 17:30 EST. Points of Contact: Maheshwari Neelam, Yaítza Luna-Cruz, Antarpreet Singh Jutla, Brian Freitag\n\nSession: GH23A: Poster Session\nTuesday, 10 December 2024, 17:00 - 17:10 EST Hall B-C (Poster Hall) (Convention Center)"
  },
  {
    "objectID": "AGU2024.html#wednesday-dec-11",
    "href": "AGU2024.html#wednesday-dec-11",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "Posters\n\n\nTalk: Human-Induced Drought: Escalating Mental Health Crises and Environmental Inequities. 08:30 - 12:20 EST. Points of Contact: Maheshwari Neelam, Brian Freitag\n\nSession: GH31B-2402: Poster Session\nWednesday, 11 December 2024, 08:30 - 12:20 EST Hall B-C (Poster Hall) (Convention Center)"
  },
  {
    "objectID": "AGU2024.html#thursday-dec-12",
    "href": "AGU2024.html#thursday-dec-12",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "Posters\n\n\nTalk: STAC And Virtualizarr Pipelines For Optimized NODD Data Accessibility. 08:30 - 12:20 EST. Points of Contact: Sean Harkins, Aimee Barciauskas, Henry Rodman\n\nSession: SY41G-2637: Poster Session\nThursday, 12 December 2024, 08:30 - 12:20 EST Hall B-C (Poster Hall) (Convention Center)"
  },
  {
    "objectID": "AGU2024.html#saturday-dec-14",
    "href": "AGU2024.html#saturday-dec-14",
    "title": "Activities by NASA VEDA and collaborators",
    "section": "",
    "text": "Hackday with Pangeo: We’ll join Pangeo hackday to meet and work with other awesome open science/cloud/geospatial folks!\nhttps://discourse.pangeo.io/t/post-agu-pangeo-working-meeting-december-14-2024-in-washington-dc/4440"
  },
  {
    "objectID": "instance-management/adding-instance.html",
    "href": "instance-management/adding-instance.html",
    "title": "Creating your Own Instance of VEDA",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "instance-management/notebooks/index.html",
    "href": "instance-management/notebooks/index.html",
    "title": "Usage Examples and Tutorials",
    "section": "",
    "text": "Base URL Changes\n\n\n\nThe VEDA Team is in the process of updating documentation notebooks to use the new stable production APIs. Currently most notebooks use deprecated staging APIs and should be updated:\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\" –&gt; https://openveda.cloud/api/stac\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\" –&gt; https://openveda.cloud/api/raster"
  },
  {
    "objectID": "instance-management/notebooks/index.html#getting-started",
    "href": "instance-management/notebooks/index.html#getting-started",
    "title": "Usage Examples and Tutorials",
    "section": "Getting started",
    "text": "Getting started\nThe example notebooks are divided into three sections:\n\nQuickstarts: Notebooks to get you started quickly and help you become more familiar with cloud-native geospatial technologies.\nTutorials: Longer notebooks that walk through more advanced use cases and examples.\nDatasets: Notebooks that showcase a particular VEDA dataset and walk through an applied geospatial analyses.\n\n\nChoosing the right data access route for your needs\nThe Quickstarts examples are further divided into two sections, which you can choose from depending on your data needs:\n\nAccessing the Data Directly: For when you want to access the raw data (e.g., to do a specific analysis). In this case, permissions are required to access the data (i.e., must be run on VEDA JupyterHub) and computation happens within the user’s instance (i.e., the user needs to think about instance size). This approach is suitable for use within notebooks. All examples provided in this section require VEDA JupyterHub access to run.\nUsing the Raster API: For when you want to show outputs to other people or do standard processing. No permissions required (i.e., notebooks can be run on mybinder). Additionally, the computation happens somewhere else (i.e., user does not have to think about instance size). Lastly, this approach is suitable for use within notebooks as well as web application frontends (e.g., like dataset discoveries). These notebook examples can be run on both VEDA JupyterHub, as well as outside of the Hub (see instructions below) and within mybinder."
  },
  {
    "objectID": "instance-management/notebooks/index.html#how-to-run",
    "href": "instance-management/notebooks/index.html#how-to-run",
    "title": "Usage Examples and Tutorials",
    "section": "How to run",
    "text": "How to run\nEvery notebook contains information about how to run it. Some can run on mybinder and all can run on the VEDA JupyterHub. See VEDA Analytics JupyterHub Access for information about how to gain access.\n\nRunning outside of VEDA JupyterHub\nTo run the notebooks locally, you can use can install the Python packages (a virtual environment is recommended)\npip install -r requirements.txt\nOnce you have installed the packages you can run the notebooks using Jupyter.\njupyter lab\nIf the notebook needs access to protected data on S3, you will need to specifically get access. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN)."
  },
  {
    "objectID": "instance-management/notebooks/index.html#how-to-contribute",
    "href": "instance-management/notebooks/index.html#how-to-contribute",
    "title": "Usage Examples and Tutorials",
    "section": "How to contribute",
    "text": "How to contribute\nPlease refer to the notebook style guide in these docs."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#run-this-notebook",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#run-this-notebook",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#approach",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#approach",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Approach",
    "text": "Approach\nThis notebook creates STAC collection metadata for a CMIP6 Kerchunk Reference File which has already been generated and stored in S3.\nThis notebook serves as documentation for the publication of the CMIP6 kerchunk reference. It is not expected to generalize for arbitrary Zarr datasets but may be a helpful example. It was run on the VEDA JupyterHub and since veda-data-store-staging is a protected bucket it is not expected to work in an environment without access to that bucket."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-1-install-and-import-necessary-libraries",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-1-install-and-import-necessary-libraries",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 1: Install and import necessary libraries",
    "text": "Step 1: Install and import necessary libraries\n\n#!pip install xstac\nimport pystac\nimport requests\nimport s3fs\nimport xstac\nimport fsspec\nimport xarray as xr"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-2-open-the-dataset-with-xarray",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-2-open-the-dataset-with-xarray",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 2: Open the dataset with xarray",
    "text": "Step 2: Open the dataset with xarray\n\ndataset_url = 's3://veda-data-store-staging/cmip6-GISS-E2-1-G-tas-kerchunk/combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json'\n\nxr_open_args = {\n    \"engine\": \"zarr\",\n    \"decode_coords\": \"all\",\n    \"consolidated\": False\n}\n\nfs = fsspec.filesystem(\n    \"reference\",\n    fo=dataset_url,\n    remote_options={\"anon\": True},\n)\nsrc_path = fs.get_mapper(\"\")\n\nds = xr.open_dataset(src_path, **xr_open_args)\n\n/tmp/ipykernel_5419/732403854.py:16: UserWarning: Variable(s) referenced in cell_measures not in variables: ['areacella']\n  ds = xr.open_dataset(src_path, **xr_open_args)"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-3-generate-stac-metadata",
    "href": "instance-management/notebooks/veda-operations/publish-cmip6-kerchunk-stac.html#step-3-generate-stac-metadata",
    "title": "Publishing a CMIP6 Kerchunk Reference to STAC",
    "section": "Step 3: Generate STAC metadata",
    "text": "Step 3: Generate STAC metadata\nThe spatial extent is taken from the xarray metadata. The temporal extent will be added by the xstac library.\n\nspatial_extent_values = [ds.lon[0].values, ds.lat[0].values, ds.lon[-1].values, ds.lat[-1].values]\nspatial_extent = list(map(int, spatial_extent_values))\n_id = 'combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk_TEST'\nzarr_asset = pystac.Asset(\n    title='zarr',\n    href=dataset_url,\n    media_type='application/vnd+zarr',\n    roles=['data'],\n)\nextent = pystac.Extent(\n    spatial=pystac.SpatialExtent(bboxes=[spatial_extent]),\n    temporal=pystac.TemporalExtent([[None, None]])\n)\n\nAdd the VEDA provider.\n\nproviders = [\n    pystac.Provider(\n        name=\"VEDA\",\n        roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST],\n        url=\"https://github.com/nasa-impact/veda-data-pipelines\",\n    )\n]\n\nPut it all together to intialize a pystac.Collection instance.\n\ncollection = pystac.Collection(\n    id=_id,\n    extent=extent,\n    assets = {'zarr': zarr_asset},\n    description='for zarr testing',\n    providers=providers,\n    stac_extensions=['https://stac-extensions.github.io/datacube/v2.0.0/schema.json'],\n    license=\"CC0-1.0\"\n)\n\nThat collection instance is used by xstac to generate additional metadata, such as the temporal extent and the datacube extension information.\n\ncollection_template = collection.to_dict()\ncollection = xstac.xarray_to_stac(\n    ds,\n    collection_template,\n    temporal_dimension=\"time\",\n    x_dimension=\"lon\",\n    y_dimension=\"lat\",\n    # TODO: get this from attributes if possible\n    reference_system=\"4326\",\n    validate=False\n)\n# It should validate, yay!\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.0.0/collection-spec/json-schema/collection.json',\n 'https://stac-extensions.github.io/datacube/v2.0.0/schema.json']"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#run-this-notebook",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#run-this-notebook",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#approach",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#approach",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Approach",
    "text": "Approach\nThis notebook demonstrates how to create a kerchunk reference for the AWS Open Data Registry of NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) NetCDF files on S3. Because the NetCDF files are publicly avaialble, this notebook should be runnable in any environment with the imported libraries, up until the last step where the kerchunk reference file is stored in the veda-data-store-staging S3 bucket, as that is a protected bucket.\nTo see how to publish a kerchunk reference to a STAC collection, see the Publishing a CMIP6 Kerchunk Reference to STAC notebook."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-1-setup",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-1-setup",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 1: Setup",
    "text": "Step 1: Setup\nImport necessary libraries and define some variables for which CMIP6 variable and model we will create references for.\n\nimport os\nfrom tempfile import TemporaryDirectory\n\nimport boto3\nimport dask.bag as db\nimport fsspec\nimport ujson\nimport xarray as xr\nfrom dask_gateway import Gateway, GatewayCluster\nfrom kerchunk.combine import MultiZarrToZarr\nfrom kerchunk.hdf import SingleHdf5ToZarr\n\n# Specify the CMIP model and variable to use.\n# Here we are using near-surface air temperature from the GISS-E2-1-G GCM\nmodel = \"GISS-E2-1-G\"\nvariable = \"tas\"\n# If this code were re-used for a protected bucket, anon should be False.\nanon = True\n# Note: We are only using the historical data in this example.\n# More years of data are available from multiple Shared Socio-Economic Pathways (SSPs) in the s3://nex-gddp-cmip6 bucket.\ns3_path = f\"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/{model}/historical/r1i1p1*/{variable}/*\""
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-2-initiate-file-systems-for-reading-and-temporary-writing",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-2-initiate-file-systems-for-reading-and-temporary-writing",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 2: Initiate file systems for reading and (temporary) writing",
    "text": "Step 2: Initiate file systems for reading and (temporary) writing\n\nfs_read = fsspec.filesystem(\"s3\", anon=anon, skip_instance_cache=False)\n\n# Create a temporary directory to store the .json reference files\n# Alternately, you could write these to cloud storage.\ntd = TemporaryDirectory()\ntemp_dir = td.name\nprint(f\"Writing single file references to {temp_dir}\")\n\nWriting single file references to /tmp/tmpugv4rwhn"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-3-discover-files-from-s3",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-3-discover-files-from-s3",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 3: Discover files from S3",
    "text": "Step 3: Discover files from S3\n\n# List available files for this model and variable\nall_files = sorted([\"s3://\" + f for f in fs_read.glob(s3_path)])\nprint(f\"{len(all_files)} discovered from {s3_path}\")\n\n65 discovered from s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/GISS-E2-1-G/historical/r1i1p1*/tas/*"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-4-define-some-functions-for-creating-and-storing-kerchunk-reference-files-for-single-files",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-4-define-some-functions-for-creating-and-storing-kerchunk-reference-files-for-single-files",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 4: Define some functions for creating and storing Kerchunk reference files for single files",
    "text": "Step 4: Define some functions for creating and storing Kerchunk reference files for single files\n\nso = dict(mode=\"rb\", anon=anon, default_fill_cache=False, default_cache_type=\"first\")\n\n# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\n\n\ndef generate_json_reference(u):\n    with fs_read.open(u, **so) as infile:\n        fname = u.split(\"/\")[-1].strip(\".nc\")\n        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n        return fname, ujson.dumps(h5chunks.translate()).encode()\n\n\ndef write_json(fname, reference_json, temp_dir):\n    outf = os.path.join(temp_dir, f\"{fname}.json\")\n    with open(outf, \"wb\") as f:\n        f.write(reference_json)\n    return outf\n\nTest we can create a kerchunk reference for one file.\n\nfname, ref_json = generate_json_reference(all_files[0])\nwrite_json(fname, ref_json, temp_dir)\n\n'/tmp/tmpugv4rwhn/tas_day_GISS-E2-1-G_historical_r1i1p1f2_gn_1950.json'"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-5-use-a-dask-cluster-to-generate-references-for-all-the-data",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#step-5-use-a-dask-cluster-to-generate-references-for-all-the-data",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Step 5: Use a dask cluster to generate references for all the data",
    "text": "Step 5: Use a dask cluster to generate references for all the data\nStart the cluster and check out the dashboard for active workers, as it may take a few seconds or minutes for them to start up.\nThis was run on the VEDA JupyterHub which has access to a distributed cluster. You could also create a LocalCluster instance to run the dask bag code below. But because this code is not using DaskArrays, you could also use a regular multiprocessing library to distribute the generate_json_refence tasks.\n\ngateway = Gateway()\nclusters = gateway.list_clusters()\n\n# connect to an existing cluster - this is useful when the kernel shutdown in the middle of an interactive session\nif clusters:\n    cluster = gateway.connect(clusters[0].name)\nelse:\n    cluster = GatewayCluster(shutdown_on_close=True)\n\ncluster.scale(16)\nclient = cluster.get_client()\nclient\n\n\n     \n    \n        Client\n        Client-1e000625-59cf-11ef-8709-c2be3c95b571\n        \n\n\n\nConnection method: Cluster object\nCluster type: dask_gateway.GatewayCluster\n\n\nDashboard: /services/dask-gateway/clusters/prod.f49e93767f6f44469809f0227bf42fc8/status\n\n\n\n\n\n\n        \n            \n                Launch dashboard in JupyterLab\n            \n        \n\n        \n            \n            Cluster Info\n            \n  GatewayCluster\n  \n    Name: prod.f49e93767f6f44469809f0227bf42fc8\n    Dashboard: /services/dask-gateway/clusters/prod.f49e93767f6f44469809f0227bf42fc8/status\n  \n\n\n            \n        \n\n    \n\n\n\n\nGenerate a dask bag for all the files and store files in the temp_dir\n\n%%time\nbag = db.from_sequence(all_files, partition_size=1)\nresult = db.map(generate_json_reference, bag)\nall_references = result.compute()\noutput_files = [\n    write_json(fname, reference_json, temp_dir)\n    for fname, reference_json in all_references\n]\n\nCPU times: user 118 ms, sys: 24.9 ms, total: 143 ms\nWall time: 2min 39s\n\n\n\n\nStep 6: Combine individual references into a single consolidated reference\nStore it to local storage and test opening it.\n\n%%time\nmzz = MultiZarrToZarr(\n    output_files,\n    remote_protocol=\"s3\",\n    remote_options={\"anon\": anon},\n    concat_dims=[\"time\"],\n    coo_map={\"time\": \"cf:time\"},\n    inline_threshold=0,\n)\nmulti_kerchunk = mzz.translate()\n\nCPU times: user 961 ms, sys: 11.1 ms, total: 972 ms\nWall time: 950 ms\n\n\nWrite the kerchunk .json file to local storage\n\noutput_fname = f\"combined_CMIP6_daily_{model}_{variable}_kerchunk.json\"\noutput_location = os.path.join(temp_dir, output_fname)\nwith open(f\"{output_location}\", \"wb\") as f:\n    print(f\"Writing combined kerchunk reference file {output_location}\")\n    f.write(ujson.dumps(multi_kerchunk).encode())\n\nWriting combined kerchunk reference file /tmp/tmpugv4rwhn/combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json\n\n\n\n# open dataset as zarr object using fsspec reference file system and Xarray\nfs = fsspec.filesystem(\n    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": anon}\n)\nm = fs.get_mapper(\"\")\n\n\n# Check the data\nds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\ndisplay(ds)  ##noqa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 82GB\nDimensions:  (lat: 600, lon: 1440, time: 23725)\nCoordinates:\n  * lat      (lat) float64 5kB -59.88 -59.62 -59.38 -59.12 ... 89.38 89.62 89.88\n  * lon      (lon) float64 12kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\n  * time     (time) object 190kB 1950-01-01 12:00:00 ... 2014-12-31 12:00:00\nData variables:\n    tas      (time, lat, lon) float32 82GB ...\nAttributes: (12/23)\n    Conventions:           CF-1.7\n    activity:              NEX-GDDP-CMIP6\n    cmip6_institution_id:  NASA-GISS\n    cmip6_license:         CC-BY-SA 4.0\n    cmip6_source_id:       GISS-E2-1-G\n    contact:               Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget...\n    ...                    ...\n    scenario:              historical\n    source:                BCSD\n    title:                 GISS-E2-1-G, r1i1p1f2, historical, global downscal...\n    tracking_id:           25d6baa3-0404-4eba-a3f1-afddbf69d4cc\n    variant_label:         r1i1p1f2\n    version:               1.0xarray.DatasetDimensions:lat: 600lon: 1440time: 23725Coordinates: (3)lat(lat)float64-59.88 -59.62 ... 89.62 89.88axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([-59.875, -59.625, -59.375, ...,  89.375,  89.625,  89.875])lon(lon)float640.125 0.375 0.625 ... 359.6 359.9axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02])time(time)object1950-01-01 12:00:00 ... 2014-12-...axis :Tlong_name :timestandard_name :timearray([cftime.DatetimeNoLeap(1950, 1, 1, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1950, 1, 2, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1950, 1, 3, 12, 0, 0, 0, has_year_zero=True), ...,\n       cftime.DatetimeNoLeap(2014, 12, 29, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 30, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(2014, 12, 31, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)Data variables: (1)tas(time, lat, lon)float32...cell_measures :area: areacellacell_methods :area: mean time: maximumcomment :near-surface (usually, 2 meter) air temperature; derived from downscaled tasmax & tasminlong_name :Daily Near-Surface Air Temperaturestandard_name :air_temperatureunits :K[20498400000 values with dtype=float32]Indexes: (3)latPandasIndexPandasIndex(Index([-59.875, -59.625, -59.375, -59.125, -58.875, -58.625, -58.375, -58.125,\n       -57.875, -57.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,   1.875,\n         2.125,   2.375,\n       ...\n       357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125, 359.375,\n       359.625, 359.875],\n      dtype='float64', name='lon', length=1440))timePandasIndexPandasIndex(CFTimeIndex([1950-01-01 12:00:00, 1950-01-02 12:00:00, 1950-01-03 12:00:00,\n             1950-01-04 12:00:00, 1950-01-05 12:00:00, 1950-01-06 12:00:00,\n             1950-01-07 12:00:00, 1950-01-08 12:00:00, 1950-01-09 12:00:00,\n             1950-01-10 12:00:00,\n             ...\n             2014-12-22 12:00:00, 2014-12-23 12:00:00, 2014-12-24 12:00:00,\n             2014-12-25 12:00:00, 2014-12-26 12:00:00, 2014-12-27 12:00:00,\n             2014-12-28 12:00:00, 2014-12-29 12:00:00, 2014-12-30 12:00:00,\n             2014-12-31 12:00:00],\n            dtype='object', length=23725, calendar='noleap', freq='D'))Attributes: (23)Conventions :CF-1.7activity :NEX-GDDP-CMIP6cmip6_institution_id :NASA-GISScmip6_license :CC-BY-SA 4.0cmip6_source_id :GISS-E2-1-Gcontact :Dr. Rama Nemani: rama.nemani@nasa.gov, Dr. Bridget Thrasher: bridget@climateanalyticsgroup.orgcreation_date :2021-10-04T18:41:40.796912+00:00disclaimer :This data is considered provisional and subject to change. This data is provided as is without any warranty of any kind, either express or implied, arising by law or otherwise, including but not limited to warranties of completeness, non-infringement, accuracy, merchantability, or fitness for a particular purpose. The user assumes all risk associated with the use of, or inability to use, this data.downscalingModel :BCSDexternal_variables :areacellafrequency :dayhistory :2021-10-04T18:41:40.796912+00:00: install global attributesinstitution :NASA Earth Exchange, NASA Ames Research Center, Moffett Field, CA 94035product :outputrealm :atmosreferences :BCSD method: Thrasher et al., 2012, Hydrol. Earth Syst. Sci.,16, 3309-3314. Ref period obs: latest version of the Princeton Global Meteorological Forcings (http://hydrology.princeton.edu/data.php), based on Sheffield et al., 2006, J. Climate, 19 (13), 3088-3111.resolution_id :0.25 degreescenario :historicalsource :BCSDtitle :GISS-E2-1-G, r1i1p1f2, historical, global downscaled CMIP6 climate projection datatracking_id :25d6baa3-0404-4eba-a3f1-afddbf69d4ccvariant_label :r1i1p1f2version :1.0\n\n\n\n# Close the cluster\nclient.close()\ncluster.close()"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#final-step-upload-kerchunk-to-veda-bucket",
    "href": "instance-management/notebooks/veda-operations/generate-cmip6-kerchunk-historical.html#final-step-upload-kerchunk-to-veda-bucket",
    "title": "Generate Kerchunk Reference from CMIP6 NetCDF files",
    "section": "Final Step: Upload Kerchunk to VEDA Bucket",
    "text": "Final Step: Upload Kerchunk to VEDA Bucket\nYou can skip this if you are not trying to upload the reference file to veda-data-store-staging.\nIf you are on the VEDA JupyterHub, you should have access to veda-data-store-staging.\n\ns3 = boto3.client(\"s3\")\nupload_bucket_name = \"veda-data-store-staging\"\nresponse = s3.upload_file(\n    output_location,\n    upload_bucket_name,\n    f\"cmip6-{model}-{variable}-kerchunk/{output_fname}\",\n)\n# None is good.\nprint(f\"Response uploading {output_fname} to {upload_bucket_name} was {response}.\")\n\nResponse uploading combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json to veda-data-store-staging was None.\n\n\n2023-10-04 21:59:02,340 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client"
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#run-this-notebook",
    "title": "Air Quality and COVID-19",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#approach",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#approach",
    "title": "Air Quality and COVID-19",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - NO₂\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize two tiles (side-by-side) allowing for comparison of each of the time points using folium.plugins.DualMap"
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#about-the-data",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#about-the-data",
    "title": "Air Quality and COVID-19",
    "section": "About the Data",
    "text": "About the Data\nThis dataset is of monthly nitrogen dioxide (NO₂) levels values across the globe. Darker colors indicate higher NO₂ levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow."
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#the-case-study---air-quality-and-covid-19",
    "title": "Air Quality and COVID-19",
    "section": "The Case Study - Air Quality and COVID-19",
    "text": "The Case Study - Air Quality and COVID-19\nIn this notebook, we’ll walk through the development of side-by-side comparisons of NO₂ levels before and after government lockdowns as demonstrated Seeing Rebounds in NO₂ in this VEDA Discovery story: Air Quality and COVID-19 available on the VEDA Dashboard."
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#querying-the-stac-api",
    "title": "Air Quality and COVID-19",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# Declare collection of interest - Nitrogen Oxide\ncollection_name = \"no2-monthly\"\n\n\n#Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'assets': {'thumbnail': {'href': 'https://thumbnails.openveda.cloud/no2--dataset-cover.jpg',\n   'type': 'image/jpeg',\n   'roles': ['thumbnail'],\n   'title': 'Thumbnail',\n   'description': 'Photo by [Mick Truyts](https://unsplash.com/photos/x6WQeNYJC1w) (Power plant shooting steam at the sky)'}},\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2016-01-01T00:00:00+00:00',\n     '2022-12-31T00:00:00+00:00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'bidx': [1],\n   'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 15000000000000000]],\n   'resampling': 'bilinear',\n   'color_formula': 'gamma r 1.05',\n   'colormap_name': 'rdbu_r'}},\n 'providers': [{'url': 'https://disc.gsfc.nasa.gov/',\n   'name': 'NASA Goddard Earth Sciences Data and Information Services Center',\n   'roles': ['producer', 'processor']},\n  {'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2023-09-30T00:00:00Z']},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/render/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from January 2015 to December 2022. By looking at the dashboard:time density we observe that the periodic frequency of these observations is monthly.\n\n# Check total number of items available\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\").json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 93 items\n\n\nThis makes sense as there are 7 years between 2016 - 2022, with 12 months per year, meaning 84 records in total.\nBelow, we’ll provide the max range of values to apply to visualizations of all items in the collection (rescale_values).\n\nrescale_values = {\n    \"max\": 50064805976866816,\n    \"min\": -1018382487283302\n}"
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#exploring-changes-in-nitrogen-oxide-no₂-related-to-changes-in-human-behavior---using-the-raster-api",
    "title": "Air Quality and COVID-19",
    "section": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API",
    "text": "Exploring Changes in Nitrogen Oxide (NO₂) Related to Changes in Human Behavior - Using the Raster API\nWe will explore changes in air quality due to changes in human behaviour resulting from the COVID-19 pandemic. With people largely confined to their homes to reduce the spread of the novel coronavirus, scientists were anticipated there were likely to be fewer cars, planes, and ships emitting fossil fuel pollution. In this notebook, we’ll explore the impacts these government lockdowns had on specific air pollutants (i.e., NO₂) and explore these changes over time. We’ll then visualize the outputs on a map using folium.\n\n# to access the year value from each item more easily, this will let us query more explicity by year and month (e.g., 2020-02)\nitems = {item[\"properties\"][\"start_datetime\"][:7]: item for item in items} \n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this twice times, once for February 2020 and again for February 2022, so that we can visualize each event independently.\n\nfebruary_2020_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2020-02']['collection']}&item={items['2020-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2020_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202002_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-1018382487283302%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\nfebruary_2022_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2022-02']['collection']}&item={items['2022-02']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=cool\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\", \n).json()\nfebruary_2022_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=no2-monthly&item=OMI_trno2_0.10x0.10_202202_Col3_V4.nc&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=cool&rescale=-1018382487283302%2C50064805976866816'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}"
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#seeing-rebounds-in-no₂",
    "title": "Air Quality and COVID-19",
    "section": "Seeing Rebounds in NO₂",
    "text": "Seeing Rebounds in NO₂\nAir pollutants with short lifespans, like NO₂, decreased dramatically with COVID-related shutdowns in the spring of 2020 (see lefthand side map). As the world began to re-open and mobility restrictions eased, travel increased and alongside it NO₂ pollutants. Air quality levels are now returning to pre-pandemic levels (see righthand side map).\nScroll and zoom within the maps below, the side-by-side comparison will follow wherever you explore. Darker purples indicate higher NO₂ levels and more activity. Lighter blues indicate lower levels of NO₂ and less activity.\n\n# We'll import folium to map and folium.plugins to allow mapping side-by-side\nimport folium\nimport folium.plugins\n\n# Set initial zoom and map for NO2 Layer\nm = folium.plugins.DualMap(location=(33.6901, 118.9325), zoom_start=5)\n\n# February 2020\nmap_layer_2020 = TileLayer(\n    tiles=february_2020_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2020.add_to(m.m1)\n\n# February 2022\nmap_layer_2022 = TileLayer(\n    tiles=february_2022_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.8,\n)\nmap_layer_2022.add_to(m.m2)\n\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/air-quality-covid.html#summary",
    "href": "instance-management/notebooks/datasets/air-quality-covid.html#summary",
    "title": "Air Quality and COVID-19",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors NO₂ emissions from space. By showcasing lockdown (February 2020) and post-lockdown (February 2022) snapshots of air quality side-by-side, we demonstrate how quickly atmospheric NO₂ responds to reductions in emissions and human behavior."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#run-this-notebook",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#approach",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#approach",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection - SO2\nPass the STAC item into raster API /stac/tilejson.json endpoint\nWe’ll visualize tiles for each of the time steps of interest using folium"
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#about-the-data",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "About the Data",
    "text": "About the Data\nCollecting measurements of Sulfur Dioxide (SO2) plumes from space is a valuable way to monitor changes in emissions. The SO2 index product is used by NASA to monitor volcanic clouds and pre-eruptive volcanic gas emissions activity. Additionally, this information is used in advisories to airlines for operational decisions.\nIn this notebook, we will explore the Sulfur Dioxide dataset and how it was used in this VEDA Discovery article to monitor air pollution across the globe."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#querying-the-stac-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provide STAC and RASTER API endpoints\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# Declare collection of interest - Sulfur Dioxide\ncollection_name = \"OMSO2PCA-COG\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'OMSO2PCA-COG',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/OMSO2PCA-COG/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/OMSO2PCA-COG'}],\n 'title': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'assets': {'thumbnail': {'href': 'https://thumbnails.openveda.cloud/so2--dataset-cover.jpg',\n   'type': 'image/jpeg',\n   'roles': ['thumbnail'],\n   'title': 'Thumbnail',\n   'description': 'Photo by NASA (2018 Social Vulnerability Index (SVI) based on minority status and language score)'}},\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2005-01-01T00:00:00+00:00',\n     '2021-12-31T00:00:00+00:00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'bidx': [1],\n   'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 1]],\n   'resampling': 'bilinear',\n   'colormap_name': 'rdylbu_r'}},\n 'providers': [{'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2005-01-01T00:00:00Z', '2021-01-01T00:00:00Z']},\n 'description': 'OMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/render/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'year'}\n\n\nExamining the contents of our collection under summaries we see that the data is available from 2005 to 2021. By looking at the dashboard:time density we observe that the periodic frequency of these observations is yearly.\nWe can verify this by checking the total items returned from our STAC API requests.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 17 items\n\n\nThis makes sense as there are 17 years between 2005 - 2021."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#exploring-sulfur-dioxide-plumes-from-space---using-the-raster-api",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API",
    "text": "Exploring Sulfur Dioxide Plumes from Space - Using the Raster API\nWe’ll explore three different time steps to show how NASA has observed volcanic activity in the Galápagos islands (2005), detected large scale emissions on the Kamchatka Peninsula (2009), and monitored the eruptions of Fagradalsfjall in Iceland (2021). We’ll then visualize the outputs on a map using folium.\nTo start, we’ll identify which item value corresponds to each year of interest and setting a rescaling_factor for the SO2 index, so that values range from 0 to 1.\n\n# to access the year value from each item more easily\nitems = {item[\"properties\"][\"datetime\"][:4]: item for item in items}\n\n\nrescaling_factor = \"0,1\"\n\nNow we will pass the item id, collection name, and rescaling_factor to the Raster API endpoint. We will do this three times, one for each time step of interest, so that we can visualize each event independently.\n\ntile_2005 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2005']['collection']}&item={items['2005']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2005\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2005&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2009 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2009']['collection']}&item={items['2009']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2009\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2009&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\n\ntile_2021 = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items['2021']['collection']}&item={items['2021']['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescaling_factor}\",\n).json()\ntile_2021\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=OMSO2PCA-COG&item=OMSO2PCA_LUT_SCD_2021&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=0%2C1'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWe will then use the tile URL prepared above to create a simple visualization for each time step using folium. In each of these visualizations you can zoom in and out of the map’s focus area to explore the data layer for that year."
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-galápagos-islands-2005",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Galápagos islands (2005)",
    "text": "Visualizing Galápagos islands (2005)\n\n# Set initial zoom and map for Galápagos islands\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -0.915435,\n        -89.57216,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2005[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-kamchatka-peninsula-2009",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Kamchatka Peninsula (2009)",
    "text": "Visualizing Kamchatka Peninsula (2009)\n\n# Set initial zoom and map for Kamchatka Peninsula\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        53.018234,\n        158.67016,\n    ],\n    zoom_start=7,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2009[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#visualizing-fagradalsfjall-iceland-2021",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Visualizing Fagradalsfjall, Iceland (2021)",
    "text": "Visualizing Fagradalsfjall, Iceland (2021)\n\n# Set initial zoom and map for Fagradalsfjall, Iceland\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        65.0294256,\n        -18.393870,\n    ],\n    zoom_start=6,\n)\n\nmap_layer = TileLayer(\n    tiles=tile_2021[\"tiles\"][0],\n    attr=\"VEDA\",\n    opacity=0.6,\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#summary",
    "href": "instance-management/notebooks/datasets/volcano-so2-monitoring.html#summary",
    "title": "Monitoring Volcanic Sulfur Dioxide Emissions",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized how NASA monitors sulfur dioxide emissions from space, by showcasing three different examples across the globe: volcanic activity in the Galápagos islands (2005), large scale emissions on the Kamchatka Peninsula (2009), and eruptions of Fagradalsfjall in Iceland (2021)."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#run-this-notebook",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "",
    "text": "You can launch this notebook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#approach",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates and temporal frequency of observations for a given collection\nPass STAC item into raster API /stac/tilejson.json endpoint\nGet time series statistics over available time period to identify seasonal trends\nVisualize peak by displaying the tile in folium\nVisualize time series of raster images"
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#about-the-data",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "About the Data",
    "text": "About the Data\nOcean Net Primary Production (NPP) is the result of CO2 fixation, through photosynthesis, by marine phytoplankton which contain chlorophyll. It is the proportion of phytoplankton-sequestered carbon that enters the oceanic food web and supports a variety of marine life."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#the-case-study---walvis-bay-namibia",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "The Case Study - Walvis Bay, Namibia",
    "text": "The Case Study - Walvis Bay, Namibia\nWalvis Bay is home to Namibia’s largest marine farming center and a well established commercial fishing industry. It’s location in the nutrient-rich Benguela upwelling system of the Atlantic Ocean, means producers can rely on this area to cultivate an abundance of shellfish including oysters, mussels, and scallops.\nOccasionally the nutrient-rich waters of the Atlantic produce higher than normal NPP levels, resulting in short-lived harmful algal blooms. This is often a result of both favorable temperatures and abundance of sufficient nutrients. The resulting algal blooms can have severe consequences causing massive fish kills, contaminating seafood with toxins and creating an unsafe environment for humans and marine life. Toxins accumulated in the shellfish organs can be subsequently transmitted to humans through consumption and resulting in serious health threats.\nIn this example we explore the Ocean NPP dataset over the year 2020 to identify spatial and temporal patterns of Ocean NPP in the Walvis Bay area."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#querying-the-stac-api",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nimport requests\nfrom folium import Map, TileLayer\n\n\n# Provife STAC and RASTER API endpoints\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# Declare collection of interest - Ocean NPP\ncollection_name = \"MO_NPP_npp_vgpm\"\n\n\n# Fetch STAC collection\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\ncollection\n\n{'id': 'MO_NPP_npp_vgpm',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/MO_NPP_npp_vgpm/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/MO_NPP_npp_vgpm'}],\n 'title': '',\n 'assets': {'thumbnail': {'href': 'https://thumbnails.openveda.cloud/ocean-production--dataset-cover.jpg',\n   'type': 'image/jpeg',\n   'roles': ['thumbnail'],\n   'title': 'Thumbnail',\n   'description': 'Photo by [Karl Callwood](https://unsplash.com/photos/Ko1sGLhZm5w) (Rocky ocean shore)'}},\n 'extent': {'spatial': {'bbox': [[-180.0000050868518,\n     -90.00000508655744,\n     180.0000050868518,\n     89.9999974571629]]},\n  'temporal': {'interval': [['2020-01-01T00:00:00+00:00',\n     '2020-12-31T00:00:00+00:00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 1500]],\n   'colormap_name': 'jet'}},\n 'providers': [{'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2020-01-01T00:00:00Z', '2020-12-31T00:00:00Z']},\n 'description': 'Ocean Net Primary Production (NPP): https://oceancolor.gsfc.nasa.gov/atbd/npp/',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/render/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/item-assets/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\n# Verify frequency of data available\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\n# Get collection summary\ncollection[\"summaries\"]\n\n{'datetime': ['2020-01-01T00:00:00Z', '2020-12-31T00:00:00Z']}\n\n\nGreat, we can explore the year 2020 time series. Let’s create a bounding box to explore the Walvis Bay area of interest (AOI) in Namibia\n\n# Walvis Bay, Namibia\nwalvis_bay_aoi = {\n    \"type\": \"Feature\",\n    \"properties\": {},\n    \"geometry\": {\n        \"coordinates\": [\n            [\n                [13.686159004559698, -21.700046934333145],\n                [13.686159004559698, -23.241974326585833],\n                [14.753560168039911, -23.241974326585833],\n                [14.753560168039911, -21.700046934333145],\n                [13.686159004559698, -21.700046934333145],\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\nLet’s visualize the AOI we have just created using folium\n\n# We'll plug in the coordinates for a location\n# central to the study area and a reasonable zoom level\n\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nfolium.GeoJson(walvis_bay_aoi, name=\"Walvis Bay\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nReturning back to our STAC API requests, let’s check how many total items are available.\n\n# Check total number of items available\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_name}/items?limit=100\"\n).json()[\"features\"]\nprint(f\"Found {len(items)} items\")\n\nFound 12 items\n\n\nThis makes sense is our collection is monthly, so we should have 12 total items.\n\n# Explore one item to see what it contains\nitems[0]\n\n{'id': 'A_202012.L3m_MO_NPP_npp_vgpm_4km',\n 'bbox': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/MO_NPP_npp_vgpm'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/MO_NPP_npp_vgpm/items/A_202012.L3m_MO_NPP_npp_vgpm_4km'},\n  {'title': 'Map of Item',\n   'href': 'https://openveda.cloud/api/raster/stac/map?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&rescale=0%2C1500&colormap_name=jet',\n   'rel': 'preview',\n   'type': 'text/html'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store/MO_NPP_npp_vgpm/A_202012.L3m_MO_NPP_npp_vgpm_4km.tif',\n   'type': 'image/tiff; application=geotiff',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'proj:bbox': [-180.0000050868518,\n    -90.00000508655744,\n    180.0000050868518,\n    89.9999974571629],\n   'proj:epsg': 4326,\n   'proj:wkt2': 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n   'proj:shape': [4320, 8640],\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -32767.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 18305.302734375,\n      'min': 20.69771957397461,\n      'count': 11,\n      'buckets': [223827, 2620, 363, 105, 45, 20, 13, 4, 1, 4]},\n     'statistics': {'mean': 448.69620531977694,\n      'stddev': 450.1186820854004,\n      'maximum': 18305.302734375,\n      'minimum': 20.69771957397461,\n      'valid_percent': 43.29719543457031}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0000050868518, -90.00000508655744],\n      [180.0000050868518, -90.00000508655744],\n      [180.0000050868518, 89.9999974571629],\n      [-180.0000050868518, 89.9999974571629],\n      [-180.0000050868518, -90.00000508655744]]]},\n   'proj:projjson': {'id': {'code': 4326, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.041666667844178655,\n    0.0,\n    -180.0000050868518,\n    0.0,\n    -0.04166666725549082,\n    89.9999974571629,\n    0.0,\n    0.0,\n    1.0]},\n  'rendered_preview': {'title': 'Rendered preview',\n   'href': 'https://openveda.cloud/api/raster/stac/preview.png?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&rescale=0%2C1500&colormap_name=jet',\n   'rel': 'preview',\n   'roles': ['overview'],\n   'type': 'image/png'}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180.0000050868518, -90.00000508655744],\n    [180.0000050868518, -90.00000508655744],\n    [180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, 89.9999974571629],\n    [-180.0000050868518, -90.00000508655744]]]},\n 'collection': 'MO_NPP_npp_vgpm',\n 'properties': {'end_datetime': '2020-12-31T00:00:00+00:00',\n  'start_datetime': '2020-12-01T00:00:00+00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/raster/v1.1.0/schema.json',\n  'https://stac-extensions.github.io/projection/v1.1.0/schema.json']}\n\n\nNow that we have explored the collection metadata by querying the STAC API, we can use the RASTER API to access the data itself.\nNOTE: The RASTER_API expects AOI as a Feature or a FeatureCollection. The datatype of the input matches the datatype of the output. So if you use a FeatureCollection as input, you will get back a FeatureCollection.\n\ndef generate_stats(item, aoi):\n    \"\"\" Generate statistics for a particular item and AOI\n\n    NOTE: This function assumes that the AOI is a geojson `Feature`.\n    \"\"\"\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][\"cog_default\"][\"href\"]},\n        json=aoi,\n    ).json()\n\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\n%%time\nstats = [generate_stats(item, walvis_bay_aoi) for item in items]\n\nCPU times: user 52.5 ms, sys: 348 µs, total: 52.8 ms\nWall time: 7.7 s\n\n\nWith the function provided above, we can generate statistics for our AOI. In the example below, we’ll explore sample statistics available from one of the tiles.\n\nstats[1]\n\n{'statistics': {'b1': {'min': 3040.676025390625,\n   'max': 20105.990234375,\n   'mean': 7803.88623046875,\n   'count': 625.0,\n   'sum': 4877429.0,\n   'std': 3784.4446091863997,\n   'median': 6578.16162109375,\n   'majority': 8848.3544921875,\n   'minority': 3040.676025390625,\n   'unique': 615.0,\n   'histogram': [[149.0, 154.0, 91.0, 76.0, 50.0, 37.0, 24.0, 26.0, 10.0, 8.0],\n    [3040.676025390625,\n     4747.20751953125,\n     6453.73876953125,\n     8160.2705078125,\n     9866.8017578125,\n     11573.3330078125,\n     13279.8642578125,\n     14986.3955078125,\n     16692.927734375,\n     18399.458984375,\n     20105.990234375]],\n   'valid_percent': 64.97,\n   'masked_pixels': 337.0,\n   'valid_pixels': 625.0,\n   'percentile_2': 3431.018310546875,\n   'percentile_98': 17551.427734375}},\n 'start_datetime': '2020-11-01T00:00:00+00:00'}\n\n\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)"
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-data-as-a-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWe can now explore the full Ocean NPP time series available (January-December 2020) for the Walvis Bay area of Namibia. We can plot the data set using the code below:\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly Ocean NPP values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NPP values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NPP values\",\n)\n\nplt.legend()\nplt.title(\"Ocean NPP Values for Walvis Bay, Namibia (2020)\")\n\nText(0.5, 1.0, 'Ocean NPP Values for Walvis Bay, Namibia (2020)')\n\n\n\n\n\n\n\n\n\nHere, we observe the seasonal variability in Ocean NPP for the Walvis Bay area. The larger peaks in the max values suggests the intensity of these events may vary spatially. Let’s explore one of the time steps (e.g., October) where there are higher maximum monthly NPP values to see if this is the case.\nImportant note: Keep in mind that the size and extent of your AOI will influence the ‘signal’ of your time series. If the phenomena you are investigating displays greater spatial variability a larger AOI will provide more ‘noise’ making it more difficult to detect."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-imagery",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the Raster Imagery",
    "text": "Visualizing the Raster Imagery\nLet’s first explore a single tile during one of the relative peaks in October, where we observe an increased sustained peak in NPP values.\n\nprint(items[2][\"properties\"][\"start_datetime\"])\n\n2020-10-01T00:00:00+00:00\n\n\n\n# Declare the range of values for this collection\nrescale_values = {\"max\": 34561.35546875, \"min\": 14.516647338867188}\n\n\noctober_tile = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={items[2]['collection']}&item={items[2]['id']}\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n    f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n).json()\noctober_tile\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0000050868518,\n  -90.00000508655744,\n  180.0000050868518,\n  89.9999974571629],\n 'center': [0.0, -3.814697265625e-06, 0]}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nimport folium\n\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=[\n        -22.421460,\n        14.268801,\n    ],\n    zoom_start=8,\n)\n\nmap_layer = TileLayer(\n    tiles=october_tile[\"tiles\"][0],\n    attr=\"VEDA\",\n)\n\nmap_layer.add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFrom the image above, we see higher Ocean NPP values (displayed in teal) located in and around Walvis Bay and the surrounding shorelines - highlighting areas of concern for the local shellfish industry."
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#visualizing-the-raster-time-series",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Visualizing the raster time series",
    "text": "Visualizing the raster time series\nNow we will look at each of the raster tiles that make up this time series to explore the spatial and temporal patterns of Ocean NPP observed in Walvis Bay throughout 2020.\nWe used the code below to examine the tiles and the order in which they are presented.\n\nimport matplotlib.pyplot as plt\n\nfor item in items:\n    tiles = requests.get(\n        f\"{RASTER_API_URL}/stac/tilejson.json?collection={item['collection']}&item={item['id']}\"\n        \"&assets=cog_default\"\n        \"&color_formula=gamma+r+1.05&colormap_name=viridis\"\n        f\"&rescale={rescale_values['min']},{rescale_values['max']}\",\n    ).json()\n    print(tiles[\"tiles\"])\n\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202012.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202011.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202010.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202009.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202008.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202007.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202006.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202005.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202004.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202003.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202002.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n['https://openveda.cloud/api/raster/stac/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?collection=MO_NPP_npp_vgpm&item=A_202001.L3m_MO_NPP_npp_vgpm_4km&assets=cog_default&color_formula=gamma+r+1.05&colormap_name=viridis&rescale=14.516647338867188%2C34561.35546875']\n\n\nSince we found the tiles to be presented in reverse time order, we’ll revise this in the code below. We’ll use reversed() to do so.\n\nimport tempfile\nfrom datetime import datetime\nimport dateutil.parser as parser\nfrom IPython.display import display, Image\n\nfor item in reversed(items):\n    response = requests.post(\n        f\"{RASTER_API_URL}/cog/feature\",\n        params={\n            \"format\": \"png\",\n            \"height\": 512,\n            \"width\": 512,\n            \"url\": item[\"assets\"][\"cog_default\"][\"href\"],\n            \"rescale\": f\"{rescale_values['min']},{rescale_values['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n        json=walvis_bay_aoi,\n    )\n    assert response.ok, response.text\n    image_bytes = response.content\n\n    # formating the datetime to make for easier reading\n    datetime_str = item[\"properties\"][\"start_datetime\"]\n    datetime_object = parser.parse(datetime_str)\n    print(datetime_object.strftime(\"%B %Y\"))\n\n    display(Image(image_bytes, height=512, width=512))\n\nJanuary 2020\n\n\n\n\n\n\n\n\n\nFebruary 2020\n\n\n\n\n\n\n\n\n\nMarch 2020\n\n\n\n\n\n\n\n\n\nApril 2020\n\n\n\n\n\n\n\n\n\nMay 2020\n\n\n\n\n\n\n\n\n\nJune 2020\n\n\n\n\n\n\n\n\n\nJuly 2020\n\n\n\n\n\n\n\n\n\nAugust 2020\n\n\n\n\n\n\n\n\n\nSeptember 2020\n\n\n\n\n\n\n\n\n\nOctober 2020\n\n\n\n\n\n\n\n\n\nNovember 2020\n\n\n\n\n\n\n\n\n\nDecember 2020"
  },
  {
    "objectID": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "href": "instance-management/notebooks/datasets/ocean-npp-timeseries-analysis.html#summary",
    "title": "Visualizing Ocean NPP time series for seasonal patterns",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully visualized the spatial and temporal variability of Ocean NPP values in the Benguela Current, which displays a seasonal pattern of peaking in the October, November, December, and January months when favorable temperatures and nutrient conditions are present."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html",
    "href": "instance-management/notebooks/tutorials/gif-generation.html",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "",
    "text": "Notebook under review\n\n\n\nThis notebook may have outdated dependencies and cell errors. It is currently under review and undergoing changes with a different set of visualization libraries."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#import-relevant-libraries",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Import relevant libraries",
    "text": "Import relevant libraries\n\n# Standard lib imports\nfrom concurrent.futures import ThreadPoolExecutor\nimport datetime\nimport glob\nimport json\nimport os\nimport requests\nimport tempfile\nimport time\nimport io\nfrom IPython import display\n\n# 3rd party imports\nimport folium\nimport numpy as np\n\n# import PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport rasterio\nimport rasterio.features\nimport rasterio.plot"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#define-global-variables",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#define-global-variables",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Define global variables",
    "text": "Define global variables\n\nSTAC_API_URL = \"https://staging.openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://staging.openveda.cloud/api/raster\"\n\n# Collection we'll be using to generate the GIF\ncollection_id = \"no2-monthly\""
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#define-an-aoi-to-crop-the-cog-data",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Define an AOI to crop the COG data",
    "text": "Define an AOI to crop the COG data\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[45, 0],\n    zoom_start=5,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#search-stac-api-for-available-data",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Search STAC API for available data",
    "text": "Search STAC API for available data\n\n# NO2 monthly has a global extent, so we don't need to specify an area within\n# which to search. For non-global datasets, use the `bbox` parameter to specify\n# the bounding box within which to search.\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_id}/items?limit=100\").json()[\n    \"features\"\n]\n\n\n# Available dates:\ndates = [item[\"properties\"][\"start_datetime\"] for item in items]\nprint(f\"Dates available: {dates[:5]} ... {dates[-5:]}\")"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#the-cogfeature-endpoint",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#the-cogfeature-endpoint",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "The /cog/feature endpoint",
    "text": "The /cog/feature endpoint\nThe endpoint accepts the following parameters, among others: - format (tif, jpeg, webp, etc) - height and width - url (for the COG file to extract data from)\nAnd any other visualization parameters specific to that dataset (eg: rescale and color_map values)\n\nGet visualization parameters:\n\n# get renders metadata\n\nrenders = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_id}\"\n).json()[\"renders\"]\n\nprint(renders)\n\nrescale = renders[\"dashboard\"][\"rescale\"][0]\n\n\n\nGenerate a PNG!\n\n# get PNG bytes from API\nresponse = requests.post(\n    f\"{RASTER_API_URL}/cog/feature\",\n    params={\n        \"format\": \"png\",\n        \"height\": 512,\n        \"width\": 512,\n        \"url\": items[0][\"assets\"][\"cog_default\"][\"href\"],\n        \"rescale\": f\"{rescale[0]},{rescale[1]}\",\n        \"colormap_name\": \"viridis\",\n    },\n    json=france_aoi,\n)\n\nassert response.ok, response.text\n\nimage_bytes = response.content\n\n# Write to temporary file in order to display\nf = tempfile.NamedTemporaryFile(suffix=\".png\")\nf.write(image_bytes)\n\n# display PNG!\ndisplay.Image(filename=f.name, height=512, width=512)"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#generating-a-gif",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#generating-a-gif",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Generating a GIF",
    "text": "Generating a GIF\nTo generate a GIF we request a PNG for each STAC Item and then use the Python Imaging Library (PIL) to combine them into a GIF. We will use a temporary directory to store all the generated PNGs and we will use multi-threading to speed up the operation\n\n# for convenience we will wrap the API call from above into a method that will\n# save the contents of the image file into a file stored within the temp directory\nfrom gif_generation_dependencies.helper_functions import generate_frame\n\n# temporary directory to hold PNGs\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    start = time.time()\n\n    args = (\n        (\n            item,  # stac item\n            france_aoi,  # aoi to crop\n            tmpdirname,  # tmpdir (optional)\n            \"png\",  # image format\n            None,  # overlay (will be discussed further)\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },  # visualization parameters\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = (Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\"))))\n    img = next(imgs)  # extract first image from iterator\n    img.save(\n        fp=\"./output.gif\",\n        format=\"GIF\",\n        append_images=imgs,\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output.gif\")"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#adding-context",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#adding-context",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Adding context",
    "text": "Adding context\nTo provide more interesting or engaging data to the users, we can add temporal and geospatial context to the GIF. This is possible because API can return images in geo-referenced tif format.\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n\n    filepath = generate_frame(items[0], france_aoi, tmpdirname, image_format=\"tif\")\n\n    # Verify that the tif returned by the API is correctly georeferenced\n    georeferenced_raster_data = rasterio.open(filepath)\n\n    print(\"Data bounds: \", georeferenced_raster_data.bounds)\n    print(\"Data CRS: \", georeferenced_raster_data.crs)\n\n\nOverlaying GeoJSON:\nIn order to overlay GeoJSON over the raster, we will have to convert the geojson boundaries to a raster format. We do this with the following steps:\nFor each feature in the geojson we rasterize the feature into a mask. We use binary dialation to detect the edges of the mask, and set the values corresponding to the mask edges to 255. This approach has one known problem: if multiple features share a border (eg: two adjoining provinces) the border between then will be detected twice, once from each side (or from each feature sharing that border). This means that internal borders will be twice as thick as external borders\n\nfrom gif_generation_dependencies.helper_functions import overlay_geojson\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        geojson = json.loads(f.read())\n\n    filepath = generate_frame(\n        items[0],\n        france_aoi,\n        tmpdirname,\n        image_format=\"tif\",\n        additional_cog_feature_args={\n            \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n            \"colormap_name\": \"viridis\",\n        },\n    )\n\n    filepath = overlay_geojson(filepath, geojson)\n    rasterio.plot.show(rasterio.open(filepath))\n\n\n\nOverlaying the raster on a basemap\nAnother way to contextualize where in the GIF’s data is, is by overlaying the GIF on top of a base map. This process is a bit more complicated: - Generate a raster image (.tif) - Overlay in on a folium map interface - Save the map interface to html - Open the html file with a headless chrome webdriver (using the selenium library) - Save a screenshot of the rendered html as a .png\n\nfrom gif_generation_dependencies.helper_functions import overlay_raster_on_folium\n\ntmpdirname = tempfile.TemporaryDirectory()\n\nimage_filepath = generate_frame(\n    items[0],\n    france_aoi,\n    tmpdirname.name,\n    image_format=\"tif\",\n    overlay=None,\n    additional_cog_feature_args={\n        \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n        \"colormap_name\": \"viridis\",\n    },\n)\n\nimage_filepath = overlay_raster_on_folium(image_filepath)\n\ndisplay.Image(filename=image_filepath)\n\n\n\nOverlaying the Date:\nNow that we have the raster data displayed over the basemap, we want to add the date of each file\n\nfrom gif_generation_dependencies.helper_functions import overlay_date\n\ndate = items[0][\"properties\"][\"start_datetime\"]\n\n# get datestring from STAC Item properties and reformat\ndatestring = datetime.datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\").date().isoformat()\n\n# Reuse the raster overlayed on the OSM basemap using folium from above:\noverlay_date(image_filepath, datestring)\n\ndisplay.Image(filename=image_filepath)"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#putting-it-all-together",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Putting it all together",
    "text": "Putting it all together\nI’ve combined all of the above functionality, along with a few helper functions in the file: ./gif_generation_dependencies/helper_functions.py\nI’ve also added the contextualizaiton steps (overlaying geojson, date, and folium basemap) directly into the generate_frame() method"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#generate-a-gif-with-geojson-overlay",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Generate a GIF with geojson overlay:",
    "text": "Generate a GIF with geojson overlay:\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    with open(\"./gif_generation_dependencies/france-departements.geojson\", \"r\") as f:\n        overlay = json.loads(f.read())\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            geojson,\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.tif\")))]\n    imgs[0].save(\n        fp=\"./output_with_geojson.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_geojson.gif\")"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#gif-with-osm-basemap-folium",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "GIF with OSM basemap (folium)",
    "text": "GIF with OSM basemap (folium)\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    start = time.time()\n\n    args = (\n        (\n            item,\n            france_aoi,\n            tmpdirname,\n            \"tif\",\n            \"folium\",\n            {\n                \"rescale\": f\"{COG_DEFAULT['min']},{COG_DEFAULT['max']}\",\n                \"colormap_name\": \"viridis\",\n            },\n        )\n        for item in items\n    )\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        result = list(executor.map(lambda a: generate_frame(*a), args))\n\n    end = time.time()\n\n    print(f\"Gather frames: {round((end-start), 2)} seconds\")\n\n    # Note: I'm searching for `*.png` files instead of *.tif files because the webdriver screenshot\n    # of the folium map interface is exported in png format (this also helps reduce the size of\n    # the final gif )\n    imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(tmpdirname, \"*.png\")))]\n    imgs[0].save(\n        fp=\"./output_with_osm_basemap.gif\",\n        format=\"GIF\",\n        append_images=imgs[1:],\n        save_all=True,\n        duration=300,\n        loop=0,\n    )\n\ndisplay.Image(filename=\"./output_with_osm_basemap.gif\")"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/gif-generation.html#cleanup",
    "href": "instance-management/notebooks/tutorials/gif-generation.html#cleanup",
    "title": "GIF generation using the TiTiler /cog/feature endpoint",
    "section": "Cleanup:",
    "text": "Cleanup:\nRun the following cell to remove the following generated images/gifs: - output.gif - output_with_geojson.gif - output_with_osm_basemap.gif\n\nfor f in glob.glob(os.path.join(\".\", \"output*.gif\")):\n    os.remove(f)"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#run-this-notebook",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#run-this-notebook",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#approach",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#approach",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Approach",
    "text": "Approach\n\nMotivation\nThe VEDA backend (via TiTiler) provides an API endpoint for computing zonal statistics (average, standard deviation, and other metrics over a geographic subset) across gridded (raster) data such as satellite imagery or climate datasets.\nSome statistics, such as average, median, standard deviation, or percentiles are sensitive to differences in grid cell / pixel sizes: when some grid cells are (in metric units) have a larger area than others, the values in these cells will be under-represented. Grid cell sizes depends on the grid / projection of the data.\nVarying grid cell sizes is common for climate datasets that are stored on a grid in geographic coordinates (lat/lon), for example a 0.1 degree by 0.1 degree global grid. Here, grid cell size will decrease from low to high latitudes. Computing averages over large spans of latitude will result in statistics where values closer to the poles are strongly over-represented.\nTo avoid this inaccuracy in zonal statistics computed with our API, we introduced a method to reproject the source data to an equal-area projection as an intermediate step in calculating statistics.\nNote: this reprojection is not needed for example for accurate zonal statistics on a Sentinel-2 scene, using the Military Grid Reference System (MGRS) and a Mercator (UTM) projection. Here, pixel areas are the same across the scene in the native projection.\n\n\nIn this notebook\nThis notebook presents a validation of VEDA’s API for zonal statistics against a known way to compute area-weighted averages for gridded datasets on a regular lat/lon grid.\nFor illustration, we choose a real dataset from the VEDA data catalog and a subsetting area that spans a large latitude range.\nThe figures below show the average calculated over that area of interest with the different methods, for comparison.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport rasterio\nimport rasterio.crs\nimport requests\nimport rioxarray  # noqa\nimport tqdm\nimport xarray as xr\nfrom pystac_client import Client"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#load-and-inspect-dataset-from-ghgc-stac-catalog",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#load-and-inspect-dataset-from-ghgc-stac-catalog",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Load and inspect dataset from GHGC STAC catalog",
    "text": "Load and inspect dataset from GHGC STAC catalog\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\nCOLLECTION_ID = \"MO_NPP_npp_vgpm\"\nASSET_NAME = \"cog_default\"\n\n\ncatalog = Client.open(STAC_API_URL)\ncollection = catalog.get_collection(COLLECTION_ID)\n\n\nitems = list(collection.get_all_items())[:15]\n\n\nwith rasterio.open(items[0].assets[ASSET_NAME].href) as ds:\n    print(ds.profile)\n\n    assert ds.crs == rasterio.crs.CRS.from_epsg(4326)\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -32767.0, 'width': 8640, 'height': 4320, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.041666667844178655, 0.0, -180.0000050868518,\n       0.0, -0.04166666725549082, 89.9999974571629), 'blockxsize': 512, 'blockysize': 512, 'tiled': True, 'compress': 'deflate', 'interleave': 'band'}"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#define-formula-for-grid-cell-area-for-geographic-coordinates",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#define-formula-for-grid-cell-area-for-geographic-coordinates",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Define formula for grid cell area for geographic coordinates",
    "text": "Define formula for grid cell area for geographic coordinates\n\ndef _get_unique_diff(arr):\n    assert np.ndim(arr) == 1\n    deltas = np.diff(arr)\n\n    if not np.allclose(deltas, deltas[0]):\n        raise ValueError(\n            f\"The spacing in the array is not uniform: {list(np.unique(deltas))}\"\n        )\n    return deltas[0]\n\n\ndef grid_cell_area(lat):\n    \"\"\"\n    https://www.mathworks.com/matlabcentral/answers/447847-how-to-calculate-the-area-of-each-grid-cell\n    https://gis.stackexchange.com/a/28156\n    \"\"\"\n    # get lat spacing asserting it is uniform\n    dlat = _get_unique_diff(lat)\n\n    # calculate cell edge lat\n    lat_edge = lat - dlat / 2.0\n\n    # radius of Earth in meters\n    R_e = 6371e3\n\n    # calculate cell area as a function of latitude\n    return (\n        R_e**2\n        * (np.sin(np.radians(lat_edge + dlat)) - np.sin(np.radians(lat_edge)))\n        * np.radians(dlat)\n    )"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#define-a-geometry-to-average-over",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#define-a-geometry-to-average-over",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Define a geometry to average over",
    "text": "Define a geometry to average over\n\nAOI_NAME = \"Americas south to north\"\n\nAOI = {\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"properties\": {},\n            \"geometry\": {\n                \"coordinates\": [\n                    [[-115, 82], [-115, -82], [-43, -82], [-43, 82], [-115, 82]]\n                ],\n                \"type\": \"Polygon\",\n            },\n        }\n    ],\n}"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compute-averages-with-xarray",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compute-averages-with-xarray",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compute averages with Xarray",
    "text": "Compute averages with Xarray\n\ntimeseries = {\"start_datetime\": [], \"average_weighted\": [], \"average_unweighted\": []}\n\nfor item in tqdm.tqdm(items):\n    item_uri = item.assets[ASSET_NAME].href\n\n    with xr.open_dataset(item_uri, engine=\"rasterio\") as xds:\n\n        # calculate area as a function of latitude\n        area_lat = grid_cell_area(xds.y.values)\n        area_lat_2d = np.ones((len(xds.y), len(xds.x))) * area_lat[:, np.newaxis]\n        xds[\"area\"] = xr.DataArray(area_lat_2d, dims=(\"y\", \"x\"))\n\n        # clip to geometry\n        xds_clip = xds.rio.clip([AOI[\"features\"][0][\"geometry\"]])\n\n        # get data arrays\n        data = xds_clip[\"band_data\"].isel(band=0).to_masked_array()\n        weights = xds_clip[\"area\"].to_masked_array()\n        weights.mask = data.mask\n\n        # calculate averages\n        average_weighted = (data * weights).sum() / weights.sum()\n        average_unweighted = data.mean()\n\n        timeseries[\"average_weighted\"].append(average_weighted)\n        timeseries[\"average_unweighted\"].append(average_unweighted)\n        timeseries[\"start_datetime\"].append(item.properties[\"start_datetime\"])\n\n        lat_vals = xds.y.values\n\n100%|██████████| 12/12 [00:52&lt;00:00,  4.39s/it]\n\n\n\ndf = pd.DataFrame(timeseries).set_index(\"start_datetime\")\ndf.index = pd.to_datetime(df.index)\n\n\ndf.plot(ylabel=f\"average {ASSET_NAME}\", title=f\"{ASSET_NAME} averaged over {AOI_NAME}\");"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#plot-grid-cell-area-as-a-function-of-latitude",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#plot-grid-cell-area-as-a-function-of-latitude",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Plot grid cell area as a function of latitude",
    "text": "Plot grid cell area as a function of latitude\n\nplt.plot(lat_vals, area_lat_2d[:, 0])\nplt.ylabel(\"Grid cell area (m²)\")\nplt.xlabel(\"Latitude\");"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compute-zonal-averages-using-titiler-api",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compute-zonal-averages-using-titiler-api",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compute zonal averages using TiTiler API",
    "text": "Compute zonal averages using TiTiler API\nWe make use of the option on TiTiler to reproject the data subset to an equal-area projection (Equal Area Cylindrical) before computing the statistics.\n\nWORKING_PROJECTION = \"+proj=cea\"\n\n\ndef generate_stats(item, geojson, asset_name, params=None):\n    params = params or {}\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item.assets[asset_name].href, **params},\n        json=geojson,\n    )\n    result.raise_for_status()\n    result_data = result.json()\n    return {\n        **result_data[\"features\"][0][\"properties\"][\"statistics\"][\"b1\"],\n        \"start_datetime\": item.properties[\"start_datetime\"],\n    }\n\n\ntimeseries_titiler_noproj = []\ntimeseries_titiler_proj = []\n\nfor item in tqdm.tqdm(items):\n    # generate stats with and without reprojection\n    stats_noproj = generate_stats(item, AOI, ASSET_NAME)\n    stats_proj = generate_stats(\n        item, AOI, ASSET_NAME, params={\"dst_crs\": WORKING_PROJECTION}\n    )\n\n    timeseries_titiler_noproj.append(stats_noproj)\n    timeseries_titiler_proj.append(stats_proj)\n\n100%|██████████| 12/12 [02:56&lt;00:00, 14.68s/it]\n\n\n\ndef _to_dataframe(stats):\n    df = pd.DataFrame(stats)\n    df = df[[\"start_datetime\", \"mean\"]]\n    df = df.set_index(\"start_datetime\")\n    df.index = pd.to_datetime(df.index)\n    return df\n\n\ndf_titiler_proj = _to_dataframe(timeseries_titiler_proj)[\"mean\"]\ndf_titiler_noproj = _to_dataframe(timeseries_titiler_noproj)[\"mean\"]"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compare-all-methods-against-the-xarray-computed-weighted-average",
    "href": "instance-management/notebooks/tutorials/zonal-statistics-validation.html#compare-all-methods-against-the-xarray-computed-weighted-average",
    "title": "Calculating accurate zonal statistics with varying grid cell / pixel area",
    "section": "Compare all methods against the xarray-computed weighted average",
    "text": "Compare all methods against the xarray-computed weighted average\n\ndf_all = df.copy()\ndf_all[\"average_titiler_noproj\"] = df_titiler_noproj\ndf_all[\"average_titiler_proj\"] = df_titiler_proj\n\n\nfig = plt.figure()\nax = fig.gca()\n\nax.set_prop_cycle(\n    linestyle=[\"-\", \"--\", \"-.\", \":\"],\n    color=[\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\"],\n    marker=[\".\", \"o\", \"x\", \"*\"],\n)\ndf_all.plot(title=f\"{ASSET_NAME} averaged over {AOI_NAME}\", ax=ax);\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nax = fig.gca()\n\nax.plot(df_all[\"average_weighted\"], df_all[\"average_weighted\"], color=\"grey\")\n\nfor key, style in {\n    \"average_titiler_proj\": dict(c=\"#e41a1c\"),\n    \"average_unweighted\": dict(c=\"#377eb8\"),\n    \"average_titiler_noproj\": dict(c=\"#984ea3\", marker=\"x\", s=100),\n}.items():\n    df_all.plot.scatter(\"average_weighted\", key, ax=ax, label=key, **style)\n\nax.set_title(f\"{ASSET_NAME} averaged over {AOI_NAME}\")\nax.set_ylabel(\"average calculated with different methods\");\n\n\n\n\n\n\n\n\n\ndef rmse(a, b):\n    return np.sqrt(np.mean((a - b) ** 2))\n\n\nkeys = [\"average_unweighted\", \"average_titiler_noproj\", \"average_titiler_proj\"]\n\ndf_rmse = pd.DataFrame(\n    {\n        \"algorithm\": keys,\n        \"average\": [rmse(df_all[key], df_all[\"average_weighted\"]) for key in keys],\n    }\n)\n\ndf_rmse = df_rmse.set_index(\"algorithm\")\ndf_rmse.plot.barh(\n    xlabel=\"RMSE wrt average_weighted\", title=f\"{ASSET_NAME} averaged over {AOI_NAME}\"\n);"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html",
    "href": "instance-management/notebooks/quickstarts/download-assets.html",
    "title": "Download STAC assets",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/download-assets.html#run-this-notebook",
    "title": "Download STAC assets",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html#approach",
    "href": "instance-management/notebooks/quickstarts/download-assets.html#approach",
    "title": "Download STAC assets",
    "section": "Approach",
    "text": "Approach\nThis notebook shows how to download data for local use.\nThis is generally not the recommended approach. Whenever possible it is better to not transfer large volumes of data out of the original physical storage location. Instead users should practice data-proximate computing by processing in the same cloud and region. That is why the data for VEDA are hosted in the same region as this VEDA JupyterHub instance.\nHowever, sometimes you do need to download assets. This might be because the assets cannot be accessed directly from remote storage, or you don’t have access to an environment running in the same cloud/region.\nFor these special cases, this is how you go about downloading data:\n\nUse pystac_client to open and search the STAC catalog\nUse stac-asset to download the assets related to that search\nIf you need the file on your local machine, zip and download the output directory\n\nNote that the default examples environment is missing the stac-asset package. We can pip install that before trying to import.\n\n!pip install -q stac-asset\n\n\nimport stac_asset\nfrom pystac_client import Client"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/download-assets.html#declare-your-collection-of-interest",
    "title": "Download STAC assets",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac/\"\ncollection = \"caldor-fire-burn-severity\"\n\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection])\n\nprint(f\"Found {len(search.item_collection())} items\")\n\nFound 1 items"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html#download-the-assets",
    "href": "instance-management/notebooks/quickstarts/download-assets.html#download-the-assets",
    "title": "Download STAC assets",
    "section": "Download the assets",
    "text": "Download the assets\nOnce you have identified the items that you are interested in, use stac_asset to download the related assets.\n\nawait stac_asset.download_item_collection(\n    search.item_collection(), \n    directory=\"data\", \n    config=stac_asset.Config(make_directory=True, s3_requester_pays=True)\n)\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"FeatureCollection\"\n        \n    \n                \n            \n                \n                    \n        features[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            type\n            \"Feature\"\n        \n    \n            \n        \n            \n                \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n            \n        \n            \n                \n        \n            id\n            \"bs_to_save\"\n        \n    \n            \n        \n            \n                \n        \n            properties\n            \n        \n            \n                \n        \n            end_datetime\n            \"2021-10-21T12:00:00+00:00\"\n        \n    \n            \n        \n            \n                \n        \n            start_datetime\n            \"2021-08-15T00:00:00+00:00\"\n        \n    \n            \n        \n            \n                \n        \n            datetime\n            None\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        links[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"collection\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openveda.cloud/api/stac/collections/caldor-fire-burn-severity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openveda.cloud/api/stac/collections/caldor-fire-burn-severity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openveda.cloud/api/stac/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"VEDA (Visualization, Exploration, and Data Analysis) STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"preview\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://openveda.cloud/api/raster/collections/caldor-fire-burn-severity/items/bs_to_save/map?assets=cog_default&rescale=0%2C5&colormap_name=inferno_r\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Map of Item\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            assets\n            \n        \n            \n                \n        \n            cog_default\n            \n        \n            \n                \n        \n            href\n            \"/home/jovyan/veda-docs/notebooks/quickstarts/data/bs_to_save/bs_to_save.tif\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Default COG Layer\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Cloud optimized default layer to display on map\"\n        \n    \n            \n        \n            \n                \n        proj:bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            proj:epsg\n            4326\n        \n    \n            \n        \n            \n                \n        \n            proj:wkt2\n            \"GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\"\n        \n    \n            \n        \n            \n                \n        proj:shape[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            1103\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            2149\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            scale\n            1.0\n        \n    \n            \n        \n            \n                \n        \n            nodata\n            -100.0\n        \n    \n            \n        \n            \n                \n        \n            offset\n            0.0\n        \n    \n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float64\"\n        \n    \n            \n        \n            \n                \n        \n            histogram\n            \n        \n            \n                \n        \n            max\n            4.0\n        \n    \n            \n        \n            \n                \n        \n            min\n            1.0\n        \n    \n            \n        \n            \n                \n        \n            count\n            11\n        \n    \n            \n        \n            \n                \n        buckets[] 10 items\n        \n            \n        \n            \n                \n        \n            0\n            10233\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            67409\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            71518\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            8\n            0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            9\n            24232\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            statistics\n            \n        \n            \n                \n        \n            mean\n            2.63295307741995\n        \n    \n            \n        \n            \n                \n        \n            stddev\n            0.7936384596443959\n        \n    \n            \n        \n            \n                \n        \n            maximum\n            4.0\n        \n    \n            \n        \n            \n                \n        \n            minimum\n            1.0\n        \n    \n            \n        \n            \n                \n        \n            valid_percent\n            32.191658745247146\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            proj:geometry\n            \n        \n            \n                \n        \n            type\n            \"Polygon\"\n        \n    \n            \n        \n            \n                \n        coordinates[] 1 items\n        \n            \n        \n            \n                \n        0[] 5 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        1[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        2[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        3[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        4[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            proj:projjson\n            \n        \n            \n                \n        \n            id\n            \n        \n            \n                \n        \n            code\n            4326\n        \n    \n            \n        \n            \n                \n        \n            authority\n            \"EPSG\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            name\n            \"WGS 84\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"GeographicCRS\"\n        \n    \n            \n        \n            \n                \n        \n            datum\n            \n        \n            \n                \n        \n            name\n            \"World Geodetic System 1984\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"GeodeticReferenceFrame\"\n        \n    \n            \n        \n            \n                \n        \n            ellipsoid\n            \n        \n            \n                \n        \n            name\n            \"WGS 84\"\n        \n    \n            \n        \n            \n                \n        \n            semi_major_axis\n            6378137\n        \n    \n            \n        \n            \n                \n        \n            inverse_flattening\n            298.257223563\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            $schema\n            \"https://proj.org/schemas/v0.4/projjson.schema.json\"\n        \n    \n            \n        \n            \n                \n        \n            coordinate_system\n            \n        \n            \n                \n        axis[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Geodetic latitude\"\n        \n    \n            \n        \n            \n                \n        \n            unit\n            \"degree\"\n        \n    \n            \n        \n            \n                \n        \n            direction\n            \"north\"\n        \n    \n            \n        \n            \n                \n        \n            abbreviation\n            \"Lat\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Geodetic longitude\"\n        \n    \n            \n        \n            \n                \n        \n            unit\n            \"degree\"\n        \n    \n            \n        \n            \n                \n        \n            direction\n            \"east\"\n        \n    \n            \n        \n            \n                \n        \n            abbreviation\n            \"Lon\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            subtype\n            \"ellipsoidal\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        proj:transform[] 9 items\n        \n            \n        \n            \n                \n        \n            0\n            0.0003230948999417961\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            -0.00032309489994179427\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            7\n            0.0\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            8\n            1.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"layer\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            rendered_preview\n            \n        \n            \n                \n        \n            href\n            \"/home/jovyan/veda-docs/notebooks/quickstarts/data/bs_to_save/preview.png\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"image/png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Rendered preview\"\n        \n    \n            \n        \n            \n                \n        \n            rel\n            \"preview\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"overview\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        bbox[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -120.61338752166166\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            38.54940283865057\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -119.91905658168675\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            38.90577651328637\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        stac_extensions[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            collection\n            \"caldor-fire-burn-severity\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNote: For downloading just one item use stac_asset.download_item."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/download-assets.html#download-from-jupyterhub",
    "href": "instance-management/notebooks/quickstarts/download-assets.html#download-from-jupyterhub",
    "title": "Download STAC assets",
    "section": "Download from JupyterHub",
    "text": "Download from JupyterHub\nIf you want to further download from this JupyterHub to your local machine you can zip the data directory:\n\n!zip -r data.zip data\n\nupdating: data/ (stored 0%)\nupdating: data/item-collection.json (deflated 74%)\nupdating: data/bs_to_save/ (stored 0%)\nupdating: data/bs_to_save/bs_to_save.tif (deflated 16%)\nupdating: data/bs_to_save/preview.png (deflated 3%)\n\n\nThen right click on the the zipped file in the Jupyter file browser and select “Download”\n\n\n\nRight click on zip file to see options that include “Download”"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#run-this-notebook",
    "title": "Calculate timeseries from COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#approach",
    "title": "Calculate timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items cropped to AOI\nCalculate the mean for each timestep over the AOI\n\n\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport rioxarray  # noqa\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#declare-your-collection-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac/\"\ncollection = \"no2-monthly\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Calculate timeseries from COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nchina_bbox = [\n    73.675,\n    18.198,\n    135.026,\n    53.459,\n]\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(\n    bbox=china_bbox, datetime=datetime, collections=[collection], limit=1000\n)\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 73 items"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#read-data",
    "title": "Calculate timeseries from COGs",
    "section": "Read data",
    "text": "Read data\nRead in data using xarray using a combination of xpystac, stackstac, and rasterio.\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": pd.to_datetime(da.start_datetime)}).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-92e4c5b4a3eab66b40bd5f869280d6b5' (time: 73,\n                                                                y: 1800, x: 3600)&gt; Size: 4GB\ndask.array&lt;getitem, shape=(73, 1800, 3600), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n    id              (time) &lt;U37 11kB 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ...\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 29kB -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 14kB 90.0 89.9 89.8 89.7 ... -89.7 -89.8 -89.9\n    start_datetime  (time) &lt;U25 7kB '2022-01-01T00:00:00+00:00' ... '2016-01-...\n    end_datetime    (time) &lt;U25 7kB '2022-01-31T00:00:00+00:00' ... '2016-01-...\n    ...              ...\n    proj:bbox       object 8B {90.0, 180.0, -90.0, -180.0}\n    proj:geometry   object 8B {'type': 'Polygon', 'coordinates': [[[-180.0, -...\n    title           &lt;U17 68B 'Default COG Layer'\n    proj:transform  object 8B {0.1, 0.0, 1.0, -0.1, -180.0, 90.0}\n    epsg            int64 8B 4326\n  * time            (time) object 584B 1640995200000000000 ... 14516064000000...\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    crs:         epsg:4326\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-92e4c5b4a3eab66b40bd5f869280d6b5'time: 73y: 1800x: 3600dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.52 GiB\n8.00 MiB\n\n\nShape\n(73, 1800, 3600)\n(1, 1024, 1024)\n\n\nDask graph\n584 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                     3600 1800 73\n\n\n\n\nCoordinates: (17)id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])start_datetime(time)&lt;U25'2022-01-01T00:00:00+00:00' ... ...array(['2022-01-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-01-01T00:00:00+00:00'], dtype='&lt;U25')end_datetime(time)&lt;U25'2022-01-31T00:00:00+00:00' ... ...array(['2022-01-31T00:00:00+00:00', '2021-12-31T00:00:00+00:00',\n       '2021-11-30T00:00:00+00:00', '2021-10-31T00:00:00+00:00',\n       '2021-09-30T00:00:00+00:00', '2021-08-31T00:00:00+00:00',\n       '2021-07-31T00:00:00+00:00', '2021-06-30T00:00:00+00:00',\n       '2021-05-31T00:00:00+00:00', '2021-04-30T00:00:00+00:00',\n       '2021-03-31T00:00:00+00:00', '2021-02-28T00:00:00+00:00',\n       '2021-01-31T00:00:00+00:00', '2020-12-31T00:00:00+00:00',\n       '2020-11-30T00:00:00+00:00', '2020-10-31T00:00:00+00:00',\n       '2020-09-30T00:00:00+00:00', '2020-08-31T00:00:00+00:00',\n       '2020-07-31T00:00:00+00:00', '2020-06-30T00:00:00+00:00',\n       '2020-05-31T00:00:00+00:00', '2020-04-30T00:00:00+00:00',\n       '2020-03-31T00:00:00+00:00', '2020-02-29T00:00:00+00:00',\n       '2020-01-31T00:00:00+00:00', '2019-12-31T00:00:00+00:00',\n       '2019-11-30T00:00:00+00:00', '2019-10-31T00:00:00+00:00',\n       '2019-09-30T00:00:00+00:00', '2019-08-31T00:00:00+00:00',\n       '2019-07-31T00:00:00+00:00', '2019-06-30T00:00:00+00:00',\n       '2019-05-31T00:00:00+00:00', '2019-04-30T00:00:00+00:00',\n       '2019-03-31T00:00:00+00:00', '2019-02-28T00:00:00+00:00',\n       '2019-01-31T00:00:00+00:00', '2018-12-31T00:00:00+00:00',\n       '2018-11-30T00:00:00+00:00', '2018-10-31T00:00:00+00:00',\n       '2018-09-30T00:00:00+00:00', '2018-08-31T00:00:00+00:00',\n       '2018-07-31T00:00:00+00:00', '2018-06-30T00:00:00+00:00',\n       '2018-05-31T00:00:00+00:00', '2018-04-30T00:00:00+00:00',\n       '2018-03-31T00:00:00+00:00', '2018-02-28T00:00:00+00:00',\n       '2018-01-31T00:00:00+00:00', '2017-12-31T00:00:00+00:00',\n       '2017-11-30T00:00:00+00:00', '2017-10-31T00:00:00+00:00',\n       '2017-09-30T00:00:00+00:00', '2017-08-31T00:00:00+00:00',\n       '2017-07-31T00:00:00+00:00', '2017-06-30T00:00:00+00:00',\n       '2017-05-31T00:00:00+00:00', '2017-04-30T00:00:00+00:00',\n       '2017-03-31T00:00:00+00:00', '2017-02-28T00:00:00+00:00',\n       '2017-01-31T00:00:00+00:00', '2016-12-31T00:00:00+00:00',\n       '2016-11-30T00:00:00+00:00', '2016-10-31T00:00:00+00:00',\n       '2016-09-30T00:00:00+00:00', '2016-08-31T00:00:00+00:00',\n       '2016-07-31T00:00:00+00:00', '2016-06-30T00:00:00+00:00',\n       '2016-05-31T00:00:00+00:00', '2016-04-30T00:00:00+00:00',\n       '2016-03-31T00:00:00+00:00', '2016-02-29T00:00:00+00:00',\n       '2016-01-31T00:00:00+00:00'], dtype='&lt;U25')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')proj:projjson()object{'id': {'code': 4326, 'authority...array({'id': {'code': 4326, 'authority': 'EPSG'}, 'name': 'WGS 84', 'type': 'GeographicCRS', 'datum': {'name': 'World Geodetic System 1984', 'type': 'GeodeticReferenceFrame', 'ellipsoid': {'name': 'WGS 84', 'semi_major_axis': 6378137, 'inverse_flattening': 298.257223563}}, '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json', 'coordinate_system': {'axis': [{'name': 'Geodetic latitude', 'unit': 'degree', 'direction': 'north', 'abbreviation': 'Lat'}, {'name': 'Geodetic longitude', 'unit': 'degree', 'direction': 'east', 'abbreviation': 'Lon'}], 'subtype': 'ellipsoidal'}},\n      dtype=object)proj:shape()object{1800, 3600}array({1800, 3600}, dtype=object)proj:epsg()int644326array(4326)proj:wkt2()&lt;U277'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984...array('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n      dtype='&lt;U277')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)epsg()int644326array(4326)time(time)object1640995200000000000 ... 14516064...array([1640995200000000000, 1638316800000000000, 1635724800000000000,\n       1633046400000000000, 1630454400000000000, 1627776000000000000,\n       1625097600000000000, 1622505600000000000, 1619827200000000000,\n       1617235200000000000, 1614556800000000000, 1612137600000000000,\n       1609459200000000000, 1606780800000000000, 1604188800000000000,\n       1601510400000000000, 1598918400000000000, 1596240000000000000,\n       1593561600000000000, 1590969600000000000, 1588291200000000000,\n       1585699200000000000, 1583020800000000000, 1580515200000000000,\n       1577836800000000000, 1575158400000000000, 1572566400000000000,\n       1569888000000000000, 1567296000000000000, 1564617600000000000,\n       1561939200000000000, 1559347200000000000, 1556668800000000000,\n       1554076800000000000, 1551398400000000000, 1548979200000000000,\n       1546300800000000000, 1543622400000000000, 1541030400000000000,\n       1538352000000000000, 1535760000000000000, 1533081600000000000,\n       1530403200000000000, 1527811200000000000, 1525132800000000000,\n       1522540800000000000, 1519862400000000000, 1517443200000000000,\n       1514764800000000000, 1512086400000000000, 1509494400000000000,\n       1506816000000000000, 1504224000000000000, 1501545600000000000,\n       1498867200000000000, 1496275200000000000, 1493596800000000000,\n       1491004800000000000, 1488326400000000000, 1485907200000000000,\n       1483228800000000000, 1480550400000000000, 1477958400000000000,\n       1475280000000000000, 1472688000000000000, 1470009600000000000,\n       1467331200000000000, 1464739200000000000, 1462060800000000000,\n       1459468800000000000, 1456790400000000000, 1454284800000000000,\n       1451606400000000000], dtype=object)Indexes: (3)xPandasIndexPandasIndex(Index([            -180.0,             -179.9,             -179.8,\n                   -179.7,             -179.6,             -179.5,\n                   -179.4,             -179.3,             -179.2,\n                   -179.1,\n       ...\n                    179.0, 179.10000000000002, 179.20000000000005,\n                    179.3, 179.40000000000003,              179.5,\n       179.60000000000002, 179.70000000000005,              179.8,\n       179.90000000000003],\n      dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Index([              90.0,               89.9,               89.8,\n                     89.7,               89.6,               89.5,\n                     89.4,               89.3,               89.2,\n                     89.1,\n       ...\n                    -89.0, -89.10000000000002, -89.20000000000002,\n       -89.30000000000001,              -89.4,              -89.5,\n       -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                    -89.9],\n      dtype='float64', name='y', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2022-01-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',\n               '2021-11-01 00:00:00+00:00', '2021-10-01 00:00:00+00:00',\n               '2021-09-01 00:00:00+00:00', '2021-08-01 00:00:00+00:00',\n               '2021-07-01 00:00:00+00:00', '2021-06-01 00:00:00+00:00',\n               '2021-05-01 00:00:00+00:00', '2021-04-01 00:00:00+00:00',\n               '2021-03-01 00:00:00+00:00', '2021-02-01 00:00:00+00:00',\n               '2021-01-01 00:00:00+00:00', '2020-12-01 00:00:00+00:00',\n               '2020-11-01 00:00:00+00:00', '2020-10-01 00:00:00+00:00',\n               '2020-09-01 00:00:00+00:00', '2020-08-01 00:00:00+00:00',\n               '2020-07-01 00:00:00+00:00', '2020-06-01 00:00:00+00:00',\n               '2020-05-01 00:00:00+00:00', '2020-04-01 00:00:00+00:00',\n               '2020-03-01 00:00:00+00:00', '2020-02-01 00:00:00+00:00',\n               '2020-01-01 00:00:00+00:00', '2019-12-01 00:00:00+00:00',\n               '2019-11-01 00:00:00+00:00', '2019-10-01 00:00:00+00:00',\n               '2019-09-01 00:00:00+00:00', '2019-08-01 00:00:00+00:00',\n               '2019-07-01 00:00:00+00:00', '2019-06-01 00:00:00+00:00',\n               '2019-05-01 00:00:00+00:00', '2019-04-01 00:00:00+00:00',\n               '2019-03-01 00:00:00+00:00', '2019-02-01 00:00:00+00:00',\n               '2019-01-01 00:00:00+00:00', '2018-12-01 00:00:00+00:00',\n               '2018-11-01 00:00:00+00:00', '2018-10-01 00:00:00+00:00',\n               '2018-09-01 00:00:00+00:00', '2018-08-01 00:00:00+00:00',\n               '2018-07-01 00:00:00+00:00', '2018-06-01 00:00:00+00:00',\n               '2018-05-01 00:00:00+00:00', '2018-04-01 00:00:00+00:00',\n               '2018-03-01 00:00:00+00:00', '2018-02-01 00:00:00+00:00',\n               '2018-01-01 00:00:00+00:00', '2017-12-01 00:00:00+00:00',\n               '2017-11-01 00:00:00+00:00', '2017-10-01 00:00:00+00:00',\n               '2017-09-01 00:00:00+00:00', '2017-08-01 00:00:00+00:00',\n               '2017-07-01 00:00:00+00:00', '2017-06-01 00:00:00+00:00',\n               '2017-05-01 00:00:00+00:00', '2017-04-01 00:00:00+00:00',\n               '2017-03-01 00:00:00+00:00', '2017-02-01 00:00:00+00:00',\n               '2017-01-01 00:00:00+00:00', '2016-12-01 00:00:00+00:00',\n               '2016-11-01 00:00:00+00:00', '2016-10-01 00:00:00+00:00',\n               '2016-09-01 00:00:00+00:00', '2016-08-01 00:00:00+00:00',\n               '2016-07-01 00:00:00+00:00', '2016-06-01 00:00:00+00:00',\n               '2016-05-01 00:00:00+00:00', '2016-04-01 00:00:00+00:00',\n               '2016-03-01 00:00:00+00:00', '2016-02-01 00:00:00+00:00',\n               '2016-01-01 00:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', name='time', freq=None))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#clip-the-data-to-the-bounding-box-for-china",
    "title": "Calculate timeseries from COGs",
    "section": "Clip the data to the bounding box for China",
    "text": "Clip the data to the bounding box for China\n\n# Subset to Bounding Box for China\nsubset = da.rio.clip_box(*china_bbox)\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-92e4c5b4a3eab66b40bd5f869280d6b5' (time: 73,\n                                                                y: 354, x: 614)&gt; Size: 127MB\ndask.array&lt;getitem, shape=(73, 354, 614), dtype=float64, chunksize=(1, 354, 535), chunktype=numpy.ndarray&gt;\nCoordinates: (12/18)\n    id              (time) &lt;U37 11kB 'OMI_trno2_0.10x0.10_202201_Col3_V4.nc' ...\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 5kB 73.7 73.8 73.9 74.0 ... 134.8 134.9 135.0\n  * y               (y) float64 3kB 53.5 53.4 53.3 53.2 ... 18.5 18.4 18.3 18.2\n    start_datetime  (time) &lt;U25 7kB '2022-01-01T00:00:00+00:00' ... '2016-01-...\n    end_datetime    (time) &lt;U25 7kB '2022-01-31T00:00:00+00:00' ... '2016-01-...\n    ...              ...\n    proj:geometry   object 8B {'type': 'Polygon', 'coordinates': [[[-180.0, -...\n    title           &lt;U17 68B 'Default COG Layer'\n    proj:transform  object 8B {0.1, 0.0, 1.0, -0.1, -180.0, 90.0}\n    epsg            int64 8B 4326\n  * time            (time) object 584B 1640995200000000000 ... 14516064000000...\n    spatial_ref     int64 8B 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    resolution:  0.1xarray.DataArray'stackstac-92e4c5b4a3eab66b40bd5f869280d6b5'time: 73y: 354x: 614dask.array&lt;chunksize=(1, 354, 535), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n121.06 MiB\n1.44 MiB\n\n\nShape\n(73, 354, 614)\n(1, 354, 535)\n\n\nDask graph\n146 chunks in 5 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                               614 354 73\n\n\n\n\nCoordinates: (18)id(time)&lt;U37'OMI_trno2_0.10x0.10_202201_Col3...array(['OMI_trno2_0.10x0.10_202201_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202112_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202111_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202110_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202109_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202108_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202107_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202106_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202105_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202104_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202103_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202102_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202012_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202011_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202010_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202009_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202008_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202007_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202006_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201601_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float6473.7 73.8 73.9 ... 134.9 135.0axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([ 73.7,  73.8,  73.9, ..., 134.8, 134.9, 135. ])y(y)float6453.5 53.4 53.3 ... 18.4 18.3 18.2axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([53.5, 53.4, 53.3, ..., 18.4, 18.3, 18.2])start_datetime(time)&lt;U25'2022-01-01T00:00:00+00:00' ... ...array(['2022-01-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-01-01T00:00:00+00:00'], dtype='&lt;U25')end_datetime(time)&lt;U25'2022-01-31T00:00:00+00:00' ... ...array(['2022-01-31T00:00:00+00:00', '2021-12-31T00:00:00+00:00',\n       '2021-11-30T00:00:00+00:00', '2021-10-31T00:00:00+00:00',\n       '2021-09-30T00:00:00+00:00', '2021-08-31T00:00:00+00:00',\n       '2021-07-31T00:00:00+00:00', '2021-06-30T00:00:00+00:00',\n       '2021-05-31T00:00:00+00:00', '2021-04-30T00:00:00+00:00',\n       '2021-03-31T00:00:00+00:00', '2021-02-28T00:00:00+00:00',\n       '2021-01-31T00:00:00+00:00', '2020-12-31T00:00:00+00:00',\n       '2020-11-30T00:00:00+00:00', '2020-10-31T00:00:00+00:00',\n       '2020-09-30T00:00:00+00:00', '2020-08-31T00:00:00+00:00',\n       '2020-07-31T00:00:00+00:00', '2020-06-30T00:00:00+00:00',\n       '2020-05-31T00:00:00+00:00', '2020-04-30T00:00:00+00:00',\n       '2020-03-31T00:00:00+00:00', '2020-02-29T00:00:00+00:00',\n       '2020-01-31T00:00:00+00:00', '2019-12-31T00:00:00+00:00',\n       '2019-11-30T00:00:00+00:00', '2019-10-31T00:00:00+00:00',\n       '2019-09-30T00:00:00+00:00', '2019-08-31T00:00:00+00:00',\n       '2019-07-31T00:00:00+00:00', '2019-06-30T00:00:00+00:00',\n       '2019-05-31T00:00:00+00:00', '2019-04-30T00:00:00+00:00',\n       '2019-03-31T00:00:00+00:00', '2019-02-28T00:00:00+00:00',\n       '2019-01-31T00:00:00+00:00', '2018-12-31T00:00:00+00:00',\n       '2018-11-30T00:00:00+00:00', '2018-10-31T00:00:00+00:00',\n       '2018-09-30T00:00:00+00:00', '2018-08-31T00:00:00+00:00',\n       '2018-07-31T00:00:00+00:00', '2018-06-30T00:00:00+00:00',\n       '2018-05-31T00:00:00+00:00', '2018-04-30T00:00:00+00:00',\n       '2018-03-31T00:00:00+00:00', '2018-02-28T00:00:00+00:00',\n       '2018-01-31T00:00:00+00:00', '2017-12-31T00:00:00+00:00',\n       '2017-11-30T00:00:00+00:00', '2017-10-31T00:00:00+00:00',\n       '2017-09-30T00:00:00+00:00', '2017-08-31T00:00:00+00:00',\n       '2017-07-31T00:00:00+00:00', '2017-06-30T00:00:00+00:00',\n       '2017-05-31T00:00:00+00:00', '2017-04-30T00:00:00+00:00',\n       '2017-03-31T00:00:00+00:00', '2017-02-28T00:00:00+00:00',\n       '2017-01-31T00:00:00+00:00', '2016-12-31T00:00:00+00:00',\n       '2016-11-30T00:00:00+00:00', '2016-10-31T00:00:00+00:00',\n       '2016-09-30T00:00:00+00:00', '2016-08-31T00:00:00+00:00',\n       '2016-07-31T00:00:00+00:00', '2016-06-30T00:00:00+00:00',\n       '2016-05-31T00:00:00+00:00', '2016-04-30T00:00:00+00:00',\n       '2016-03-31T00:00:00+00:00', '2016-02-29T00:00:00+00:00',\n       '2016-01-31T00:00:00+00:00'], dtype='&lt;U25')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')proj:projjson()object{'id': {'code': 4326, 'authority...array({'id': {'code': 4326, 'authority': 'EPSG'}, 'name': 'WGS 84', 'type': 'GeographicCRS', 'datum': {'name': 'World Geodetic System 1984', 'type': 'GeodeticReferenceFrame', 'ellipsoid': {'name': 'WGS 84', 'semi_major_axis': 6378137, 'inverse_flattening': 298.257223563}}, '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json', 'coordinate_system': {'axis': [{'name': 'Geodetic latitude', 'unit': 'degree', 'direction': 'north', 'abbreviation': 'Lat'}, {'name': 'Geodetic longitude', 'unit': 'degree', 'direction': 'east', 'abbreviation': 'Lon'}], 'subtype': 'ellipsoidal'}},\n      dtype=object)proj:shape()object{1800, 3600}array({1800, 3600}, dtype=object)proj:epsg()int644326array(4326)proj:wkt2()&lt;U277'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984...array('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n      dtype='&lt;U277')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)epsg()int644326array(4326)time(time)object1640995200000000000 ... 14516064...array([1640995200000000000, 1638316800000000000, 1635724800000000000,\n       1633046400000000000, 1630454400000000000, 1627776000000000000,\n       1625097600000000000, 1622505600000000000, 1619827200000000000,\n       1617235200000000000, 1614556800000000000, 1612137600000000000,\n       1609459200000000000, 1606780800000000000, 1604188800000000000,\n       1601510400000000000, 1598918400000000000, 1596240000000000000,\n       1593561600000000000, 1590969600000000000, 1588291200000000000,\n       1585699200000000000, 1583020800000000000, 1580515200000000000,\n       1577836800000000000, 1575158400000000000, 1572566400000000000,\n       1569888000000000000, 1567296000000000000, 1564617600000000000,\n       1561939200000000000, 1559347200000000000, 1556668800000000000,\n       1554076800000000000, 1551398400000000000, 1548979200000000000,\n       1546300800000000000, 1543622400000000000, 1541030400000000000,\n       1538352000000000000, 1535760000000000000, 1533081600000000000,\n       1530403200000000000, 1527811200000000000, 1525132800000000000,\n       1522540800000000000, 1519862400000000000, 1517443200000000000,\n       1514764800000000000, 1512086400000000000, 1509494400000000000,\n       1506816000000000000, 1504224000000000000, 1501545600000000000,\n       1498867200000000000, 1496275200000000000, 1493596800000000000,\n       1491004800000000000, 1488326400000000000, 1485907200000000000,\n       1483228800000000000, 1480550400000000000, 1477958400000000000,\n       1475280000000000000, 1472688000000000000, 1470009600000000000,\n       1467331200000000000, 1464739200000000000, 1462060800000000000,\n       1459468800000000000, 1456790400000000000, 1454284800000000000,\n       1451606400000000000], dtype=object)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :73.65000000000002 0.09999999999999998 0.0 53.55 0.0 -0.09999999999999999array(0)Indexes: (3)xPandasIndexPandasIndex(Index([ 73.70000000000002,  73.80000000000001,               73.9,\n                     74.0,  74.10000000000002,  74.20000000000002,\n        74.30000000000001,               74.4,               74.5,\n        74.60000000000002,\n       ...\n       134.10000000000002, 134.20000000000005,              134.3,\n       134.40000000000003,              134.5, 134.60000000000002,\n       134.70000000000005,              134.8, 134.90000000000003,\n                    135.0],\n      dtype='float64', name='x', length=614))yPandasIndexPandasIndex(Index([              53.5,               53.4,               53.3,\n       53.199999999999996,               53.1,               53.0,\n                     52.9,               52.8, 52.699999999999996,\n                     52.6,\n       ...\n       19.099999999999994,               19.0,  18.89999999999999,\n       18.799999999999997, 18.700000000000003, 18.599999999999994,\n                     18.5,  18.39999999999999, 18.299999999999997,\n       18.200000000000003],\n      dtype='float64', name='y', length=354))timePandasIndexPandasIndex(DatetimeIndex(['2022-01-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',\n               '2021-11-01 00:00:00+00:00', '2021-10-01 00:00:00+00:00',\n               '2021-09-01 00:00:00+00:00', '2021-08-01 00:00:00+00:00',\n               '2021-07-01 00:00:00+00:00', '2021-06-01 00:00:00+00:00',\n               '2021-05-01 00:00:00+00:00', '2021-04-01 00:00:00+00:00',\n               '2021-03-01 00:00:00+00:00', '2021-02-01 00:00:00+00:00',\n               '2021-01-01 00:00:00+00:00', '2020-12-01 00:00:00+00:00',\n               '2020-11-01 00:00:00+00:00', '2020-10-01 00:00:00+00:00',\n               '2020-09-01 00:00:00+00:00', '2020-08-01 00:00:00+00:00',\n               '2020-07-01 00:00:00+00:00', '2020-06-01 00:00:00+00:00',\n               '2020-05-01 00:00:00+00:00', '2020-04-01 00:00:00+00:00',\n               '2020-03-01 00:00:00+00:00', '2020-02-01 00:00:00+00:00',\n               '2020-01-01 00:00:00+00:00', '2019-12-01 00:00:00+00:00',\n               '2019-11-01 00:00:00+00:00', '2019-10-01 00:00:00+00:00',\n               '2019-09-01 00:00:00+00:00', '2019-08-01 00:00:00+00:00',\n               '2019-07-01 00:00:00+00:00', '2019-06-01 00:00:00+00:00',\n               '2019-05-01 00:00:00+00:00', '2019-04-01 00:00:00+00:00',\n               '2019-03-01 00:00:00+00:00', '2019-02-01 00:00:00+00:00',\n               '2019-01-01 00:00:00+00:00', '2018-12-01 00:00:00+00:00',\n               '2018-11-01 00:00:00+00:00', '2018-10-01 00:00:00+00:00',\n               '2018-09-01 00:00:00+00:00', '2018-08-01 00:00:00+00:00',\n               '2018-07-01 00:00:00+00:00', '2018-06-01 00:00:00+00:00',\n               '2018-05-01 00:00:00+00:00', '2018-04-01 00:00:00+00:00',\n               '2018-03-01 00:00:00+00:00', '2018-02-01 00:00:00+00:00',\n               '2018-01-01 00:00:00+00:00', '2017-12-01 00:00:00+00:00',\n               '2017-11-01 00:00:00+00:00', '2017-10-01 00:00:00+00:00',\n               '2017-09-01 00:00:00+00:00', '2017-08-01 00:00:00+00:00',\n               '2017-07-01 00:00:00+00:00', '2017-06-01 00:00:00+00:00',\n               '2017-05-01 00:00:00+00:00', '2017-04-01 00:00:00+00:00',\n               '2017-03-01 00:00:00+00:00', '2017-02-01 00:00:00+00:00',\n               '2017-01-01 00:00:00+00:00', '2016-12-01 00:00:00+00:00',\n               '2016-11-01 00:00:00+00:00', '2016-10-01 00:00:00+00:00',\n               '2016-09-01 00:00:00+00:00', '2016-08-01 00:00:00+00:00',\n               '2016-07-01 00:00:00+00:00', '2016-06-01 00:00:00+00:00',\n               '2016-05-01 00:00:00+00:00', '2016-04-01 00:00:00+00:00',\n               '2016-03-01 00:00:00+00:00', '2016-02-01 00:00:00+00:00',\n               '2016-01-01 00:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', name='time', freq=None))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "href": "instance-management/notebooks/quickstarts/timeseries-rioxarray-stackstac.html#aggregate-the-data",
    "title": "Calculate timeseries from COGs",
    "section": "Aggregate the data",
    "text": "Aggregate the data\nCalculate the mean at each time across regional data. Note this is the first time that the data is actually loaded.\n\nmeans = subset.mean(dim=(\"x\", \"y\")).compute()\n\nPlot the mean monthly NO2 using hvplot\n\nmeans.hvplot.line(x=\"time\", ylabel=\"NO2\", title=\"Mean Monthly NO2 in China\")"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#run-this-notebook",
    "title": "Downsample zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#approach",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#approach",
    "title": "Downsample zarr",
    "section": "Approach",
    "text": "Approach\nThis notebook demonstrates 2 strategies for resampling data from a Zarr dataset in order to visualize within the memory limits of a notebook.\n\nDownsample the temporal resolution of the data using xarray.DataArray.resample\nCoarsening the spatial resolution of the data using xarray.DataArray.coarsen\n\nA strategy for visualizing any large amount of data is Datashader which bins data into a fixed 2-D array. Using the rasterize argument within hvplot calls ensures the use of the datashader library to bin the data. Optionally an external Dask cluster is used to parallelize and distribute these large downsampling operations across compute nodes."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#about-the-data",
    "title": "Downsample zarr",
    "section": "About the data",
    "text": "About the data\nThe SMAP mission is an orbiting observatory that measures the amount of water in the surface soil everywhere on Earth."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#load-libraries",
    "title": "Downsample zarr",
    "section": "Load libraries",
    "text": "Load libraries\n\nimport xarray as xr\nimport hvplot.xarray\nimport cartopy.crs as ccrs"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#optional-create-and-scale-a-dask-cluster",
    "title": "Downsample zarr",
    "section": "Optional: Create and Scale a Dask Cluster",
    "text": "Optional: Create and Scale a Dask Cluster\nWe create a separate Dask cluster to speed up reprojecting the data (and other potential computations which could be required and are parallelizable).\nNote if you skip this cell you will still be using Dask, you’ll just be using the machine where you are running this notebook.\n\nfrom dask_gateway import GatewayCluster, Gateway\n\ngateway = Gateway()\nclusters = gateway.list_clusters()\n\n# connect to an existing cluster - this is useful when the kernel shutdown in the middle of an interactive session\nif clusters:\n    cluster = gateway.connect(clusters[0].name)\nelse:\n    cluster = GatewayCluster(shutdown_on_close=True)\n\ncluster.scale(16)\nclient = cluster.get_client()\nclient\n\n\n     \n    \n        Client\n        Client-f633df20-5c0c-11ef-92ac-860c0d05aa65\n        \n\n\n\nConnection method: Cluster object\nCluster type: dask_gateway.GatewayCluster\n\n\nDashboard: /services/dask-gateway/clusters/prod.f63d365675734e2ba2dee10ae8769a7c/status\n\n\n\n\n\n\n        \n            \n                Launch dashboard in JupyterLab\n            \n        \n\n        \n            \n            Cluster Info\n            \n  GatewayCluster\n  \n    Name: prod.f63d365675734e2ba2dee10ae8769a7c\n    Dashboard: /services/dask-gateway/clusters/prod.f63d365675734e2ba2dee10ae8769a7c/status"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#open-the-dataset-from-s3",
    "title": "Downsample zarr",
    "section": "Open the dataset from S3",
    "text": "Open the dataset from S3\n\nds = xr.open_zarr(\"s3:///veda-data-store-staging/EIS/zarr/SPL3SMP.zarr\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 68GB\nDimensions:                        (northing_m: 406, easting_m: 964,\n                                    datetime: 1679)\nCoordinates:\n  * datetime                       (datetime) datetime64[ns] 13kB 2018-01-01 ...\n  * easting_m                      (easting_m) float64 8kB -1.735e+07 ... 1.7...\n  * northing_m                     (northing_m) float64 3kB 7.297e+06 ... -7....\nData variables: (12/26)\n    albedo                         (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    albedo_pm                      (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density                   (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    bulk_density_pm                (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction                  (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    clay_fraction_pm               (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    ...                             ...\n    static_water_body_fraction     (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    static_water_body_fraction_pm  (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag                   (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_flag_pm                (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature            (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    surface_temperature_pm         (northing_m, easting_m, datetime) float32 3GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;xarray.DatasetDimensions:northing_m: 406easting_m: 964datetime: 1679Coordinates: (3)datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')easting_m(easting_m)float64-1.735e+07 -1.731e+07 ... 1.735e+07array([-17349514.34, -17313482.12, -17277449.9 , ...,  17277449.08,\n        17313481.3 ,  17349513.52])northing_m(northing_m)float647.297e+06 7.26e+06 ... -7.297e+06array([ 7296524.72,  7260492.5 ,  7224460.28, ..., -7224459.94, -7260492.16,\n       -7296524.38])Data variables: (26)albedo(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Diffuse reflecting power of the Earth&apos;s surface used in DCA within the grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nalbedo_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Diffuse reflecting power of the Earth&apos;s surface used in DCA retrievals within the grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nbulk_density(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of aggregated bulk_density within the 36 km grid cell.valid_max :2.6500000953674316valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nbulk_density_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of aggregated bulk density within the 36 km grid cell.valid_max :2.6500000953674316valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nclay_fraction(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nclay_fraction_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of aggregated clay fraction within the 36 km grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nfreeze_thaw_fraction(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Fraction of the 36 km grid cell that is denoted as frozen.  Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nfreeze_thaw_fraction_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Fraction of the 36 km grid cell that is denoted as frozen.  Based on binary flag that specifies freeze thaw conditions in each of the component 3 km grid cells.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\ngrid_surface_status(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Indicates if the grid point lies on land (0) or water (1).valid_max :1valid_min :0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\ngrid_surface_status_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Indicates if the grid point lies on land (0) or water (1).valid_max :1valid_min :0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nradar_water_body_fraction(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :The fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nradar_water_body_fraction_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :The fraction of the area of the 36 km grid cell that is covered by water based on the radar detection algorithm.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nretrieval_qual_flag(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;flag_masks :1s, 2s, 4s, 8sflag_meanings :Retrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_successlong_name :Bit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nretrieval_qual_flag_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;flag_masks :1s, 2s, 4s, 8sflag_meanings :Retrieval_recommended Retrieval_attempted Retrieval_success FT_retrieval_successlong_name :Bit flags that record the conditions and the quality of the DCA retrieval algorithms that generate soil moisture for the grid cell.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nroughness_coefficient(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of bare soil roughness used in DCA within the 36 km grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nroughness_coefficient_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :A unitless value that is indicative of bare soil roughness used in DCA retrievals within the 36 km grid cell.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsoil_moisture(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsoil_moisture_error(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Net uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.units :cm**3/cm**3valid_max :0.20000000298023224valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsoil_moisture_error_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Net uncertainty measure of soil moisture measure for the Earth based grid cell. - Calculation method is TBD.units :cm**3/cm**3valid_max :0.20000000298023224valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsoil_moisture_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nstatic_water_body_fraction(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :The fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nstatic_water_body_fraction_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :The fraction of the area of the 36 km grid cell that is covered by static water based on a Digital Elevation Map.valid_max :1.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsurface_flag(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;flag_masks :1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048sflag_meanings :36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radiometer_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_regionlong_name :Bit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsurface_flag_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;flag_masks :1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s, 512s, 1024s, 2048sflag_meanings :36_km_static_water_body 36_km_radar_water_body_detection 36_km_coastal_proximity 36_km_urban_area 36_km_precipitation 36_km_snow_or_ice 36_km_permanent_snow_or_ice 36_km_radar_frozen_ground 36_km_model_frozen_ground 36_km_mountainous_terrain 36_km_dense_vegetation 36_km_nadir_regionlong_name :Bit flags that record ambient surface conditions for the grid cell\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsurface_temperature(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Temperature at land surface based on GMAO GEOS-5 data.units :Kelvinsvalid_max :350.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nsurface_temperature_pm(northing_m, easting_m, datetime)float32dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Temperature at land surface based on GMAO GEOS-5 data.units :Kelvinsvalid_max :350.0valid_min :0.0\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nIndexes: (3)datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n               '2018-01-09', '2018-01-10',\n               ...\n               '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03',\n               '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07',\n               '2022-09-08', '2022-09-09'],\n              dtype='datetime64[ns]', name='datetime', length=1679, freq=None))easting_mPandasIndexPandasIndex(Index([      -17349514.34,       -17313482.12,        -17277449.9,\n             -17241417.68,       -17205385.46,       -17169353.24,\n             -17133321.02,        -17097288.8,       -17061256.58,\n             -17025224.36,\n       ...\n       17025223.540000003,        17061255.76,        17097287.98,\n               17133320.2, 17169352.419999998, 17205384.640000004,\n       17241416.860000003, 17277449.080000002,         17313481.3,\n              17349513.52],\n      dtype='float64', name='easting_m', length=964))northing_mPandasIndexPandasIndex(Index([        7296524.72,          7260492.5,  7224460.279999999,\n               7188428.06,         7152395.84,         7116363.62,\n        7080331.399999999,         7044299.18,         7008266.96,\n               6972234.74,\n       ...\n       -6972234.400000001,        -7008266.62, -7044298.840000001,\n       -7080331.060000001,        -7116363.28, -7152395.500000001,\n       -7188427.720000002,        -7224459.94, -7260492.160000001,\n              -7296524.38],\n      dtype='float64', name='northing_m', length=406))Attributes: (0)\n\n\nSelect the variable of interest (soil moisture for this example).\n\nsoil_moisture = ds.soil_moisture\nsoil_moisture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (northing_m: 406, easting_m: 964,\n                                   datetime: 1679)&gt; Size: 3GB\ndask.array&lt;open_dataset-soil_moisture, shape=(406, 964, 1679), dtype=float32, chunksize=(100, 100, 100), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * datetime    (datetime) datetime64[ns] 13kB 2018-01-01 ... 2022-09-09\n  * easting_m   (easting_m) float64 8kB -1.735e+07 -1.731e+07 ... 1.735e+07\n  * northing_m  (northing_m) float64 3kB 7.297e+06 7.26e+06 ... -7.297e+06\nAttributes:\n    long_name:  Representative DCA soil moisture measurement for the Earth ba...\n    units:      cm**3/cm**3\n    valid_max:  0.5\n    valid_min:  0.019999999552965164xarray.DataArray'soil_moisture'northing_m: 406easting_m: 964datetime: 1679dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.45 GiB\n3.81 MiB\n\n\nShape\n(406, 964, 1679)\n(100, 100, 100)\n\n\nDask graph\n850 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                   1679 964 406\n\n\n\n\nCoordinates: (3)datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')easting_m(easting_m)float64-1.735e+07 -1.731e+07 ... 1.735e+07array([-17349514.34, -17313482.12, -17277449.9 , ...,  17277449.08,\n        17313481.3 ,  17349513.52])northing_m(northing_m)float647.297e+06 7.26e+06 ... -7.297e+06array([ 7296524.72,  7260492.5 ,  7224460.28, ..., -7224459.94, -7260492.16,\n       -7296524.38])Indexes: (3)datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n               '2018-01-09', '2018-01-10',\n               ...\n               '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03',\n               '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07',\n               '2022-09-08', '2022-09-09'],\n              dtype='datetime64[ns]', name='datetime', length=1679, freq=None))easting_mPandasIndexPandasIndex(Index([      -17349514.34,       -17313482.12,        -17277449.9,\n             -17241417.68,       -17205385.46,       -17169353.24,\n             -17133321.02,        -17097288.8,       -17061256.58,\n             -17025224.36,\n       ...\n       17025223.540000003,        17061255.76,        17097287.98,\n               17133320.2, 17169352.419999998, 17205384.640000004,\n       17241416.860000003, 17277449.080000002,         17313481.3,\n              17349513.52],\n      dtype='float64', name='easting_m', length=964))northing_mPandasIndexPandasIndex(Index([        7296524.72,          7260492.5,  7224460.279999999,\n               7188428.06,         7152395.84,         7116363.62,\n        7080331.399999999,         7044299.18,         7008266.96,\n               6972234.74,\n       ...\n       -6972234.400000001,        -7008266.62, -7044298.840000001,\n       -7080331.060000001,        -7116363.28, -7152395.500000001,\n       -7188427.720000002,        -7224459.94, -7260492.160000001,\n              -7296524.38],\n      dtype='float64', name='northing_m', length=406))Attributes: (4)long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#strategy-1-downsample-the-temporal-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 1: Downsample the temporal resolution of the data",
    "text": "Strategy 1: Downsample the temporal resolution of the data\nTo plot one day from every month, resample the data to 1 observation a month.\n\nsomo_one_month = soil_moisture.resample(datetime=\"1ME\").nearest()\n\n\nQuick plot\nWe can generate a quick plot using hvplot and datashader.\n\n# workaround to avoid warnings that are triggered within Dask.\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"All-NaN slice encountered\", category=RuntimeWarning\n)\n\n\nsomo_one_month.hvplot(\n    x=\"easting_m\",\n    y=\"northing_m\",\n    groupby=\"datetime\",\n    crs=ccrs.epsg(6933),  # this is a workaround for https://github.com/holoviz/hvplot/issues/1329\n    rasterize=True,\n    coastline=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nReproject before plotting\nReproject the data for map visualization.\n\nsomo_one_month = somo_one_month.transpose(\"datetime\", \"northing_m\", \"easting_m\")\nsomo_one_month = somo_one_month.rio.set_spatial_dims(\n    x_dim=\"easting_m\", y_dim=\"northing_m\"\n)\nsomo_one_month = somo_one_month.rio.write_crs(\"EPSG:6933\")\nsomo_reprojected = somo_one_month.rio.reproject(\"EPSG:4326\")\nsomo_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 57, y: 1046, x: 2214)&gt; Size: 528MB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 18kB -179.9 -179.8 -179.6 ... 179.6 179.8 179.9\n  * y            (y) float64 8kB 84.96 84.8 84.64 84.48 ... -84.64 -84.8 -84.96\n  * datetime     (datetime) datetime64[ns] 456B 2018-01-31 ... 2022-09-30\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:  Representative DCA soil moisture measurement for the Earth ba...\n    units:      cm**3/cm**3\n    valid_max:  0.5\n    valid_min:  0.019999999552965164xarray.DataArray'soil_moisture'datetime: 57y: 1046x: 2214nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.9 -179.8 ... 179.8 179.9axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.918425, -179.755825, -179.593224, ...,  179.591393,  179.753993,\n        179.916594])y(y)float6484.96 84.8 84.64 ... -84.8 -84.96axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.963262,  84.800655,  84.638047, ..., -84.636773, -84.79938 ,\n       -84.961988])datetime(datetime)datetime64[ns]2018-01-31 ... 2022-09-30array(['2018-01-31T00:00:00.000000000', '2018-02-28T00:00:00.000000000',\n       '2018-03-31T00:00:00.000000000', '2018-04-30T00:00:00.000000000',\n       '2018-05-31T00:00:00.000000000', '2018-06-30T00:00:00.000000000',\n       '2018-07-31T00:00:00.000000000', '2018-08-31T00:00:00.000000000',\n       '2018-09-30T00:00:00.000000000', '2018-10-31T00:00:00.000000000',\n       '2018-11-30T00:00:00.000000000', '2018-12-31T00:00:00.000000000',\n       '2019-01-31T00:00:00.000000000', '2019-02-28T00:00:00.000000000',\n       '2019-03-31T00:00:00.000000000', '2019-04-30T00:00:00.000000000',\n       '2019-05-31T00:00:00.000000000', '2019-06-30T00:00:00.000000000',\n       '2019-07-31T00:00:00.000000000', '2019-08-31T00:00:00.000000000',\n       '2019-09-30T00:00:00.000000000', '2019-10-31T00:00:00.000000000',\n       '2019-11-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000',\n       '2020-01-31T00:00:00.000000000', '2020-02-29T00:00:00.000000000',\n       '2020-03-31T00:00:00.000000000', '2020-04-30T00:00:00.000000000',\n       '2020-05-31T00:00:00.000000000', '2020-06-30T00:00:00.000000000',\n       '2020-07-31T00:00:00.000000000', '2020-08-31T00:00:00.000000000',\n       '2020-09-30T00:00:00.000000000', '2020-10-31T00:00:00.000000000',\n       '2020-11-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000',\n       '2021-01-31T00:00:00.000000000', '2021-02-28T00:00:00.000000000',\n       '2021-03-31T00:00:00.000000000', '2021-04-30T00:00:00.000000000',\n       '2021-05-31T00:00:00.000000000', '2021-06-30T00:00:00.000000000',\n       '2021-07-31T00:00:00.000000000', '2021-08-31T00:00:00.000000000',\n       '2021-09-30T00:00:00.000000000', '2021-10-31T00:00:00.000000000',\n       '2021-11-30T00:00:00.000000000', '2021-12-31T00:00:00.000000000',\n       '2022-01-31T00:00:00.000000000', '2022-02-28T00:00:00.000000000',\n       '2022-03-31T00:00:00.000000000', '2022-04-30T00:00:00.000000000',\n       '2022-05-31T00:00:00.000000000', '2022-06-30T00:00:00.000000000',\n       '2022-07-31T00:00:00.000000000', '2022-08-31T00:00:00.000000000',\n       '2022-09-30T00:00:00.000000000'], dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.1626005508861423 0.0 85.04456635019852 0.0 -0.16260789541619725array(0)Indexes: (3)xPandasIndexPandasIndex(Index([-179.91842511650856, -179.75582456562242, -179.59322401473628,\n       -179.43062346385014,   -179.268022912964, -179.10542236207786,\n        -178.9428218111917, -178.78022126030555,  -178.6176207094194,\n       -178.45502015853327,\n       ...\n        178.45318903654908,  178.61578958743524,  178.77839013832136,\n        178.94099068920752,  179.10359124009364,   179.2661917909798,\n        179.42879234186597,  179.59139289275208,  179.75399344363825,\n        179.91659399452436],\n      dtype='float64', name='x', length=2214))yPandasIndexPandasIndex(Index([ 84.96326240249043,  84.80065450707423,  84.63804661165804,\n        84.47543871624184,  84.31283082082564,  84.15022292540944,\n        83.98761502999325,  83.82500713457705,  83.66239923916085,\n        83.49979134374465,\n       ...\n       -83.49851724868991, -83.66112514410612, -83.82373303952231,\n        -83.9863409349385, -84.14894883035471,  -84.3115567257709,\n       -84.47416462118711,  -84.6367725166033, -84.79938041201949,\n        -84.9619883074357],\n      dtype='float64', name='y', length=1046))datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n               '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31',\n               '2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30',\n               '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n               '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31',\n               '2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30',\n               '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n               '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31',\n               '2021-01-31', '2021-02-28', '2021-03-31', '2021-04-30',\n               '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31',\n               '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31',\n               '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30',\n               '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31',\n               '2022-09-30'],\n              dtype='datetime64[ns]', name='datetime', freq=None))Attributes: (4)long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164\n\n\n\n[!NOTE] This is now a fully materialized data array - when we reprojected we triggered an implicit compute.\n\n\nsomo_reprojected.hvplot(\n    x=\"x\",\n    y=\"y\",\n    groupby=\"datetime\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#strategy-2-coarsen-spatial-resolution-of-the-data",
    "title": "Downsample zarr",
    "section": "Strategy 2: Coarsen spatial resolution of the data",
    "text": "Strategy 2: Coarsen spatial resolution of the data\nBelow, we coarsen the spatial resolution of the data by a factor of 4 in the x and 2 in the y. These values were chosen because they can be used with the exact boundary argument as the dimensions size is a multiple of these values.\nYou can also coarsen by datetime, using the same strategy as below but replacing easting_m and northing_m with datetime. If {datetime: n} is the value given to the dim argument, this would create a mean of the soil moisture average for n days.\nOnce the data has been coarsened, again it is reprojected for map visualization and then visualized.\n\ncoarsened = soil_moisture.coarsen(dim={\"easting_m\": 4, \"northing_m\": 2}).mean()\n\ncoarsened = coarsened.transpose(\"datetime\", \"northing_m\", \"easting_m\")\ncoarsened = coarsened.rio.set_spatial_dims(x_dim=\"easting_m\", y_dim=\"northing_m\")\ncoarsened = coarsened.rio.write_crs(\"epsg:6933\")\ncoarsened_reprojected = coarsened.rio.reproject(\"EPSG:4326\")\ncoarsened_reprojected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'soil_moisture' (datetime: 1679, y: 315, x: 667)&gt; Size: 1GB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 5kB -179.7 -179.2 -178.7 ... 178.6 179.2 179.7\n  * y            (y) float64 3kB 84.77 84.23 83.7 83.16 ... -83.64 -84.18 -84.72\n  * datetime     (datetime) datetime64[ns] 13kB 2018-01-01 ... 2022-09-09\n    spatial_ref  int64 8B 0\nAttributes:\n    long_name:   Representative DCA soil moisture measurement for the Earth b...\n    units:       cm**3/cm**3\n    valid_max:   0.5\n    valid_min:   0.019999999552965164\n    _FillValue:  3.402823466e+38xarray.DataArray'soil_moisture'datetime: 1679y: 315x: 667nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float64-179.7 -179.2 ... 179.2 179.7axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-179.729872, -179.190164, -178.650456, ...,  178.636044,  179.175751,\n        179.715459])y(y)float6484.77 84.23 83.7 ... -84.18 -84.72axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([ 84.774672,  84.234883,  83.695095, ..., -83.639381, -84.17917 ,\n       -84.718958])datetime(datetime)datetime64[ns]2018-01-01 ... 2022-09-09array(['2018-01-01T00:00:00.000000000', '2018-01-02T00:00:00.000000000',\n       '2018-01-03T00:00:00.000000000', ..., '2022-09-07T00:00:00.000000000',\n       '2022-09-08T00:00:00.000000000', '2022-09-09T00:00:00.000000000'],\n      dtype='datetime64[ns]')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-179.99972539195164 0.5397077035841558 0.0 85.04456635019838 0.0 -0.5397886314149526array(0)Indexes: (3)xPandasIndexPandasIndex(Index([-179.72987154015956,  -179.1901638365754, -178.65045613299125,\n        -178.1107484294071, -177.57104072582294, -177.03133302223878,\n       -176.49162531865463, -175.95191761507047, -175.41220991148631,\n       -174.87250220790216,\n       ...\n        174.85808971463078,  175.39779741821494,   175.9375051217991,\n        176.47721282538325,   177.0169205289674,  177.55662823255156,\n        178.09633593613572,  178.63604363971987,  179.17575134330403,\n        179.71545904688818],\n      dtype='float64', name='x', length=667))yPandasIndexPandasIndex(Index([  84.7746720344909,  84.23488340307595,    83.695094771661,\n        83.15530614024604,  82.61551750883109,  82.07572887741614,\n        81.53594024600119,  80.99615161458624,  80.45636298317129,\n        79.91657435175634,\n       ...\n       -79.86086054706963, -80.40064917848458, -80.94043780989954,\n       -81.48022644131449, -82.02001507272944, -82.55980370414439,\n       -83.09959233555936, -83.63938096697431, -84.17916959838927,\n       -84.71895822980422],\n      dtype='float64', name='y', length=315))datetimePandasIndexPandasIndex(DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08',\n               '2018-01-09', '2018-01-10',\n               ...\n               '2022-08-31', '2022-09-01', '2022-09-02', '2022-09-03',\n               '2022-09-04', '2022-09-05', '2022-09-06', '2022-09-07',\n               '2022-09-08', '2022-09-09'],\n              dtype='datetime64[ns]', name='datetime', length=1679, freq=None))Attributes: (5)long_name :Representative DCA soil moisture measurement for the Earth based grid cell.units :cm**3/cm**3valid_max :0.5valid_min :0.019999999552965164_FillValue :3.402823466e+38\n\n\n\ncoarsened_reprojected.hvplot(\n    x=\"x\",\n    y=\"y\",\n    groupby=\"datetime\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    frame_height=400,\n    widget_location=\"bottom\",\n)"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/downsample-zarr.html#cleanup",
    "href": "instance-management/notebooks/quickstarts/downsample-zarr.html#cleanup",
    "title": "Downsample zarr",
    "section": "Cleanup",
    "text": "Cleanup\nWhen using a remote Dask cluster it is recommented to explicitly close the cluster.\nclient.shutdown()"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#run-this-notebook",
    "title": "Get tiles from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#approach",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#approach",
    "title": "Get tiles from COGs",
    "section": "Approach",
    "text": "Approach\n\nIdentify available dates within a bounding box, which is also an area of interest (AOI) in this example, for a given collection\nRegister a dynamic tiler search for an AOI and specific date range for a given collection\nExplore different options for displaying multi-band Harmonized Landsat and Sentinel (HLS) assets with the Raster API."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#about-the-data",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#about-the-data",
    "title": "Get tiles from COGs",
    "section": "About the Data",
    "text": "About the Data\nA small subset of HLS data has been ingested to the VEDA datastore to visually explore data using the Raster API, which is a VEDA instance of (pgstac-titiler). This limited subset includes a two granules for dates before and after Hurricane Maria in 2017 and Hurricane Ida in 2021.\nNote about HLS datasets: The Sentinel and Landsat assets have been “harmonized” in the sense that these products have been generated to use the same spatial resolution and grid system. Thus these 2 HLS S30 and L30 productscan be used interchangeably in algorithms. However, the individual band assets are specific to each provider. This notebook focuses on displaying HLS data with a dynamic tiler so separate examples are provided for rendering the unique band assets of each collection.\nAdditional Resources\n\nHLSL30 Dataset Landing Page\nLandsat 8 Bands and Combinations Blog\nHLSS30 Dataset Landing Page\nSentinel 2 Bands and Combinations Blog\nCQL2 STAC-API Examples\n\n\nimport json\nimport requests\n\nfrom folium import Map, TileLayer"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#parameters-for-investigating-hurricane-events-with-the-dynamic-tiler-and-custom-band-combinations",
    "title": "Get tiles from COGs",
    "section": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations",
    "text": "Parameters for investigating hurricane events with the dynamic tiler and custom band combinations\nIn this notebook we will focus on HLS S30 data for Hurricane Ida, but Hurricane Maria and L30 parameters are provided below for further exploration.\n\n# Endpoints\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# Harmonized Sentinel collection id and configuration info\ns30_collection_id = \"hls-s30-002-ej-reprocessed\"\ns30_swir_assets = [\"B12\", \"B8A\", \"B04\"]\ns30_vegetation_index_assets = [\"B08\", \"B04\"]\ns30_vegetation_index_expression = \"(B08_b1-B04_b1)/(B08_b1+B04_b1)\"\ns30_vegetation_index_rescaling = \"0,1\"\ns30_vegetation_index_colormap = \"rdylgn\"\n\n# Harmonized Landsat collection id and map configuration info\nl30_collection_id = \"hls-l30-002-ej-reprocessed\"\nl30_swir_assets = [\"B07\", \"B05\", \"B04\"]\nl30_ndwi_expression = \"(B03_b1-B05_b1)/(B03_b1+B05_b1)\"\nl30_ndwi_assets = [\"B03\", \"B05\"]\nl30_ndwi_rescaling = \"0,1\"\nl30_ndwi_colormap = \"spectral\"\n\n# Search criteria for events in both HLS Events collections\nmaria_bbox = [-66.167596, 17.961538, -65.110098, 18.96772]\nmaria_temporal_range = [\"2017-06-06T00:00:00Z\", \"2017-11-30T00:00:00Z\"]\n\nida_bbox = [-90.932637, 29.705366, -89.766437, 30.71627]\nida_temporal_range = [\"2021-07-01T00:00:00Z\", \"2021-10-28T00:00:00Z\"]\n\n\nFirst, search the STAC API to find the specific dates available within timeframe of interest (Hurricane Ida)\nTo focus on a specific point in time, we will restrict the temporal range when defining the item search in the example below.\n\ncollections_filter = {\n    \"op\": \"=\",\n    \"args\": [{\"property\": \"collection\"}, s30_collection_id],\n}\n\nspatial_filter = {\"op\": \"s_intersects\", \"args\": [{\"property\": \"bbox\"}, ida_bbox]}\n\ntemporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [{\"property\": \"datetime\"}, {\"interval\": ida_temporal_range}],\n}\n\n# Additional filters can be applied for other search criteria like &lt;= maximum eo:cloud_cover in item properties\ncloud_filter = {\"op\": \"&lt;=\", \"args\": [{\"property\": \"eo:cloud_cover\"}, 80]}\n\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"limit\": 100,\n    \"sortby\": [{\"direction\": \"asc\", \"field\": \"properties.datetime\"}],\n    \"context\": \"on\",  # add context for a summary of matched results\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, temporal_filter, cloud_filter],\n    },\n}\n\n# Note this search body can also be used for a stac item search\nstac_items_response = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json=search_body,\n).json()\n\n# Check how many items were matched in search\nprint(\"search context:\", stac_items_response[\"context\"])\n\n# Iterate over search results to get an array of item datetimes\n[item[\"properties\"][\"datetime\"] for item in stac_items_response[\"features\"]]\n\nsearch context: {'limit': 100, 'matched': 14, 'returned': 14}\n\n\n['2021-07-14T16:55:15.122720+00:00',\n '2021-07-24T16:55:15.112940+00:00',\n '2021-07-29T16:55:16.405890+00:00',\n '2021-08-08T16:55:15.798510+00:00',\n '2021-08-13T16:55:13.394950+00:00',\n '2021-08-23T16:55:11.785040+00:00',\n '2021-09-02T16:55:09.568600+00:00',\n '2021-09-07T16:55:13.430530+00:00',\n '2021-09-22T16:55:10.763010+00:00',\n '2021-09-27T16:55:17.027350+00:00',\n '2021-10-07T16:55:18.213640+00:00',\n '2021-10-12T16:55:14.209080+00:00',\n '2021-10-17T16:55:18.517600+00:00',\n '2021-10-22T16:55:14.670710+00:00']"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#visualizing-the-data-on-a-map",
    "title": "Get tiles from COGs",
    "section": "Visualizing the data on a map",
    "text": "Visualizing the data on a map\nThe VEDA backend is based on eoAPI, an application for searching and tiling earth observation STAC records. The application uses titiler-pgstac for dynamically mosaicing cloud optimized data from a registerd STAC API search.\nTo use the dynamic tiler, register a STAC item search and then use the registered search ID to dynamically mosaic the search results on the map.\n\nUpdate the temporal range in search body and register that search with the Raster API\nThe registered search id can be reused for alternate map layer visualizations.\n\n# Restricted date range\nrestricted_temporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        {\"property\": \"datetime\"},\n        {\"interval\": [\"2021-10-16T00:00:00Z\", \"2021-10-18T00:00:00Z\"]},\n    ],\n}\n\n# Specify cql2-json filter language in search body\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [collections_filter, spatial_filter, restricted_temporal_filter],\n    },\n}\n\nmosaic_response = requests.post(\n    f\"{RASTER_API_URL}/searches/register\",\n    json=search_body,\n).json()\nprint(json.dumps(mosaic_response, indent=2))\n\n{\n  \"id\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"links\": [\n    {\n      \"href\": \"https://openveda.cloud/api/raster/searches/7743bcb31bff7151aff7e5508785fce1/info\",\n      \"rel\": \"metadata\",\n      \"title\": \"Mosaic metadata\"\n    },\n    {\n      \"href\": \"https://openveda.cloud/api/raster/searches/7743bcb31bff7151aff7e5508785fce1/{tileMatrixSetId}/tilejson.json\",\n      \"rel\": \"tilejson\",\n      \"templated\": true,\n      \"title\": \"Link for TileJSON (Template URL)\"\n    },\n    {\n      \"href\": \"https://openveda.cloud/api/raster/searches/7743bcb31bff7151aff7e5508785fce1/{tileMatrixSetId}/WMTSCapabilities.xml\",\n      \"rel\": \"wmts\",\n      \"templated\": true,\n      \"title\": \"Link for WMTS (Template URL)\"\n    }\n  ]\n}\n\n\n\n# Get base url for tiler from the register mosaic request\ntiles_href = next(\n    link[\"href\"] for link in mosaic_response[\"links\"] if link[\"rel\"] == \"tilejson\"\n)\n\n\n\nConfigure map formatting parameters\nSee the openveda.cloud/api/raster/docs for more formatting options"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#use-the-built-in-swir-post-processing-algorithm",
    "title": "Get tiles from COGs",
    "section": "Use the built-in SWIR post processing algorithm",
    "text": "Use the built-in SWIR post processing algorithm\nNote in the example below the band assets for HLS S30 are selected. The equivalent SWIR band assets for L30 are provided at the top of this notebook.\n\n# Add additional map formatting parameters to tiles url\n\n# Use default tile matrix set\ntile_matrix_set_id = \"WebMercatorQuad\"\n\ntilejson_response = requests.get(\n    tiles_href.format(tileMatrixSetId=tile_matrix_set_id),\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"algorithm\": \"swir\",\n        \"assets\": s30_swir_assets\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://openveda.cloud/api/raster/searches/7743bcb31bff7151aff7e5508785fce1/tiles/WebMercatorQuad/{z}/{x}/{y}?algorithm=swir&assets=B12&assets=B8A&assets=B04\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.00000000000009,\n    85.0511287798066\n  ],\n  \"center\": [\n    4.263256414560601e-14,\n    0.0,\n    6\n  ]\n}\n\n\n\nDisplay the data on a map\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFormat and render tiles using custom formatting\nThe titiler/raster-api supports user defined band combinations, band math expressions, rescaling, band index, resampling and more.\n\n# Add additional map formatting parameters to tiles url\n\n# Use default tile matrix set\ntile_matrix_set_id = \"WebMercatorQuad\"\n\ntilejson_response = requests.get(\n    tiles_href.format(tileMatrixSetId=tile_matrix_set_id),\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"assets\": s30_vegetation_index_assets,\n        \"expression\": s30_vegetation_index_expression,\n        \"rescale\": s30_vegetation_index_rescaling,\n        \"colormap_name\": s30_vegetation_index_colormap,\n    },\n).json()\nprint(json.dumps(tilejson_response, indent=2))\n\n{\n  \"tilejson\": \"2.2.0\",\n  \"name\": \"7743bcb31bff7151aff7e5508785fce1\",\n  \"version\": \"1.0.0\",\n  \"scheme\": \"xyz\",\n  \"tiles\": [\n    \"https://openveda.cloud/api/raster/searches/7743bcb31bff7151aff7e5508785fce1/tiles/WebMercatorQuad/{z}/{x}/{y}?assets=B08&assets=B04&expression=%28B08_b1-B04_b1%29%2F%28B08_b1%2BB04_b1%29&rescale=0%2C1&colormap_name=rdylgn\"\n  ],\n  \"minzoom\": 6,\n  \"maxzoom\": 12,\n  \"bounds\": [\n    -180.0,\n    -85.0511287798066,\n    180.00000000000009,\n    85.0511287798066\n  ],\n  \"center\": [\n    4.263256414560601e-14,\n    0.0,\n    6\n  ]\n}\n\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((ida_bbox[1] + ida_bbox[3]) / 2, (ida_bbox[0] + ida_bbox[2]) / 2),\n    zoom_start=zoom_start,\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",\n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/hls-visualization.html#l30-hurricane-maria-example",
    "href": "instance-management/notebooks/quickstarts/hls-visualization.html#l30-hurricane-maria-example",
    "title": "Get tiles from COGs",
    "section": "L30 Hurricane Maria Example",
    "text": "L30 Hurricane Maria Example\n\ncollections_filter = {\n    \"op\": \"=\", \n    \"args\" : [{ \"property\": \"collection\" }, l30_collection_id]\n}\n\nspatial_filter = {\n    \"op\": \"s_intersects\",\n    \"args\": [\n        {\"property\": \"bbox\"}, maria_bbox\n    ]\n}\n\ntemporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        { \"property\": \"datetime\" },\n        { \"interval\" : maria_temporal_range }\n    ]\n}\n\n# Additional filters can be applied for other search criteria like &lt;= maximum eo:cloud_cover in item properties\ncloud_filter = {\n    \"op\": \"&lt;=\",\n    \"args\": [\n        {\"property\": \"eo:cloud_cover\"},\n        80\n    ]\n}\n\n# Specify cql2-json filter language in search body and add context for a summary of matched results\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"context\": \"on\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            collections_filter,\n            temporal_filter,\n            cloud_filter\n        ]\n    }\n}\n\n# Note this search body can also be used for a stac item search \nstac_items_response = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json=search_body,\n).json()\n\n# Check how many items were matched in searc\nprint(\"search context:\", stac_items_response[\"context\"])\n\n# Iterate over search results to get an array of unique item datetimes\ndatetimes = []\nfeatures = stac_items_response[\"features\"]\ndatetimes += [item[\"properties\"][\"datetime\"] for item in features]\nnext_link = next((link for link in stac_items_response[\"links\"] if link[\"rel\"] == \"next\"), None)\nwhile next_link:\n    stac_items_response = requests.post(\n        f\"{STAC_API_URL}/search\",\n        json=next_link[\"body\"],\n    ).json()\n    features = stac_items_response[\"features\"]\n    datetimes += [item[\"properties\"][\"datetime\"] for item in features]\n    next_link = next((link for link in stac_items_response[\"links\"] if link[\"rel\"] == \"next\"), False)\n\nsorted(datetimes)\n\nsearch context: {'limit': 10, 'matched': 9, 'returned': 9}\n\n\n['2017-06-06T14:43:41.335694+00:00',\n '2017-06-22T14:43:47.156698+00:00',\n '2017-07-24T14:43:56.898518+00:00',\n '2017-08-09T14:44:03.584741+00:00',\n '2017-08-25T14:44:07.854507+00:00',\n '2017-09-26T14:44:14.813967+00:00',\n '2017-10-12T14:44:19.576858+00:00',\n '2017-11-13T14:44:17.834919+00:00',\n '2017-11-29T14:44:11.126689+00:00']\n\n\n\n# Restricted date range \nrestricted_temporal_filter = {\n    \"op\": \"t_intersects\",\n    \"args\": [\n        { \"property\": \"datetime\" },\n        { \"interval\" : [ \"2017-10-11T00:00:00Z\", \"2017-10-13T00:00:00Z\"] }\n    ]\n}\n\n# Specify cql2-json filter language in search body\nsearch_body = {\n    \"filter-lang\": \"cql2-json\",\n    \"filter\": {\n        \"op\": \"and\",\n        \"args\": [\n            collections_filter,\n            spatial_filter,\n            restricted_temporal_filter\n        ]\n    }\n}\n\nmosaic_response = requests.post(\n    f\"{RASTER_API_URL}/searches/register\",\n    json=search_body,\n).json()\n\n# Set up format for Map API url\n# Get base url for tiler from the register mosaic request\ntiles_href = next(link[\"href\"] for link in mosaic_response[\"links\"] if link[\"rel\"]==\"tilejson\")\n\n# Use default tile matrix set\ntile_matrix_set_id = \"WebMercatorQuad\"\n\ntilejson_response = requests.get(\n    tiles_href.format(tileMatrixSetId=tile_matrix_set_id),\n    params={\n        # Info to add to the tilejson response\n        \"minzoom\": 6,\n        \"maxzoom\": 12,\n        \"assets\": l30_ndwi_assets,\n        \"expression\": l30_ndwi_expression,\n        \"rescale\": l30_ndwi_rescaling,\n        \"colormap_name\": \"viridis\"\n    }\n).json()\n\n\n# Use bbox initial zoom and map\n# Set up a map located w/in event bounds\nzoom_start = 11\nm = Map(\n    tiles=\"OpenStreetMap\",\n    location=((maria_bbox[1] + maria_bbox[3]) / 2,(maria_bbox[0] + maria_bbox[2]) / 2),\n    zoom_start=zoom_start\n)\n\n# Add the formatted map layer\nmap_layer = TileLayer(\n    tiles=tilejson_response[\"tiles\"][0],\n    attr=\"Mosaic\",  \n)\nmap_layer.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#run-this-notebook",
    "title": "Get map from COGs - NO2",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#approach",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#approach",
    "title": "Get map from COGs - NO2",
    "section": "Approach",
    "text": "Approach\n\nFetch STAC item for a particular date and collection - NO2\nPass STAC item in to the raster API /stac/tilejson.json endpoint\nVisualize tiles using folium\n\n\nimport requests\nimport folium"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#declare-your-collection-of-interest",
    "title": "Get map from COGs - NO2",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\ncollection_id = \"no2-monthly\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#fetch-stac-collection",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_id}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'assets': {'thumbnail': {'href': 'https://thumbnails.openveda.cloud/no2--dataset-cover.jpg',\n   'type': 'image/jpeg',\n   'roles': ['thumbnail'],\n   'title': 'Thumbnail',\n   'description': 'Photo by [Mick Truyts](https://unsplash.com/photos/x6WQeNYJC1w) (Power plant shooting steam at the sky)'}},\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2016-01-01T00:00:00+00:00',\n     '2022-12-31T00:00:00+00:00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'bidx': [1],\n   'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 15000000000000000]],\n   'resampling': 'bilinear',\n   'color_formula': 'gamma r 1.05',\n   'colormap_name': 'rdbu_r'}},\n 'providers': [{'url': 'https://disc.gsfc.nasa.gov/',\n   'name': 'NASA Goddard Earth Sciences Data and Information Services Center',\n   'roles': ['producer', 'processor']},\n  {'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2023-09-30T00:00:00Z']},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/render/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#fetch-stac-item-for-a-particular-time",
    "title": "Get map from COGs - NO2",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\nlen(items)\n\n1\n\n\nLet’s take a look at that one item.\n\nitem = items[0]\nitem\n\n{'id': 'OMI_trno2_0.10x0.10_202101_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://openveda.cloud/api/stac/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc'},\n  {'title': 'Map of Item',\n   'href': 'https://openveda.cloud/api/raster/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc/map?bidx=1&assets=cog_default&rescale=0%2C15000000000000000&resampling=bilinear&color_formula=gamma+r+1.05&colormap_name=rdbu_r',\n   'rel': 'preview',\n   'type': 'text/html'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store/no2-monthly/OMI_trno2_0.10x0.10_202101_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n   'proj:epsg': 4326,\n   'proj:wkt2': 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n   'proj:shape': [1800, 3600],\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 35781585143857150,\n      'min': -4107596126486528.0,\n      'count': 11,\n      'buckets': [7437, 432387, 2866, 699, 356, 207, 76, 27, 7, 1]},\n     'statistics': {'mean': 367152773066762.6,\n      'stddev': 961254458662885.4,\n      'maximum': 35781585143857150,\n      'minimum': -4107596126486528.0,\n      'valid_percent': 84.69829559326172}}],\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-180.0, -90.0],\n      [180.0, -90.0],\n      [180.0, 90.0],\n      [-180.0, 90.0],\n      [-180.0, -90.0]]]},\n   'proj:projjson': {'id': {'code': 4326, 'authority': 'EPSG'},\n    'name': 'WGS 84',\n    'type': 'GeographicCRS',\n    'datum': {'name': 'World Geodetic System 1984',\n     'type': 'GeodeticReferenceFrame',\n     'ellipsoid': {'name': 'WGS 84',\n      'semi_major_axis': 6378137,\n      'inverse_flattening': 298.257223563}},\n    '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json',\n    'coordinate_system': {'axis': [{'name': 'Geodetic latitude',\n       'unit': 'degree',\n       'direction': 'north',\n       'abbreviation': 'Lat'},\n      {'name': 'Geodetic longitude',\n       'unit': 'degree',\n       'direction': 'east',\n       'abbreviation': 'Lon'}],\n     'subtype': 'ellipsoidal'}},\n   'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0]},\n  'rendered_preview': {'title': 'Rendered preview',\n   'href': 'https://openveda.cloud/api/raster/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc/preview.png?bidx=1&assets=cog_default&rescale=0%2C15000000000000000&resampling=bilinear&color_formula=gamma+r+1.05&colormap_name=rdbu_r',\n   'rel': 'preview',\n   'roles': ['overview'],\n   'type': 'image/png'}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'end_datetime': '2021-01-31T00:00:00+00:00',\n  'start_datetime': '2021-01-01T00:00:00+00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/raster/v1.1.0/schema.json',\n  'https://stac-extensions.github.io/projection/v1.1.0/schema.json']}\n\n\n\nitem_stats = item['assets']['cog_default']['raster:bands'][0]['statistics']\nrescale_values = item_stats['minimum'], item_stats['maximum']"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "href": "instance-management/notebooks/quickstarts/no2-map-plot.html#use-stactilejson.json-to-get-tiles",
    "title": "Get map from COGs - NO2",
    "section": "Use /stac/tilejson.json to get tiles",
    "text": "Use /stac/tilejson.json to get tiles\nWe pass the, item id, collection name, and the rescale_values in to the RASTER API endpoint and get back a tile.\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/collections/{collection_id}/items/{item['id']}/tilejson.json?\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values[0]},{rescale_values[1]}\",\n).json()\ntiles\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://openveda.cloud/api/raster/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202101_Col3_V4.nc/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?assets=cog_default&color_formula=gamma+r+1.05&colormap_name=rdbu_r&rescale=-4107596126486528.0%2C35781585143857150'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-180.0, -90.0, 180.0, 90.0],\n 'center': [0.0, 0.0, 0]}\n\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    min_zoom=3,\n    attr=\"VEDA\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html",
    "title": "Template (Using the raster API)",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that use the raster API. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#run-this-notebook",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#run-this-notebook",
    "title": "Template (Using the raster API)",
    "section": "Run this notebook",
    "text": "Run this notebook\nYou can launch this notbook using mybinder, by clicking the button below.\n  \n\nFill in the text in italics in the following cells"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#approach",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#approach",
    "title": "Template (Using the raster API)",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#about-the-data",
    "title": "Template (Using the raster API)",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#declare-your-collection-of-interest",
    "title": "Template (Using the raster API)",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://openveda.cloud/api/stac/collections\nSTAC Browser: http://openveda.cloud\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. In some notebooks we get the collection and use all the items, and in others we search for specific items."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#fetch-stac-collection",
    "title": "Template (Using the raster API)",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_id}\").json()\ncollection"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#fetch-stac-item-for-a-particular-time",
    "title": "Template (Using the raster API)",
    "section": "Fetch STAC item for a particular time",
    "text": "Fetch STAC item for a particular time\nWe can use the search API to find the item that matches exactly our time of interest.\n\nresponse = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id],\n        \"query\": {\"datetime\": {\"eq\": \"2021-01-01T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems = response[\"features\"]\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#define-an-aoi",
    "title": "Template (Using the raster API)",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm\n\n\nWith the STAC object and optionally the AOI in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is one idea though :)"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-using-the-raster-api.html#use-the-stac-item-to-get-tiles-with-the-raster-api",
    "href": "instance-management/notebooks/templates/template-using-the-raster-api.html#use-the-stac-item-to-get-tiles-with-the-raster-api",
    "title": "Template (Using the raster API)",
    "section": "Use the STAC Item to get tiles with the RASTER API",
    "text": "Use the STAC Item to get tiles with the RASTER API\nWe pass the item_id, collection id, and rescale_values in to the RASTER API /collections/{collection_id}/items/{item_id}/tilejson.json endpoint and get back a tile. See the tips below for choosing visualization parameters for your tiles.\n\nColormap Tip: Find the list of available colormaps at {RASTER_API}/colorMaps (openveda.cloud/api/raster/colorMaps) and get colormap metadata and/or legend image at {RASTER_API}/colorMaps/{colorMapName} (See docs at openveda.cloud/api/raster/docs#/ColorMaps/getColorMap)\n\n\nTiling schemes Tip: Find the list of available tile matrix set ids at {RASTER_API}/tileMatrixSets (openveda.cloud/api/raster/tileMatrixSets) and get tiling scheme metadata at {RASTER_API}/colorMaps/{colorMapName} (See docs at openveda.cloud/api/raster/docs#/tileMatrixSets/tileMatrixSetId)\n\n\nRaster rescale range tip: Get the statistics for item assets at {RASTER_API}/collections/{collection_id/items/{item_id}/statistics ()\n\n\n# Start with the first item returned\nitem = items[0]\n\n# Here is a default tile matrix id\ntile_matrix_set_id = \"WebMercatorQuad\"\n\n# Adjust these values to find the ideal range for the range of data you will be visualizing\nrescale_min = 0\nrescale_max = 1\n\n# Set the asset key you want to visualize (many STAC Items in the VEDA catalog have a cog_default assets)\nasset_key = \"cog_default\"\n\n# Choose a colormap\ncolormap_name = \"viridis\"\n\n# Use stac item url with to get tilejson from raster api\nurl = f\"{RASTER_API_URL}/collections/{collection_id}/items/{item['id']}/{tile_matrix_set_id}/tilejson.json\"\n\ntiles = requests.get(\n    url,\n    params = {\n        \"assets\": asset_key,\n        \"colormap_name\": colormap_name,\n        \"rescale\": f\"{rescale_min},{rescale_max}\"\n    }\n).json()\ntiles\n\nWith that tile url in hand we can create a simple visualization using folium.\n\nfolium.Map(\n    tiles=tiles[\"tiles\"][0],\n    attr=\"VEDA\",\n)"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html",
    "title": "Template (Accessing data directly)",
    "section": "",
    "text": "This notebook is intended to act as a template for the example notebooks that access the data directly. These green cells should all be deleted and in several sections only one of the provided cells should be included in the notebook."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#run-this-notebook",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#run-this-notebook",
    "title": "Template (Accessing data directly)",
    "section": "Run this notebook",
    "text": "Run this notebook\nYou can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\nInside the Hub\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\nOutside the Hub\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'\n\n\nFill in the text in italics in the following cells"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#approach",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#approach",
    "title": "Template (Accessing data directly)",
    "section": "Approach",
    "text": "Approach\n\nlist a few steps that outline the approach\nyou will be taking in this notebook\n\n\n# include all your imports in this cell\nimport folium\nimport requests\nimport stackstac\n\nfrom pystac_client import Client"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#about-the-data",
    "title": "Template (Accessing data directly)",
    "section": "About the data",
    "text": "About the data\nOptional description of the dataset."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#declare-your-collection-of-interest",
    "title": "Template (Accessing data directly)",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://openveda.cloud/api/stac/collections\nSTAC Browser: http://openveda.cloud\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\n\ncollection_id = \n\n\nNext step is to get STAC objects from the STAC API. We use pystac-client to do a search. Here is an some example of what that might look like."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Template (Accessing data directly)",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\nbbox = [-180.0, -90.0, 180.0, 90.0]\ndatetime = \"2000-01-01/2022-01-02\"\n\n\ncatalog = Client.open(STAC_API_URL)\n\nsearch = catalog.search(\n    bbox=bbox, datetime=datetime, collections=[collection_id], limit=1000\n)\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\n\nThe next step is often to define an Area of Interest. Note that it is preferred to get large geojson objects directly from their source rather than storing them in this repository or inlining them in the notebook. Here is an example of what that might look like."
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#define-an-aoi",
    "title": "Template (Accessing data directly)",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON from an authoritative online source for instance: https://gadm.org/download_country.html\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\naoi = result[\"features\"][0]\n\n\nNext some notebooks read in the data. If you are using the raster API to trigger computation server side skip this section. Here is an example of reading the data in using stackstac and clipping using rasterio.\n\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=2,\n)\n\nfolium.GeoJson(aoi, name=\"AOI\").add_to(m)\nm"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#read-data",
    "title": "Template (Accessing data directly)",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataSet using stackstac\n\n# This is a workaround that is planning to move up into stackstac itself\nimport rasterio as rio\nimport boto3\n\n\nimport pandas as pd\nda = stackstac.stack(search.item_collection())\nda"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#clip-the-data-to-aoi",
    "title": "Template (Accessing data directly)",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.clip([aoi[\"geometry\"]])\nsubset\n\n\nWith the STAC object, and optionally the AOI and/or the data in hand, the next step is to do some analysis. The sections in the rest of the notebooks are totally up to you! Here is an idea though :)"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#select-a-band-of-data",
    "title": "Template (Accessing data directly)",
    "section": "Select a band of data",
    "text": "Select a band of data\nThere is just one band in this case, cog_default.\n\ndata_band = da.sel(band=\"cog_default\")"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "href": "instance-management/notebooks/templates/template-accessing-the-data-directly.html#compute-and-plot",
    "title": "Template (Accessing data directly)",
    "section": "Compute and plot",
    "text": "Compute and plot\nCalculate the mean at each time across the whole dataset. Note this is the first time that the data is actually loaded.\n\n# Average over entire AOI for each month\nmeans = data_band.mean(dim=(\"x\", \"y\")).compute()\n\n\nmeans.plot()"
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/index.html",
    "href": "instance-management/adding-content/dashboard-configuration/index.html",
    "title": "Dashboard Configuration",
    "section": "",
    "text": "This guide explains how to publish content to your instance of the VEDA Dashboard. You will do this in your own customized repository that resembles veda-config, the configuration repository for the Earthdata VEDA Dashboard.\nDetailed technical documentation for each of the steps is available on GitHub and other places, links provided in the Appendix below.\nflowchart LR\n    A(Data & Content Prep) --&gt; B{Is the data already in VEDA?}\n    B --&gt;|No| C[Go to Dataset Ingestion]\n    C --&gt; E\n    B --&gt;|Yes| E{Do you have a story?}\n    E --&gt;|Yes| D[Go to Story Configuration]\n    E --&gt;|No| F[Go to Dataset Configuration]\n    click C \"../dataset-ingestion/index.html\" \"Docs on Dataset Ingestion\" _blank\n    click D \"./story-configuration.html\" \"Docs on story Configuration\" _blank\n    click F \"./dataset-configuration.html\" \"Docs on Dataset Configuration\" _blank"
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/index.html#data-content-preparation",
    "href": "instance-management/adding-content/dashboard-configuration/index.html#data-content-preparation",
    "title": "Dashboard Configuration",
    "section": "Data & Content Preparation",
    "text": "Data & Content Preparation\nThis is an important step before ingesting or configuring anything within VEDA. This will set you up for success in later steps.\n\nRequired Content\nFor Stories, the required content is:\n\nText for the actual story itself\nAny visuals you would like to include, whether that be images, charts, maps, or other\n\nIf maps, identify which dataset and layer you would like to show and whether that is included in VEDA. (⚠️ If the dataset is not yet included in VEDA you’ll have to provide information about it and configure it as explained below).\nIf charts, gather the relevant data to build the chart. A csv file is the most common, but json is also supported\n\nA cover image for the dataset as it will appear in the Dashboard\nA title and short description/sub-title (5-10 words) for the story\n\nNext step: Story Configuration.\nFor Datasets, the required content is:\n\nA descriptive overview of the dataset, how it came to exist, who maintains it, and how it should be used\nShort descriptions for each layer that you will want to reveal within VEDA (an example of this would be “CO2 mean vs CO2 difference”) for users to explore on a map\nA cover image for the dataset as it will appear in the Dashboard\nAny other relevant metadata you might want included https://nasa-impact.github.io/veda-docs/adding-content/dashboard-content.html\n\nNext step: If your data is already in VEDA go to Dataset Configuration. Otherwise go to Dataset Ingestion."
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/index.html#useful-links",
    "href": "instance-management/adding-content/dashboard-configuration/index.html#useful-links",
    "title": "Dashboard Configuration",
    "section": "Useful Links",
    "text": "Useful Links\n\nVEDA Dashboard core repository and documentation - veda-ui\nExample content repository for the Earthdata VEDA Dashboard - veda-config\nData processing from EIS\nAlexey’s notes on helpful tips"
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/dataset-configuration.html",
    "href": "instance-management/adding-content/dashboard-configuration/dataset-configuration.html",
    "title": "Dataset Configuration",
    "section": "",
    "text": "Once a dataset is available in your VEDA STAC through VEDA Data Services (following the steps in the Dataset Ingestion docs), you will need to configure the Dashboard.\nPlease note that the VEDA Dashboard relies on its own set of metadata about datasets. No information from STAC is loaded initially, so some metadata may require copying information from the STAC records, such as title, description, and dataset providers.\n\nKey Steps\nThe key steps for configuring a dataset overview page (which is required to reference a dataset in any stories) are similar to configuring a story. The steps are outlined below.\n\nGo to the veda-config repo in GitHub\nIf using a local environment:\n\nFamiliarize yourself with the Setup and Configuration sections of the documentation\nUsing your local environment, create a branch for your dataset overview\nFollowing the guidelines outlined in the Content section of the GitHub documentation, create your Dataset Overview MDX file\nAdd relevant files and assets as needed\nPush your branch and create a pull request in GitHub\n\nIf configuring through GitHub\n\nFollowing the guidelines outlined in the Content section of the VEDA UI documentation, create your Dataset Overview MDX file and add it to a new branch on GitHub.\nAdd relevant files and assets as needed\nCommit your changes and open a Pull Request\n\nOnce the pull request is created, you will be able to see a preview of the dataset overview in a Netlify box under the Conversation tab of the pull request\n🍀 You don’t have to fully finish your dataset overview all in one go. Every time you make a commit the preview will be regenerated with your changes (takes about 3 minutes).\nOnce you feel good about the dataset overview, add the necessary team members and stakeholders to review the dataset overview\nPaste a comment in the pull request with any additional information, such as any goal dates for publishing this dataset overview or any outstanding questions you have\nOnce the pull request is merged, the files will still need to be pushed to production. Coordinate with your development team and managers to determine the release date.\n\nIf you have any questions along the way, we prefer that you open tickets in veda-config. Alternatively, you can reach the VEDA team at veda@uah.edu."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/file-preparation.html",
    "href": "instance-management/adding-content/dataset-ingestion/file-preparation.html",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The rio-cogeo documentation is a great starting reference point and have a very helpful guide on preparing COGs, too.\n\nTo inspect and validate your raster dataset use the following\nrio cogeo info /path/to/file.tif\nThis will allow you to explore the size in rows and lines, understand the data type (e.g., byte, float, etc.), and verify how NoData is defined. Once you have explored the COG’s definitions you can validate it by using the following commands:\nrio cogeo validate /path/to/file.tif\nCOG validation can be used to identify if any errors are found. The following steps can be used to resolve any errors.\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the most appropriate resampling method for overviews. For example, use average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore)."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "href": "instance-management/adding-content/dataset-ingestion/file-preparation.html#step-i-prepare-the-data",
    "title": "File preparation",
    "section": "",
    "text": "VEDA supports inclusion of cloud optimized GeoTIFFs (COGs) to its data store.\n\n\nWe often encounter issues like missing or wrong nodata value, missing coordinate-reference system, missing or wrong overviews - polluted by fill values or not conserving class values in categorical data, empty files, or artifacts in the data.\nDiscovering these issues early on (ideally before upload to our buckets) can save us all a lot of time.\nA command-line tool for creating and validating COGs is rio-cogeo. The rio-cogeo documentation is a great starting reference point and have a very helpful guide on preparing COGs, too.\n\nTo inspect and validate your raster dataset use the following\nrio cogeo info /path/to/file.tif\nThis will allow you to explore the size in rows and lines, understand the data type (e.g., byte, float, etc.), and verify how NoData is defined. Once you have explored the COG’s definitions you can validate it by using the following commands:\nrio cogeo validate /path/to/file.tif\nCOG validation can be used to identify if any errors are found. The following steps can be used to resolve any errors.\nIf your raster contains empty pixels, make sure the nodata value is set correctly (check with rio cogeo info). The nodata value needs to be set before cloud-optimizing the raster, so overviews are computed from real data pixels only. Pro-tip: For floating-point rasters, using NaN for flagging nodata helps avoid roundoff errors later on.\nYou can set the nodata flag on a GeoTIFF in-place with:\nrio edit_info --nodata 255 /path/to/file.tif\nor in Python with\nimport rasterio\n\nwith rasterio.open(\"/path/to/file.tif\", \"r+\") as ds:\n    ds.nodata = 255\nNote that this only changes the flag. If you want to change the actual value you have in the data, you need to create a new copy of the file where you change the pixel values.\nMake sure the coordinate reference system is embedded in the COG (check with rio cogeo info)\nWhen creating the COG, use the most appropriate resampling method for overviews. For example, use average for continuous / floating point data and mode for categorical / integer.\nrio cogeo create --overview-resampling \"mode\" /path/to/input.tif /path/to/output.tif\n\n\n\n\nMake sure that the COG filename is meaningful and contains the datetime associated with the COG in the following format. All the datetime values in the file should be preceded by the _ underscore character. Some examples are shown below:\n\n\n\nYear data: nightlights_2012.tif, nightlights_2012-yearly.tif\nMonth data: nightlights_201201.tif, nightlights_2012-01_monthly.tif\nDay data: nightlights_20120101day.tif, nightlights_2012-01-01_day.tif\n\n\n\n\n\nYear data: nightlights_2012_2014.tif, nightlights_2012_year_2015.tif\nMonth data: nightlights_201201_201205.tif, nightlights_2012-01_month_2012-06_data.tif\nDay data: nightlights_20120101day_20121221.tif, nightlights_2012-01-01_to_2012-12-31_day.tif\n\nNote that the date/datetime value is always preceded by an _ (underscore)."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "href": "instance-management/adding-content/dataset-ingestion/file-preparation.html#step-ii-upload-to-the-veda-data-store",
    "title": "File preparation",
    "section": "STEP II: Upload to the VEDA data store",
    "text": "STEP II: Upload to the VEDA data store\nOnce you have the COGs, obtain permissions (Cognito credentials) from the VEDA team to upload them to the veda-data-store-staging bucket.\nUpload the data to a sensible location inside the bucket. Example: s3://veda-data-store-staging/&lt;collection-id&gt;/"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html",
    "href": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. For example, current VEDA datasets like no2-mean and no2-diff should be two different collections, because one measures the mean levels of nitrogen dioxide and the other the differences in observed levels. Likewise, datasets like no2-monthly and no2-yearly should be different because the periodicity is different.\nOnce you have logically grouped the datasets into collections, you will need to create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: If you are unable to complete the following steps or have a new type of data that does not work with the example docs, open an issue in the veda-data GitHub repository."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "href": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html#step-iii-create-dataset-definitions",
    "title": "Catalog Ingestion",
    "section": "",
    "text": "The next step is to divide all the data into logical collections. A collection is basically what it sounds like, a collection of data files that share the same properties like, the data it’s measuring, the periodicity, the spatial region, etc. For example, current VEDA datasets like no2-mean and no2-diff should be two different collections, because one measures the mean levels of nitrogen dioxide and the other the differences in observed levels. Likewise, datasets like no2-monthly and no2-yearly should be different because the periodicity is different.\nOnce you have logically grouped the datasets into collections, you will need to create dataset definitions for each of these collections. The data definition is a json file that contains some metadata of the dataset and information on how to discover these datasets in the s3 bucket. An example is shown below:\nlis-global-da-evap.json\n{\n  \"collection\": \"lis-global-da-evap\",\n  \"title\": \"Evapotranspiration - LIS 10km Global DA\",\n  \"description\": \"Gridded total evapotranspiration (in kg m-2 s-1) from 10km global LIS with assimilation\",\n  \"license\": \"CC0-1.0\",\n  \"is_periodic\": true,\n  \"time_density\": \"day\",\n  \"spatial_extent\": {\n    \"xmin\": -179.95,\n    \"ymin\": -59.45,\n    \"xmax\": 179.95,\n    \"ymax\": 83.55\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2002-08-02T00:00:00Z\",\n    \"enddate\": \"2021-12-01T00:00:00Z\"\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/EIS/COG/LIS_GLOBAL_DA/Evap/LIS_Evap_200208020000.d01.cog.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"dry_run\": false,\n      \"prefix\": \"EIS/COG/LIS_GLOBAL_DA/Evap/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)LIS_Evap_(.*).tif$\",\n      \"datetime_range\": \"day\"\n    }\n  ]\n}\n\n\nClick to show field descriptions\n\nThe following table describes what each of these fields mean:\n\n\n\nfield\ndescription\nallowed value\nexample\n\n\n\n\ncollection\nthe id of the collection\nlowercase letters with optional “-” delimeters\nno2-monthly-avg\n\n\ntitle\na short human readable title for the collection\nstring with 5-6 words\n“Average NO2 measurements (Monthly)”\n\n\ndescription\na detailed description for the dataset\nshould include what the data is, what sensor was used to measure, where the data was pulled/derived from, etc\n\n\n\nlicense\nlicense for data use; Default open license: CC0-1.0\nSPDX license id\nCC0-1.0\n\n\nis_periodic\nis the data periodic? specifies if the data files repeat at a uniform time interval\ntrue | false\ntrue\n\n\ntime_density\nthe time step in which we want to navigate the dataset in the dashboard\nyear | month | day | hour | minute | null\n\n\n\nspatial_extent\nthe spatial extent of the collection; a bounding box that includes all the data files in the collection\n\n{\"xmin\": -180, \"ymin\": -90, \"xmax\": 180, \"ymax\": 90}\n\n\nspatial_extent[\"xmin\"]\nleft x coordinate of the spatial extent bounding box\n-180 &lt;= xmin &lt;= 180; xmin &lt; xmax\n23\n\n\nspatial_extent[\"ymin\"]\nbottom y coordinate of the spatial extent bounding box\n-90 &lt;= ymin &lt;= 90; ymin &lt; ymax\n-40\n\n\nspatial_extent[\"xmax\"]\nright x coordinate of the spatial extent bounding box\n-180 &lt;= xmax &lt;= 180; xmax &gt; xmin\n150\n\n\nspatial_extent[\"ymax\"]\ntop y coordinate of the spatial extent bounding box\n-90 &lt;= ymax &lt;= 90; ymax &gt; ymin\n40\n\n\ntemporal_extent\ntemporal extent that covers all the data files in the collection\n\n{\"start_date\": \"2002-08-02T00:00:00Z\", \"end_date\": \"2021-12-01T00:00:00Z\"}\n\n\ntemporal_extent[\"start_date\"]\nthe start_date of the dataset\niso datetime that ends in Z\n2002-08-02T00:00:00Z\n\n\ntemporal_extent[\"end_date\"]\nthe end_date of the dataset\niso datetime that ends in Z\n2021-12-01T00:00:00Z\n\n\nsample_files\na list of s3 urls for the sample files that go into the collection\n\n[ \"s3://veda-data-store-staging/no2-diff/no2-diff_201506.tif\", \"s3://veda-data-store-staging/no2-diff/no2-diff_201507.tif\"]\n\n\ndiscovery_items[\"discovery\"]\nwhere to discover the data from; currently supported are s3 buckets and cmr\ns3 | cmr\ns3\n\n\ndiscovery_items[\"cogify\"]\ndoes the file need to be converted to a cloud optimized geptiff (COG)? false if it is already a COG\ntrue | false\nfalse\n\n\ndiscovery_items[\"upload\"]\ndoes it need to be uploaded to the veda s3 bucket? false if it already exists in veda-data-store-staging\ntrue | false\nfalse\n\n\ndiscovery_items[\"dry_run\"]\nif set to true, the items will go through the pipeline, but won’t actually publish to the stac catalog; useful for testing purposes\ntrue | false\nfalse\n\n\ndiscovery_items[\"bucket\"]\nthe s3 bucket where the data is uploaded to\nany bucket that the data pipelines has access to\nveda-data-store-staging | climatedashboard-data | {any-public-bucket}\n\n\ndiscovery_items[\"prefix\"]\nwithin the s3 bucket, the prefix or path to the “folder” where the data files exist\nany valid path winthin the bucket\nEIS/COG/LIS_GLOBAL_DA/Evap/\n\n\ndiscovery_items[\"filename_regex\"]\na common filename pattern that all the files in the collection follow\na valid regex expression\n(.*)LIS_Evap_(.*).cog.tif$\n\n\ndiscovery_items[\"datetime_range\"]\nbased on the naming convention in STEP I, the datetime range to be extracted from the filename\nyear | month | day\nyear\n\n\n\n\n\nNote: If you are unable to complete the following steps or have a new type of data that does not work with the example docs, open an issue in the veda-data GitHub repository."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "href": "instance-management/adding-content/dataset-ingestion/catalog-ingestion.html#step-iv-publication",
    "title": "Catalog Ingestion",
    "section": "STEP IV: Publication",
    "text": "STEP IV: Publication\nThe publication process involves 3 steps:\n\n[VEDA] Publishing to the staging STAC catalog https://staging.openveda.cloud\n[EIS] Reviewing the collection/items published to the dev STAC catalog\n[VEDA] Publishing to the production STAC catalog https://openveda.cloud by submitting the configuration you just created in a pull request to the veda-data repo.\n\nTo use the VEDA Workflows API to schedule ingestion/publication of the data follow these steps:\n\nPrerequisite: obtain credentials from a VEDA team member\nAsk a VEDA team member to create Cognito credentials (username and password) for VEDA authentication.\n\n\nSign in to the workflows API docs using your credentials\nOpen the workflows API at staging.openveda.cloud/api/workflows/docs in a second browser tab and click the green authorize button at the upper right to authenticate your session with your username and password (you will be temporarily redirected to a login widget and then back to the workflows-api docs).\n\n\n/dataset/validate\nAfter creating your dataset definition, copy the printed json and paste it into the /dataset/validate input in the workflows-api docs page in the second tab. Note that if you navigate away from this page you will need to click authorize again.\nChoose POST dataset/validate in the Dataset section of the workflows-api docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nIf the json is valid, the response will confirm that it is ready to be published on the VEDA Platform.\n\n\n/dataset/publish\nNow that you have validated your dataset, you can initiate a workflow and publish the dataset to the VEDA Platform.\nChoose POST dataset/publish in the Dataset section of the workflows-api docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nOn success, you will recieve a success message containing the id of your workflow, for example\n{\"message\":\"Successfully published collection: geoglam. 1  workflows initiated.\",\"workflows_ids\":[\"db6a2097-3e4c-45a3-a772-0c11e6da8b44\"]}"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-collection-conventions.html",
    "href": "instance-management/adding-content/dataset-ingestion/stac-collection-conventions.html",
    "title": "STAC collection conventions",
    "section": "",
    "text": "Copied from veda-backend#29\nDashboard-specific notes that supplement the full stac-api collection specification. Note that there is no schema enforcement on the collection table content in pgstac—this provides flexibility but also requires caution when creating and modifying Collections."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "href": "instance-management/adding-content/dataset-ingestion/stac-collection-conventions.html#collection-field-extension-and-naming-recommendations",
    "title": "STAC collection conventions",
    "section": "Collection field, extension, and naming recommendations",
    "text": "Collection field, extension, and naming recommendations\n\n\n\nField &/or Extension\nRecommendations\n\n\n\n\nid\nIf dataset exists in NASA’s Earthdata or presumably from some other data provider like ESA, use that ID. If appropriate, add a suffix for any additional processing that has been performed, e.g. “OMSO2PCA_cog”. If dataset is not from NASA’s Earthdata, we can use a human readable name with underscores like “facebook_population_density”.\n\n\ndashboard extension\nTo support the delta-ui we have added two new fields in a proposed dashboard extension. For now we are just adding the fields but after testing things out, we can formalize the extension with a hosted json schema. Dashboard extension properties are only required for collections that will be viewed in the delta-ui dashboard.\n\n\ndashboard:is_periodic\nTrue/False This boolean is used when summarizing the collection—if the collection is periodic, the temporal range of the items in the collection and the time density are all the front end needs to generate a time picker. If the items in the collection are not periodic, a complete list of the unique item datetimes is needed.\n\n\ndashboard:time_density\nyear, month, day, hour, minute, or null. These time steps should be treated as enum when the extension is formalized. For collections with a single time snapshot this value is null.\n\n\nitem_assets\nstac-extension/item_assets is used to explain the assets that are provided for each item in the collection. We’re not providing thumbnails yet, but this example below includes a thumbnail asset to illustrate how the extension will be used. The population of this property is not automated, the creator of the collection writes the item assets documentation. Item assets are only required for collections that will be viewed in the delta-ui dashboard.\n\n\nsummaries\nThe implementation of this core stac-spec field is use-case specific. Our implementation is intended to support the dashboard and will supply datetime and raster statistics for the default map layer asset across the entire collection. Currently summaries are manually updated with a delta-ui specific user defined function in pgstac.\n\n\ntitle and description\nUse these properties to provide specific information about the collection to API users and catalog browsers. These properties correspond to dataset name and info in the covid-api but the delta dashboard will use delta-config to set these values in the UI so the information in our stac collections will be for data curators and API users.\n\n\ncollection name style choices\nPrefer lower-case kebab-case collection names. Decision: Should names align with underlying data identifiers or should it be an interpreted name? omi-trno2-dhrm and omi-trno2-dhrm-difference vs no2-monthly and no2-monthly-diff; bmhd-30m-monthly vs nightlights-hd-monthly\n\n\nlicense\nSPDX license id, license is likely available in CMR but we may need to research other sources of data. Default open license: CC0-1.0\n\n\n\nitem_assets example\n\n\"item_assets\": {\n    \"thumbnail\": {\n      \"type\": \"image/jpeg\",\n      \"roles\": [\n        \"thumbnail\"\n      ],\n      \"title\": \"Thumbnail\",\n      \"description\": \"A medium sized thumbnail\"\n    },\n    \"cog_default\": {\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"roles\": [\n        \"data\",\n        \"layer\"\n      ],\n      \"title\": \"Default COG Layer\",\n      \"description\": \"Cloud optimized default layer to display on map\"\n    }\n  }\nsummaries example for periodic collection\n\"summaries\": {\n    \"datetime\": [\"2016-01-01T00:00:00Z\", \"2022-01-01T00:00:00Z\"],\n    \"cog_default\": {\n      \"max\": 50064805976866820,\n      \"min\": -6618294421291008\n    }\n  }\nsummaries example for non-periodic collection\n\"summaries\": {\n    \"datetime\": [\n      \"2020-01-01T00:00:00Z\",\n      \"2020-02-01T00:00:00Z\",\n      \"2020-03-01T00:00:00Z\",\n      \"2020-04-01T00:00:00Z\",\n      \"2020-05-01T00:00:00Z\",\n      \"2020-06-01T00:00:00Z\",\n      \"2020-07-01T00:00:00Z\",\n      \"2020-08-01T00:00:00Z\",\n      \"2020-09-01T00:00:00Z\",\n      \"2020-10-01T00:00:00Z\",\n      \"2020-11-01T00:00:00Z\",\n      \"2020-12-01T00:00:00Z\",\n      \"2021-01-01T00:00:00Z\",\n      \"2021-02-01T00:00:00Z\",\n      \"2021-03-01T00:00:00Z\",\n      \"2021-04-01T00:00:00Z\",\n      \"2021-05-01T00:00:00Z\",\n      \"2021-06-01T00:00:00Z\",\n      \"2021-07-01T00:00:00Z\",\n      \"2021-08-01T00:00:00Z\",\n      \"2021-09-01T00:00:00Z\"\n    ],\n    \"cog_default\": {\n      \"max\": 255,\n      \"min\": 0\n    }\n  }"
  },
  {
    "objectID": "nasa-veda-platform/dashboard/index.html",
    "href": "nasa-veda-platform/dashboard/index.html",
    "title": "VEDA Dashboard",
    "section": "",
    "text": "The VEDA Dashboard is an open-source set of configurable tools and pages that enable you and your team to visualize, analyze, and communicate science data.\nLearn more about the principles the VEDA Dashboard is designed around below.\nIf you would like to learn more about how to create your own instance of the VEDA Dashboard, visit the VEDA UI docs.",
    "crumbs": [
      "NASA VEDA Platform",
      "VEDA Dashboard"
    ]
  },
  {
    "objectID": "nasa-veda-platform/dashboard/index.html#configuration",
    "href": "nasa-veda-platform/dashboard/index.html#configuration",
    "title": "VEDA Dashboard",
    "section": "Configuration",
    "text": "Configuration\nPlease see our docs on Dashboard Configuration.",
    "crumbs": [
      "NASA VEDA Platform",
      "VEDA Dashboard"
    ]
  },
  {
    "objectID": "nasa-veda-platform/why.html",
    "href": "nasa-veda-platform/why.html",
    "title": "Why",
    "section": "",
    "text": "Source: 2022 ESDS Program Highlights"
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/index.html",
    "href": "nasa-veda-platform/scientific-computing/index.html",
    "title": "Scientific Computing",
    "section": "",
    "text": "VEDA scientific computing services provide scientists the resources they need to develop (interactively) and execute (operationally) algorithms for geospatial data science and scalable computing, and to support open science by giving all users access to sufficient resources and tools to do science and replicate scientific results. This approach includes:",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/index.html#collaborative-and-interactive-data-science-with-jupyterhub",
    "href": "nasa-veda-platform/scientific-computing/index.html#collaborative-and-interactive-data-science-with-jupyterhub",
    "title": "Scientific Computing",
    "section": "Collaborative and Interactive Data Science with JupyterHub",
    "text": "Collaborative and Interactive Data Science with JupyterHub\nVEDA promotes the use of JupyterHub environments for interactive data science. JupyterHub enables you to analyze massive archives of Earth science data in the cloud in an interactive environment that alleviates the complexities of managing compute resources (virtual machines, roles and permissions, etc).\nUsers affiliated with VEDA can get access to a dedicated JupyterHub service, provided in collaboration with 2i2c: hub.openveda.cloud. Please find instructions for requesting access below.\nIf you are a scientist affiliated with NASA projects such as EIS and MAAP, you can also keep using the resources provided by these projects.\nThrough the use of open-source technology, we make sure our services are interoperable and exchangeable.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/index.html#instructory-notebooks",
    "href": "nasa-veda-platform/scientific-computing/index.html#instructory-notebooks",
    "title": "Scientific Computing",
    "section": "Instructory notebooks",
    "text": "Instructory notebooks\nThis documentation site provides Jupyter notebooks on how to load and analyze Earth data an interactive cloud computing environment.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/index.html#access",
    "href": "nasa-veda-platform/scientific-computing/index.html#access",
    "title": "Scientific Computing",
    "section": "Getting access to VEDA’s JupyterHub environment",
    "text": "Getting access to VEDA’s JupyterHub environment\nPlease see Getting Access.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/ssh.html",
    "href": "nasa-veda-platform/scientific-computing/ssh.html",
    "title": "How to ssh into the JupyterHub",
    "section": "",
    "text": "This is a how-to guide for connecting to the VEDA JupyterHub from your local environment via ssh. This allows you to use all of ssh’s features (copy, run commands) as well as connecting via VS Code’s proprietary Remote Development functionality.\n\n\n\n\n\n\nNote\n\n\n\nwebsocat must be installed on your local machine in order to remotely connect to the VEDA JupyterHub. The full installation instructions are available on websocat’s GitHub README. websocat can be installed on macOS via Homebrew as brew install websocat.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe are exploring two options for using VS Code on the VEDA JupyterHub. These options should be considered experimental and may not be supported long-term.\n\n\n\n\n\n\n\nLaunch a server from the VEDA JupyterHub home page. The server must be started in order to connect remotely.\nGet a new token from the VEDA JupyterHub token page:\n\nEnter a descriptive name in the ‘Note’ text window.\nEnter an expiration date.\nClick “request a new API token”\n\n\n\n\nGenerate a new token\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTreat this token like you would treat a password to your JupyterHub instance!\n\n\n\n\n\nBefore your first time remotely connecting to the VEDA JupyterHub, you need to configure your local ssh.\n\nAdd an entry that looks like the following to the end of your ~/.ssh/config. Create it if it does not exist.\nHost hub.openveda.cloud\n    User jovyan\n    ProxyCommand websocat --binary -H='Authorization: token &lt;YOUR-JUPYTERHUB-TOKEN&gt;' asyncstdio: wss://%h/user/&lt;YOUR-JUPYTERHUB-USERNAME&gt;/sshd/\nReplace &lt;YOUR-JUPYTERHUB-TOKEN&gt; with the token you created earlier.\nReplace &lt;YOUR-JUPYTERHUB-USERNAME&gt; with your VEDA JupyterHub username.\n\n\n\n\nYou need to put some ssh public keys in ~/.ssh/authorized_keys after you start your JupyterHub server and have completed the setup of your private keys on your local machine.\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a terminal in JupyterLab\nRun the following commands, replacing  with your github username:\nmkdir -p ~/.ssh\nwget https://github.com/&lt;YOUR-GITHUB-USERNAME&gt;.keys -O ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n\n\n\n\n\nThere are two ways to connect to JupyterHub: Connect to the JupyterHub using VS Code’s Remote SSH feature and Connect via ssh on the command line.\n\n\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a new VS Code Window on your local maachine.\nOpen the command prompt (command + shift + P on macOS)\nEnter Remote-SSH: Connect to Host...\nSelect hub.openveda.cloud\nSelect “Open Folder” and select the specific folder that you want to work in.\n\nNow you’re connected and ready to develop using VS Code! You may need to install some extensions in the SSH server to use your regular development workflows.\n\n\n\n\n\n\nTip\n\n\n\nMost times, you will want to select a folder that is a git repository, perhaps cloned from GitHub, so that your code is version controlled.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have not ssh’d into the JupyterHub before, you will receive a notice that “The authenticity of host ‘hub.openveda.cloud’ can’t be established.”. Enter ‘yes’ in response to this prompt. This will add the key to your list of known hosts, so that you will be notified if it changes in the future.\n\n\n\n\nYou will need to install the extensions that you want to use in the SSH.\n\nNavigate to the Extensions view by clicking on the icon with four boxes and one slightly offset:\n\n\n\nExtensions icon in VS Code\n\n\nThe view shows the already installed extensions. Type the name of the extension you’re looking for in the ‘Search Extensions’ text box, or scroll to see your locally installed extensions:\n\n\n\nInstall VS Code extension\n\n\nSelect ‘Install in SSH: hub.openveda.cloud’\n\n\n\n\n\nSelect File &gt; Close Remote Connection to disconnect from the JupyterHub:\n\n\n\nClose remote connection\n\n\n\nExiting VS Code will also close the remote connection.\n\n\n\n\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a new terminal on your local machine.\nEnter ssh hub.openveda.cloud\n\nYou are now ssh’d into the JupyterHub! If you enter a command (e.g., touch am-i-on-the-jupyterhub), it will be run on the remote server.\n\n\n\n\n\n\nNote\n\n\n\nIf you have not ssh’d into the JupyterHub before, you will receive a notice that “The authenticity of host ‘hub.openveda.cloud’ can’t be established.”. Enter ‘yes’ in response to this prompt. This will add the key to your list of known hosts, so that you will be notified if it changes in the future.\n\n\n\n\n\n\nThese instructions are based off the jupyter-sshd-proxy documentation and a screen recording Yuvi Panda shared in the NASA IMPACT slack workspace. Thank you to Yuvi Panda for developing jupyter-sshd-proxy!",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to ssh into the JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/ssh.html#setup",
    "href": "nasa-veda-platform/scientific-computing/ssh.html#setup",
    "title": "How to ssh into the JupyterHub",
    "section": "",
    "text": "Launch a server from the VEDA JupyterHub home page. The server must be started in order to connect remotely.\nGet a new token from the VEDA JupyterHub token page:\n\nEnter a descriptive name in the ‘Note’ text window.\nEnter an expiration date.\nClick “request a new API token”\n\n\n\n\nGenerate a new token\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTreat this token like you would treat a password to your JupyterHub instance!\n\n\n\n\n\nBefore your first time remotely connecting to the VEDA JupyterHub, you need to configure your local ssh.\n\nAdd an entry that looks like the following to the end of your ~/.ssh/config. Create it if it does not exist.\nHost hub.openveda.cloud\n    User jovyan\n    ProxyCommand websocat --binary -H='Authorization: token &lt;YOUR-JUPYTERHUB-TOKEN&gt;' asyncstdio: wss://%h/user/&lt;YOUR-JUPYTERHUB-USERNAME&gt;/sshd/\nReplace &lt;YOUR-JUPYTERHUB-TOKEN&gt; with the token you created earlier.\nReplace &lt;YOUR-JUPYTERHUB-USERNAME&gt; with your VEDA JupyterHub username.\n\n\n\n\nYou need to put some ssh public keys in ~/.ssh/authorized_keys after you start your JupyterHub server and have completed the setup of your private keys on your local machine.\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a terminal in JupyterLab\nRun the following commands, replacing  with your github username:\nmkdir -p ~/.ssh\nwget https://github.com/&lt;YOUR-GITHUB-USERNAME&gt;.keys -O ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to ssh into the JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/ssh.html#connect-to-jupyterhub",
    "href": "nasa-veda-platform/scientific-computing/ssh.html#connect-to-jupyterhub",
    "title": "How to ssh into the JupyterHub",
    "section": "",
    "text": "There are two ways to connect to JupyterHub: Connect to the JupyterHub using VS Code’s Remote SSH feature and Connect via ssh on the command line.\n\n\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a new VS Code Window on your local maachine.\nOpen the command prompt (command + shift + P on macOS)\nEnter Remote-SSH: Connect to Host...\nSelect hub.openveda.cloud\nSelect “Open Folder” and select the specific folder that you want to work in.\n\nNow you’re connected and ready to develop using VS Code! You may need to install some extensions in the SSH server to use your regular development workflows.\n\n\n\n\n\n\nTip\n\n\n\nMost times, you will want to select a folder that is a git repository, perhaps cloned from GitHub, so that your code is version controlled.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have not ssh’d into the JupyterHub before, you will receive a notice that “The authenticity of host ‘hub.openveda.cloud’ can’t be established.”. Enter ‘yes’ in response to this prompt. This will add the key to your list of known hosts, so that you will be notified if it changes in the future.\n\n\n\n\nYou will need to install the extensions that you want to use in the SSH.\n\nNavigate to the Extensions view by clicking on the icon with four boxes and one slightly offset:\n\n\n\nExtensions icon in VS Code\n\n\nThe view shows the already installed extensions. Type the name of the extension you’re looking for in the ‘Search Extensions’ text box, or scroll to see your locally installed extensions:\n\n\n\nInstall VS Code extension\n\n\nSelect ‘Install in SSH: hub.openveda.cloud’\n\n\n\n\n\nSelect File &gt; Close Remote Connection to disconnect from the JupyterHub:\n\n\n\nClose remote connection\n\n\n\nExiting VS Code will also close the remote connection.\n\n\n\n\n\nLaunch a server from the VEDA JupyterHub home page if you don’t already have one running.\nOpen a new terminal on your local machine.\nEnter ssh hub.openveda.cloud\n\nYou are now ssh’d into the JupyterHub! If you enter a command (e.g., touch am-i-on-the-jupyterhub), it will be run on the remote server.\n\n\n\n\n\n\nNote\n\n\n\nIf you have not ssh’d into the JupyterHub before, you will receive a notice that “The authenticity of host ‘hub.openveda.cloud’ can’t be established.”. Enter ‘yes’ in response to this prompt. This will add the key to your list of known hosts, so that you will be notified if it changes in the future.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to ssh into the JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/ssh.html#acknowledgments",
    "href": "nasa-veda-platform/scientific-computing/ssh.html#acknowledgments",
    "title": "How to ssh into the JupyterHub",
    "section": "",
    "text": "These instructions are based off the jupyter-sshd-proxy documentation and a screen recording Yuvi Panda shared in the NASA IMPACT slack workspace. Thank you to Yuvi Panda for developing jupyter-sshd-proxy!",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to ssh into the JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/index.html",
    "href": "nasa-veda-platform/data-services/index.html",
    "title": "Data Services",
    "section": "",
    "text": "The goal of VEDA Data Services is to support open science through community standard tooling, Application Programming Interfaces (APIs), and providing access to persistent, discoverable Earth science datasets. This approach includes:\n\nHosting and making available key datasets in analysis-optimized formats\nTying together VEDA products by providing access to hosted data, existing third-party geospatial datasets, and new datasets produced within the VEDA ecosystem\nMaking data Findable, Accessible, Interoperable, and Reusable (FAIR), within and also beyond VEDA\n\nThe Data Services hosts a suite of services that make these pointers possible.\nSome of them include:\n\nARCO Store: holds all the Analysis Ready Cloud-Optimized (ARCO) data products\nDynamic Tiler: dynamic tiling service that makes visualization of cloud-optimized geotiffs possible\nSTAC Catalog: a catalog of all the available data in the VEDA system\nData Transformation & Publication Pipeline: a cloud-native workflow that handles data transformation, ingestion and publication to the VEDA data catalog\n\nA technical architecture diagram of how all of these work together can be found in the software architecture description.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/index.html#data-services",
    "href": "nasa-veda-platform/data-services/index.html#data-services",
    "title": "Data Services",
    "section": "",
    "text": "The goal of VEDA Data Services is to support open science through community standard tooling, Application Programming Interfaces (APIs), and providing access to persistent, discoverable Earth science datasets. This approach includes:\n\nHosting and making available key datasets in analysis-optimized formats\nTying together VEDA products by providing access to hosted data, existing third-party geospatial datasets, and new datasets produced within the VEDA ecosystem\nMaking data Findable, Accessible, Interoperable, and Reusable (FAIR), within and also beyond VEDA\n\nThe Data Services hosts a suite of services that make these pointers possible.\nSome of them include:\n\nARCO Store: holds all the Analysis Ready Cloud-Optimized (ARCO) data products\nDynamic Tiler: dynamic tiling service that makes visualization of cloud-optimized geotiffs possible\nSTAC Catalog: a catalog of all the available data in the VEDA system\nData Transformation & Publication Pipeline: a cloud-native workflow that handles data transformation, ingestion and publication to the VEDA data catalog\n\nA technical architecture diagram of how all of these work together can be found in the software architecture description.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-store.html",
    "href": "nasa-veda-platform/data-services/data-store.html",
    "title": "VEDA Data Store",
    "section": "",
    "text": "The VEDA Data Store consists of cloud object storage (AWS S3 in us-west-2) and a central Spatio-Temporal Asset Catalog (STAC) that exposes the datasets.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Store"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-store.html#dataset-selection",
    "href": "nasa-veda-platform/data-services/data-store.html#dataset-selection",
    "title": "VEDA Data Store",
    "section": "Dataset selection",
    "text": "Dataset selection\nThe VEDA Data Store is meant for\n\nNovel datasets produced by NASA Earth data scientists to be presented on the VEDA Dashboard or shared with other science users on VEDA, but not (yet) suited for publication in one of the EOSDIS Distributed Active Archive Centers (DAAC)\nNon-authoritative cloud-optimized versions of Datasets from a DAAC\nOther datasets that do not have an authoritative, cloud-optimized source, to be published or used within the VEDA platform",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Store"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-store.html#browsing-the-data-store",
    "href": "nasa-veda-platform/data-services/data-store.html#browsing-the-data-store",
    "title": "VEDA Data Store",
    "section": "Browsing the Data Store",
    "text": "Browsing the Data Store\nThe main public interface is the STAC browser and APIs that provide access to the data using various protocols.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Store"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-store.html#contributing-data",
    "href": "nasa-veda-platform/data-services/data-store.html#contributing-data",
    "title": "VEDA Data Store",
    "section": "Contributing data",
    "text": "Contributing data\nThe process of data ingestion into the VEDA Data Store is under active development.\nPlease see our docs on dataset ingestion.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Store"
    ]
  },
  {
    "objectID": "open-source-ecosystem/index.html",
    "href": "open-source-ecosystem/index.html",
    "title": "VEDA Open Source Ecosystem",
    "section": "",
    "text": "An explanation why we think VEDA platform software is needed\nAn overview of the VEDA software architecture\nOur project’s code repositories of redeployable software components\nA collection of external resources for learning and reference",
    "crumbs": [
      "VEDA Open Source Ecosystem"
    ]
  },
  {
    "objectID": "open-source-ecosystem/index.html#in-this-section-you-can-find",
    "href": "open-source-ecosystem/index.html#in-this-section-you-can-find",
    "title": "VEDA Open Source Ecosystem",
    "section": "",
    "text": "An explanation why we think VEDA platform software is needed\nAn overview of the VEDA software architecture\nOur project’s code repositories of redeployable software components\nA collection of external resources for learning and reference",
    "crumbs": [
      "VEDA Open Source Ecosystem"
    ]
  },
  {
    "objectID": "open-source-ecosystem/architecture.html",
    "href": "open-source-ecosystem/architecture.html",
    "title": "Software Architecture",
    "section": "",
    "text": "Architecture diagram\n\n\n\nThis diagram shows a high level overview of the components in the the VEDA Open-Source Ecosystem. See the repositories page for links to software used for VEDA services.",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "Software Architecture"
    ]
  },
  {
    "objectID": "open-source-ecosystem/architecture.html#architecture",
    "href": "open-source-ecosystem/architecture.html#architecture",
    "title": "Software Architecture",
    "section": "",
    "text": "Architecture diagram\n\n\n\nThis diagram shows a high level overview of the components in the the VEDA Open-Source Ecosystem. See the repositories page for links to software used for VEDA services.",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "Software Architecture"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html",
    "href": "open-source-ecosystem/repositories.html",
    "title": "VEDA Repositories",
    "section": "",
    "text": "All the relevant VEDA repositories are public and owned by the NASA-IMPACT GitHub organization.",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#repositories",
    "href": "open-source-ecosystem/repositories.html#repositories",
    "title": "VEDA Repositories",
    "section": "",
    "text": "All the relevant VEDA repositories are public and owned by the NASA-IMPACT GitHub organization.",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#data-services",
    "href": "open-source-ecosystem/repositories.html#data-services",
    "title": "VEDA Repositories",
    "section": "Data Services",
    "text": "Data Services\n\nveda-backend\nCentral index (database) and APIs for ingesting, discovering, viewing, and accessing VEDA assets\n\n\nveda-routes\nCloudfront Distribution, and DNS resources for all VEDA services\n\n\nveda-data-airflow\nSTAC metadata creation and ingestion using Apache Airflow\n\n\nveda-features-api-cdk\nHosting and serving collections of vector data features for VEDA\n\n\nveda-auth\nAuthentication service for VEDA systems\n\n\nveda-data-processing\nScripts for data downloading, transformation, and related processing for the VEDA project",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#veda-dashboard",
    "href": "open-source-ecosystem/repositories.html#veda-dashboard",
    "title": "VEDA Repositories",
    "section": "VEDA Dashboard",
    "text": "VEDA Dashboard\n\nveda-ui\nDashboard UI for viewing and analysing VEDA assets\n\n\nveda-config\nConfiguration for viewing VEDA assets in dashboard UI",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#veda-analytics-platform",
    "href": "open-source-ecosystem/repositories.html#veda-analytics-platform",
    "title": "VEDA Repositories",
    "section": "VEDA Analytics Platform",
    "text": "VEDA Analytics Platform\n\n2i2c-org/infrastructure\nInfrastructure for configuring and deploying community JupyterHubs by 2i2c",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#veda-documentation",
    "href": "open-source-ecosystem/repositories.html#veda-documentation",
    "title": "VEDA Repositories",
    "section": "VEDA Documentation",
    "text": "VEDA Documentation\n\nveda-docs\nProvides documentation and example notebooks for the various APIs, datasets and analytics platforms maintained by NASA VEDA project\n\n\nveda-architecture\nHosts architecture discussions and decisions made for various VEDA services",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/repositories.html#veda-machine-learning",
    "href": "open-source-ecosystem/repositories.html#veda-machine-learning",
    "title": "VEDA Repositories",
    "section": "VEDA Machine Learning",
    "text": "VEDA Machine Learning\n\nveda-ai-supraglacial_segmentation\nDetection of Supraglacial melts as a result of Climate Change",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "VEDA Repositories"
    ]
  },
  {
    "objectID": "open-source-ecosystem/external-resources.html",
    "href": "open-source-ecosystem/external-resources.html",
    "title": "External Resources",
    "section": "",
    "text": "This list is intended for scientists who want to get started with cloud-based, collaborative, open geospatial data analysis, or those looking to refresh their knowledge. It is by no means complete, but contains pointers to more elaborate resources. We anticipate this list to evolve as our platform and use cases evolve. Suggestions for additional resources or topics are highly welcome.\n\n\n\nGit - for managing code versions\nConda - for managing dependencies\nJupyter - for running code\n\n\n\n\n\nData Carpentry geospatial course\nUW course on Geospatial Data Analysis with Python\nGeographic data book - emphasis on vector data\n\n\n\n\n\nCloud-Optimized Geospatial Formats Guide\nNASA Openscapes Earthdata Cloud Cookbook\nCryoCloud JupyterBook\nProject Pythia - Pangeo’s education hub\nPlanetary Computer\nGeospatial Computing Platform library of training resources (by Python package)\n\n\n\n\n\nUW GeoHackWeek\nUW OceanHackWeek\n\n\n\n\n\nBlog series on latest trends and resources in geospatial - entry-level",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "External Resources"
    ]
  },
  {
    "objectID": "open-source-ecosystem/external-resources.html#learning-about-open-source-cloud-native-geospatial",
    "href": "open-source-ecosystem/external-resources.html#learning-about-open-source-cloud-native-geospatial",
    "title": "External Resources",
    "section": "",
    "text": "This list is intended for scientists who want to get started with cloud-based, collaborative, open geospatial data analysis, or those looking to refresh their knowledge. It is by no means complete, but contains pointers to more elaborate resources. We anticipate this list to evolve as our platform and use cases evolve. Suggestions for additional resources or topics are highly welcome.\n\n\n\nGit - for managing code versions\nConda - for managing dependencies\nJupyter - for running code\n\n\n\n\n\nData Carpentry geospatial course\nUW course on Geospatial Data Analysis with Python\nGeographic data book - emphasis on vector data\n\n\n\n\n\nCloud-Optimized Geospatial Formats Guide\nNASA Openscapes Earthdata Cloud Cookbook\nCryoCloud JupyterBook\nProject Pythia - Pangeo’s education hub\nPlanetary Computer\nGeospatial Computing Platform library of training resources (by Python package)\n\n\n\n\n\nUW GeoHackWeek\nUW OceanHackWeek\n\n\n\n\n\nBlog series on latest trends and resources in geospatial - entry-level",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "External Resources"
    ]
  },
  {
    "objectID": "open-source-ecosystem/external-resources.html#related-projects",
    "href": "open-source-ecosystem/external-resources.html#related-projects",
    "title": "External Resources",
    "section": "Related projects",
    "text": "Related projects\n\nMicrosoft Planetary Computer\nCopernicus Data Space Ecosystems\nEarth Observation Exploitation Platform Common Architecture (EOEPCA)\nEarth Observing Dashboard by ESA, NASA, and JAXA",
    "crumbs": [
      "VEDA Open Source Ecosystem",
      "External Resources"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-ingestion.html",
    "href": "nasa-veda-platform/data-services/data-ingestion.html",
    "title": "VEDA Data Ingestion Services",
    "section": "",
    "text": "VEDA’s data ingestion services are designed to handle and manage the flow of data from various sources efficiently. The system integrates with Apache Airflow for orchestrating and scheduling data pipelines, and APIs to facilitate and manage data ingestion tasks.\n\n\n\nApache Airflow Instance\n\nAirflow orchestrates various data ingestion tasks and workflows. It supports scheduling, monitoring, and managing pipelines, described using Directed Acyclic Graphs (DAGs).\nFor details on how to deploy, configure, or modify our Airflow instance, refer to the veda-data-airflow repository.\n\nIngest APIs\n\nThe Ingest API facilitates data ingestion from multiple sources and manages the data flow into VEDA’s system. This API is included in veda-backend\nTo learn how to interact with these APIs as a user, consult the Dataset Ingestion Guide.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Ingestion Services"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/data-ingestion.html#overview",
    "href": "nasa-veda-platform/data-services/data-ingestion.html#overview",
    "title": "VEDA Data Ingestion Services",
    "section": "",
    "text": "VEDA’s data ingestion services are designed to handle and manage the flow of data from various sources efficiently. The system integrates with Apache Airflow for orchestrating and scheduling data pipelines, and APIs to facilitate and manage data ingestion tasks.\n\n\n\nApache Airflow Instance\n\nAirflow orchestrates various data ingestion tasks and workflows. It supports scheduling, monitoring, and managing pipelines, described using Directed Acyclic Graphs (DAGs).\nFor details on how to deploy, configure, or modify our Airflow instance, refer to the veda-data-airflow repository.\n\nIngest APIs\n\nThe Ingest API facilitates data ingestion from multiple sources and manages the data flow into VEDA’s system. This API is included in veda-backend\nTo learn how to interact with these APIs as a user, consult the Dataset Ingestion Guide.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "VEDA Data Ingestion Services"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/apis/index.html",
    "href": "nasa-veda-platform/data-services/apis/index.html",
    "title": "APIs",
    "section": "",
    "text": "Most of the VEDA APIs are hosted out of a single project (veda-backend) that combines multiple standalone services.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "APIs"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/apis/index.html#environments",
    "href": "nasa-veda-platform/data-services/apis/index.html#environments",
    "title": "APIs",
    "section": "Environments",
    "text": "Environments\nWhile some of our services are already very mature, VEDA is currently in the build-up phase. Therefore, we do not yet have a production environment for users. Maintenance on the staging environment will be announced internally and selected known stakeholders will be informed of any larger changes.\n\n\n\n\n\n\nBase URL Changes\n\n\n\nThe VEDA Team is in the process of updating documentation notebooks to use the new stable production APIs. Currently most notebooks use deprecated staging APIs and should be updated:\nSTAC_API_URL = \"https://staging-stac.delta-backend.com\" –&gt; https://openveda.cloud/api/stac\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\" –&gt; https://openveda.cloud/api/raster\n\n\n\n\n\n\n\n\nBreaking tiling endpoint changes\n\n\n\nVEDA tilers are being upgraded to titiler-pgstac v1 which better aligns with the stac-api spec and has many improvements but also introduces some breaking endpoint changes. The VEDA team will be updating notebooks for the new endpoints but, in the meantime, the titiler-pgstac migration documentation provides support for using the new endpoints.\n\n\n\nProduction (stable):\n\nSTAC browser: openveda.cloud\nSTAC API (metadata): openveda.cloud/api/stac/docs\nList collections: openveda.cloud/api/stac/collections\nRaster API (tiling): openveda.cloud/api/raster/docs\n\n\n\nStaging (maintenance will be announced):\n\nSTAC browser: staging.openveda.cloud\nSTAC API (metadata): staging.openveda.cloud/api/stac/docs\nList collections: staging.openveda.cloud/api/stac/collections\nRaster API (map tiles and timeseries): staging.openveda.cloud/api/raster/docs\nFeatures API (vector data): firenrt.delta-backend.com - see also the usage tutorial\nWorkflows API (request airflow ingestions): staging.openveda.cloud/api/workflows/docs\nSTAC viewer (experimental): staging.openveda.cloud/api/stac/index.html",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "APIs"
    ]
  },
  {
    "objectID": "nasa-veda-platform/data-services/apis/index.html#using-tile-layers-in-external-services",
    "href": "nasa-veda-platform/data-services/apis/index.html#using-tile-layers-in-external-services",
    "title": "APIs",
    "section": "Using tile layers in external services",
    "text": "Using tile layers in external services\n\nUI for single tile layers\nAs you can see from our API docs referenced above, our raster API provides WMTS and XYZ tiles for public consumption.\nFor any layer you are seeing in the VEDA dataset Explorer, you can retrieve the tile URL:\n\n\n\nVEDA Dashboard Exploration API grab\n\n\nAnd paste that into any client that loads these tiles, like QGIS, ArcGIS, Leaflet, even online tools such as geojson.io or felt.com.\n\n\nSTAC for layer timeseries\nIf you want to integrate tile layer time series into your application, you will need to fetch the information about which time steps exist and what the layer URLs are from our Spatio Tempoeral Asset Catalog (STAC) API (see above).\nThat is because, unfortunately, neither XYZ nor WMTS have time series capabilities (unlike good old WMS, which our services do not provide, though).\nYou can see how to retrieve time steps and tile layer URLs from these tutorial Python notebooks (mostly REST API calls):\n\nUsing /stac/tilejson.json with STAC collection and item IDs\nCreating layers from filters and mosaics (advanced)\n\nIt comes down to querying for STAC items (timesteps) and then asking the Raster API for tilejson.json specifications for the items you are interested in.\nOnce you retrieved the WMTS or XYZ layer URLs this way, you can use them seamlessly with all mapping clients.",
    "crumbs": [
      "NASA VEDA Platform",
      "Data Services",
      "APIs"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/github-authentication.html",
    "href": "nasa-veda-platform/scientific-computing/github-authentication.html",
    "title": "How to push to GitHub from the VEDA JupyterHub",
    "section": "",
    "text": "This is a quick how-to guide for pushing to GitHub from the VEDA JupyterHub. There are several methods for GitHub Authentication, which are detailed in GitHub’s documentation. While some of those are applicable across all compute environments (e.g., personal access tokens), we recommend using gh-scoped-creds as a fine tuned GitHub authenticatin method designed for shared computing environments like HPCs and JupyterHubs.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to push to GitHub from the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/github-authentication.html#user-authentication-via-veda-hub-github-scoped-creds-app",
    "href": "nasa-veda-platform/scientific-computing/github-authentication.html#user-authentication-via-veda-hub-github-scoped-creds-app",
    "title": "How to push to GitHub from the VEDA JupyterHub",
    "section": "User authentication via veda-hub-github-scoped-creds app",
    "text": "User authentication via veda-hub-github-scoped-creds app\n\nCopy notebook for running gh-scoped-creds\nCopy the /home/jovyan/veda-docs/notebooks/templates/template-github-auth.ipynb notebook to your personal home directory on the Hub as github-auth.ipynb.\n\n\n\n\n\n\nTip\n\n\n\nYou only need to do this step once.\n\n\n\n\nAuthenticate to GitHub\n\n\n\n\n\n\nTip\n\n\n\nYou need to do this step each time you start a new JupyterHub instance.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis will only allow pushing to repositories that have been configured with the veda-hub-github-scoped-creds-app. While gh-scoped-creds is the recommend method for authenticating on VEDA, you can fallback on personal access tokens if necessary, while taking precautions to not store long-lived tokens directly on the Hub to avoid leaked credentials.\n\n\nThese steps are also shown in the GIF below.\n\nOpen the github-auth.ipynb in your home directory.\nRun the cell that contains import gh_scoped_creds followed by %ghscopedcreds.\nCopy the code that was displayed in the cell output.\nNavigate to https://github.com/login/device/select_account.\nSelect the account that you want to authorize push access for.\nEnter the code from the cell output in the Jupyter Notebook.\nSelect Authorize veda-hub-github-scoped-creds.\n\nNow, you’re all set! You should be able to push to the repositories that you configured in the first portion, as long as you’re using the https rather than ssh protocol. You can set this up by selecting https when cloning the repository, or using git remote set-url to change from ssh to https.\n\n\n\nRunning gh-scoped-creds",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to push to GitHub from the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/github-authentication.html#repository-admin-configuration-of-the-veda-hub-github-scoped-creds-app",
    "href": "nasa-veda-platform/scientific-computing/github-authentication.html#repository-admin-configuration-of-the-veda-hub-github-scoped-creds-app",
    "title": "How to push to GitHub from the VEDA JupyterHub",
    "section": "Repository admin configuration of the veda-hub-github-scoped-creds app",
    "text": "Repository admin configuration of the veda-hub-github-scoped-creds app\nRepository admins need to configure the veda-hub-github-scoped-creds app to allow users to push from the JupyterHub.\n\nNavigate to https://github.com/apps/veda-hub-github-scoped-creds.\nSelect Configure.\nSelect the organization that contains the repository that you would like to push to.\nSelect either All repositories or Only select repositories depending on whether you only want to allow pushing to specific repositories. In order for a Hub user to write to those repositories, the repository needs to be added to the application and the user needs to have read/write permissions for the repository. The following restrictions define whether you can install the application yourself or if you need to request permission:\n\nOrganization owners can install GitHub Apps for all repositories.\nRepository admins can install the application if they only grant access to repositories that they administer.\nIf you do not have sufficient permissions, GitHub will send a notification to the organization owner requesting that they install the app.\n\nClick save.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to push to GitHub from the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/code-server.html",
    "href": "nasa-veda-platform/scientific-computing/code-server.html",
    "title": "How to use VS Code on the VEDA JupyterHub",
    "section": "",
    "text": "This is a quick how-to guide for using VS Code as an IDE on the VEDA JupyterHub.\n\n\n\n\n\n\nWarning\n\n\n\nWe are exploring two options for using VS Code on the VEDA JupyterHub. These options should be considered experimental and may not be supported long-term.\n\n\n\n\n\nAccess the launcher from the home screen by clicking the + tab if it is not already open.\nClick the “VS Code” icon on the top row. This will open a new browser tab with the VS Code interface:\n\n\n\nVS Code icon on the Launcher page\n\n\nOpen the specific folder that you want to work in:\n\nClick the three lines on the upper left (a.k.a., hamburger symbol)\nSelect ‘File’\nSelect ‘Open Folder…’\nNavigate to the appropriate folder\nClick ‘OK’\n\n\n\n\nProcess for selecting the workspace folder\n\n\nSelect that you trust the repository to enable all VS Code features:\n\n\n\nProcess for trusting the workspace folder\n\n\n\nNow, you have access to VS Code on the VEDA Hub and are ready to get started on your development!\n\n\n\n\n\n\nWarning\n\n\n\nUsing the full set of VS Code features requires trusting the folders, which should only be done for individual, known/trusted repositories rather than the entire JupyterHub home directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs an alternative to step 2, you can type the path to your folder after ?folder= in the browser URL and press enter.\n\n\n\n\n\n\n\n\nTip\n\n\n\nMost times, you will want to select a folder that is a git repository, perhaps cloned from GitHub, so that your code is version controlled.\n\n\n\n\n\nThe VEDA JupyterHub comes with several VS Code extensions pre-installed. However, you may need access to others that are not installed. For example, the ms-python Python extension is required for debugging Python code and isn’t curently installed by default.\n\nNavigate to the Extensions view by clicking on the icon with four boxes and one slightly offset:\n\n\n\nExtensions icon in VS Code\n\n\nThe view shows the already installed extensions. Type the name of the extension you’re looking for in the ‘Search Extensions’ text box:\n\n\n\nInstall VS Code extension\n\n\nClick install.\n\n\n\n\n\n\n\nNote\n\n\n\nThe extensions will be installed for the duration of your session. For persistant access to VS Code extensions, raise an issue in VEDA Hub docker image repository.\n\n\n\n\n\n\n\n\nSelect the Run view in the sidebar:\n\n\n\n\nVS Code Run View\n\n\n\nClick on “create a launch.json file”:\n\n\n\n\nCreate a launch.json file\n\n\n\nSelect “Python Debugger”.\nSelect “Python File: Debug the currently active Python file”.\n\n\n\n\n\n\n\nNote\n\n\n\nCheck out the Visual Studio Code Debugging docs for additional configuration options. One of the more useful controls is setting \"justMyCode\": false to debug imported libraries.\n\n\n\n\n\n\nOpen the Python file that you want to debug\nClick to the left of a line number to add a breakpoint\n\n\n\n\nAdd a breakpoint\n\n\n\nFrom the Run view, click on the green triangle to start debugging\n\n\n\n\nStart debugger\n\n\n\nUse the debugger controls to continue, restart, stop, step over, step in, or step out of functions.\n\n\n\n\nDebugger control\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdd the .vscode folder to your .gitignore and put ephemeral testing modules in their for debugging without poluting the git workspace.\n\n\n\n\n\n\nSeveral images in this section are from Microsoft’s public documentation for Visual Studio Code. Images are used under the terms of the under the Creative Commons Attribution 3.0 United States License.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to use VS Code on the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/code-server.html#launch-vscode",
    "href": "nasa-veda-platform/scientific-computing/code-server.html#launch-vscode",
    "title": "How to use VS Code on the VEDA JupyterHub",
    "section": "",
    "text": "Access the launcher from the home screen by clicking the + tab if it is not already open.\nClick the “VS Code” icon on the top row. This will open a new browser tab with the VS Code interface:\n\n\n\nVS Code icon on the Launcher page\n\n\nOpen the specific folder that you want to work in:\n\nClick the three lines on the upper left (a.k.a., hamburger symbol)\nSelect ‘File’\nSelect ‘Open Folder…’\nNavigate to the appropriate folder\nClick ‘OK’\n\n\n\n\nProcess for selecting the workspace folder\n\n\nSelect that you trust the repository to enable all VS Code features:\n\n\n\nProcess for trusting the workspace folder\n\n\n\nNow, you have access to VS Code on the VEDA Hub and are ready to get started on your development!\n\n\n\n\n\n\nWarning\n\n\n\nUsing the full set of VS Code features requires trusting the folders, which should only be done for individual, known/trusted repositories rather than the entire JupyterHub home directory.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs an alternative to step 2, you can type the path to your folder after ?folder= in the browser URL and press enter.\n\n\n\n\n\n\n\n\nTip\n\n\n\nMost times, you will want to select a folder that is a git repository, perhaps cloned from GitHub, so that your code is version controlled.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to use VS Code on the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/code-server.html#installing-extensions",
    "href": "nasa-veda-platform/scientific-computing/code-server.html#installing-extensions",
    "title": "How to use VS Code on the VEDA JupyterHub",
    "section": "",
    "text": "The VEDA JupyterHub comes with several VS Code extensions pre-installed. However, you may need access to others that are not installed. For example, the ms-python Python extension is required for debugging Python code and isn’t curently installed by default.\n\nNavigate to the Extensions view by clicking on the icon with four boxes and one slightly offset:\n\n\n\nExtensions icon in VS Code\n\n\nThe view shows the already installed extensions. Type the name of the extension you’re looking for in the ‘Search Extensions’ text box:\n\n\n\nInstall VS Code extension\n\n\nClick install.\n\n\n\n\n\n\n\nNote\n\n\n\nThe extensions will be installed for the duration of your session. For persistant access to VS Code extensions, raise an issue in VEDA Hub docker image repository.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to use VS Code on the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/code-server.html#debugging-python-code",
    "href": "nasa-veda-platform/scientific-computing/code-server.html#debugging-python-code",
    "title": "How to use VS Code on the VEDA JupyterHub",
    "section": "",
    "text": "Select the Run view in the sidebar:\n\n\n\n\nVS Code Run View\n\n\n\nClick on “create a launch.json file”:\n\n\n\n\nCreate a launch.json file\n\n\n\nSelect “Python Debugger”.\nSelect “Python File: Debug the currently active Python file”.\n\n\n\n\n\n\n\nNote\n\n\n\nCheck out the Visual Studio Code Debugging docs for additional configuration options. One of the more useful controls is setting \"justMyCode\": false to debug imported libraries.\n\n\n\n\n\n\nOpen the Python file that you want to debug\nClick to the left of a line number to add a breakpoint\n\n\n\n\nAdd a breakpoint\n\n\n\nFrom the Run view, click on the green triangle to start debugging\n\n\n\n\nStart debugger\n\n\n\nUse the debugger controls to continue, restart, stop, step over, step in, or step out of functions.\n\n\n\n\nDebugger control\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdd the .vscode folder to your .gitignore and put ephemeral testing modules in their for debugging without poluting the git workspace.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to use VS Code on the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/scientific-computing/code-server.html#acknowledgments",
    "href": "nasa-veda-platform/scientific-computing/code-server.html#acknowledgments",
    "title": "How to use VS Code on the VEDA JupyterHub",
    "section": "",
    "text": "Several images in this section are from Microsoft’s public documentation for Visual Studio Code. Images are used under the terms of the under the Creative Commons Attribution 3.0 United States License.",
    "crumbs": [
      "NASA VEDA Platform",
      "Scientific Computing",
      "How to use VS Code on the VEDA JupyterHub"
    ]
  },
  {
    "objectID": "nasa-veda-platform/getting-access.html",
    "href": "nasa-veda-platform/getting-access.html",
    "title": "Getting access",
    "section": "",
    "text": "VEDA data service APIs are open to free and fair use. For access to other resources, please see the sections below.",
    "crumbs": [
      "NASA VEDA Platform",
      "Getting access"
    ]
  },
  {
    "objectID": "nasa-veda-platform/getting-access.html#veda-interactive-compute-and-processing-environment",
    "href": "nasa-veda-platform/getting-access.html#veda-interactive-compute-and-processing-environment",
    "title": "Getting access",
    "section": "VEDA Interactive Compute and Processing Environment",
    "text": "VEDA Interactive Compute and Processing Environment\nAccess to the VEDA compute and processing environment is currently on an as-need basis. If you are a user afficiliated with VEDA, you can gain access by following these steps:\n\nMake sure you have a GitHub Account. Take note of your GitHub username\nSend an email to the VEDA team (veda@uah.edu) asking for access to the VEDA notebook environment. Please include your GitHub username. They will invite you through GitHub to join the VEDA Analytics GitHub Team. Please watch your email for the invite.\nOnce you accepted the invitation, you should be able to go to https://hub.openveda.cloud/ and login via your GitHub credentials.",
    "crumbs": [
      "NASA VEDA Platform",
      "Getting access"
    ]
  },
  {
    "objectID": "nasa-veda-platform/about/index.html",
    "href": "nasa-veda-platform/about/index.html",
    "title": "About the VEDA project",
    "section": "",
    "text": "Visualization Exploration and Data Analysis (VEDA) is an ecosystem of tools and services built with the goal to provide a redeployable and unified open-source science cyberinfrastructure for data processing, visualization, exploration, and GIS capabilities with NASA data on the cloud.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.",
    "crumbs": [
      "NASA VEDA Platform",
      "About the VEDA project"
    ]
  },
  {
    "objectID": "nasa-veda-platform/about/index.html#about",
    "href": "nasa-veda-platform/about/index.html#about",
    "title": "About the VEDA project",
    "section": "",
    "text": "Visualization Exploration and Data Analysis (VEDA) is an ecosystem of tools and services built with the goal to provide a redeployable and unified open-source science cyberinfrastructure for data processing, visualization, exploration, and GIS capabilities with NASA data on the cloud.\nDeveloped through a collaboration between NASA IMPACT, Development Seed, University of Alabama in Huntsville, Element 84, Indiana University, International Interactive Computing Collaboration (2i2c), Earth Science Data and Information System (ESDIS) Project, NASA Science Managed Cloud Environment (SMCE), and NASA Mission Cloud Platform (MCP), VEDA significantly reduces the barriers to accessing Earth science data and the computational resources needed for exploring and processing the petabyte-scale Earth data archives in the cloud. VEDA’s achievement exemplifies the core principles of NASA’s Open-Source Science Initiative (OSSI), showcasing commitment to promoting transparent, accessible, and collaborative scientific research.\nRead more about the history of VEDA in this blog post.",
    "crumbs": [
      "NASA VEDA Platform",
      "About the VEDA project"
    ]
  },
  {
    "objectID": "nasa-veda-platform/about/index.html#goals",
    "href": "nasa-veda-platform/about/index.html#goals",
    "title": "About the VEDA project",
    "section": "Goals",
    "text": "Goals\n\nLeverage existing open source components (NASA-MAAP, Dashboard, Cumulus, Pangeo, STAC, etc) to assemble interoperable ecosystem of tools\nMaximize the capabilities offered by data and compute on the cloud\nProvide services for future mission data processing\nProvide a platform for new priorities and directives: NASA’s Earth Action, NASA’s Earth Science Data Systems (ESDS) Program, and other priority initiatives\n\n\nStakeholder Benefits\n\nGeneral Public: Interactive stories\nScientist: scientific (visual) analysis and communication\nData producers/Researchers: processing and self publishing\nTOPS: summer schools and competitions\nApplied science/Decision makers: geospatial analysis and communication\nCommunication: Interactive storytelling\n\n\n\nOverview\n\n\n\nVEDA Overview Diagram",
    "crumbs": [
      "NASA VEDA Platform",
      "About the VEDA project"
    ]
  },
  {
    "objectID": "instance-management/adding-content/docs-and-notebooks.html",
    "href": "instance-management/adding-content/docs-and-notebooks.html",
    "title": "Usage Example Notebook Submission",
    "section": "",
    "text": "Contribution to VEDA’s documentation is always welcome - just open a Pull Request on the veda-docs repository.\nYou can submit a PR by forking the repository and submitting a PR with your fork. However, PR previews will not work for PRs from forks. You can push directly to this repository by becoming a collaborator. If you are not already a collaborator of the veda-docs repository, please email your github handle and veda@uah.edu along with a message like “please add me as a collaborator to the veda-docs repository so I can push a branch”. If you are not someone already familiar with the VEDA team, please add some additional information about your interest in contributing to the documentation.\nOnce you are a collaborator, you will be able to submit a PR from a branch of this repository (that is, not a branch from a fork) and PR previews will help in the review process.\nPlease note that this documentation site is rendered using Quarto, which adds a small set of configuration options on top of vanilla Markdown and Jupyter Notebooks."
  },
  {
    "objectID": "instance-management/adding-content/docs-and-notebooks.html#notebook-author-guidelines",
    "href": "instance-management/adding-content/docs-and-notebooks.html#notebook-author-guidelines",
    "title": "Usage Example Notebook Submission",
    "section": "Notebook Author Guidelines",
    "text": "Notebook Author Guidelines\nThere are two template notebooks in this section that you can use as a starting place. Alternatively you can pull specific cells from that notebook into your own.\n\nUsing the raster API: template-using-the-raster-api.ipynb\nAccessing the data directly: template-accessing-the-data-directly.ipynb\n\n\nStyle\n\nEach code cell should come after a markdown cell with some explanatory text. This is preferred over comments in the code cells.\nThe max header should be ##.\nOnly include imports that are needed for the notebook to run.\nWe don’t enforce any formatting, but periodically run black on all the notebooks. We also encourage you to run black on your notebooks via pre-commit, following the instructions below on automatic linting with pre-commit.\n\n\nAutomatic linting with pre-commit\nThis repository is configured with pre-commit, which can automatically lint your notebooks anytime you git commit changes. Follow these steps to enable pre-commit (you only need to do this once):\n\nInstall the pre-commit library.\nOpen a terminal.\nNavigate to the root of the veda-docs repository.\nRun pre-commit install.\n\n\n\n\nRendering information\nThe first cell in every notebook is a raw cell that contains the following metadata for rendering with our site builder Quarto.\n---\ntitle: Short title\ndescription: One sentence description\nauthor: Author Name\ndate: May 2, 2023\nexecute:\n  freeze: true\n---\n\n\nRunning notebooks\nWe store evaluated notebooks in this repository. So before you commit your notebook, you should restart your kernel and run all cells in order.\nNormally we run the notebooks on VEDA JupyterHub.\nTo run the notebooks with a new image, use the JupyterHub image selection interface and under “Custom Image” type in the address to the public ecr image with the full tag sha.\nSomething like: public.ecr.aws/nasa-veda/pangeo-notebook:60b023fba2ca5f9e19d285c245987e368e27c0ea626b65777b204cec14b697c7\n\n\nStandard sections\nTo give the notebooks a standard look and feel we typically include the following sections:\n\nRun this Notebook: The section explains how to run the notebook locally, on VEDA JupyterHub or on mybinder. There are several examples of what this section can look like in the template notebooks.\nApproach: List a few steps that outline the approach you be taking in this notebook.\nAbout the data: Optional description of the dataset\nDeclare your collection of interest: This section reiterates how you can discover which collections are available. You can copy the example of this section from one of the template notebooks.\n\nFrom then on the standard sections diverge depending on whether the notebook access the data directly or uses the raster API. Check the template notebooks for some ideas of common patterns.\n\n\nUsing complex geometries\nIf you are defining the AOI using a bounding box, you can include it in the text of the notebook, but for more complex geometries we prefer that the notebook access the geometry directly from a canonical source. You can check the template notebooks for examples of this. If the complex geometry is not available online the VEDA team can help get it up in a public s3 bucket.\n\n\nRecommended libraries\n\nMapping + Visualization\n\nfolium: folium adds Leaflet.js support to python projects for visualizing data in a map.\nholoviz: High-level tools that make it easier to apply Python plotting libraries to your data.\nipyleaflet: Interactive maps in the Jupyter notebook. ipyleaflet is built on ipywidgets allowing for bidirectional communication between front- and backends (learn more: Interactive GIS in Jupyter with ipyleaflet).\n\n\n\nUsing STAC for cataloging data\nTo present consistent best practices, we always access data via the STAC API.\n\npystac: PySTAC is a library for creating SpatioTemporal Asset Catalogs (STAC) in Python 3.\npystac-client: A Python client for working with STAC Catalogs and APIs.\n\n\n\nAnalyzing data\n\nrioxarray: rasterio xarray extension\nstackstac: stackstac.stack turns a STAC collection into a lazy xarray.DataArray, backed by dask.\n\n\n\n\nGenerate “Launch in VEDA JupyterHub” link\nWe use nbgitpuller links to open the VEDA JupyterHub with a particular notebook pulled in. These links have the form: https://hub.openveda.cloud/hub/user-redirect/git-pull?repo=https://github.com/NASA-IMPACT/veda-docs&urlpath=lab/tree/veda-docs/notebooks/quickstarts/open-and-plot.ipynb&branch=main\nIf you are writing a notebook and want to share it with others you can generate your own nbgitpuller link using this link generator."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html",
    "href": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html",
    "title": "STAC item conventions",
    "section": "",
    "text": "Copied from veda-backend#28\nThis document defines a set of conventions for generating STAC Items consistently for the VEDA Dashboard UI and future API users. Ultimately, these represent the minimum metadata API users can expect from the backend."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "href": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#rio-stac-conventions-for-generating-stac-items",
    "title": "STAC item conventions",
    "section": "Rio-stac conventions for generating STAC Items",
    "text": "Rio-stac conventions for generating STAC Items\nAll of our current ingestion plans will use rio-stac to generate item metadata for COGs so the notes below are organized around the input parameters of the create_stac_item method.\nexample rio-stac python usage\nitem = rio_stac.stac.create_stac_item(\n  id = item_id,\n  source = f\"s3://{obj.bucket_name}/{obj.key}\", \n  collection = collection_id, \n  input_datetime = &lt;datetime.datetime&gt;,\n  with_proj = True,\n  with_raster = True,\n  asset_name = \"cog_default\",\n  asset_roles = [\"data\", \"layer\"],\n  asset_media_type = \"image/tiff; application=geotiff; profile=cloud-optimized\",\n)\nRio-stac create item parameter recommendations These recommendations are for generating STAC Item metadata for collections intended for the dasboard and may not be applicable to all ARCO collections.\n\n\n\nParameter\nRecommendations\n\n\n\n\nid\n(1) When STAC Item metadata is generated from a COG file, strip the full file extension from the filename for the item id. (2) When ids are not unique across collections, append the collection id to the item id. For example the no2-monthly and no2-monthly-diff COGs are stored with unique bucket prefixes but within the prefix all the filenames are the same, so the collection id is appended: OMI_trno2_0.10x0.10_201604_Col3_V4 → OMI_trno2_0.10x0.10_201604_Col3_V4-no2-monthly).\n\n\nwith_proj\nTrue. Generate projection extension metadata for the item for future ARCO datastore users.\n\n\nwith_raster\nTrue. This will generate gdal statistics for every band in the COG—we use these to get the range of values for the full collection.\n\n\nasset_name\nA meaningful asset name for the default cloud optimized asset to be displayed on a map. cog_default is a placeholder—we need to choose and commit to an asset name for all collections. If not set, will default to asset. * TODO Decision: For items with many assets we should ingest all with appropriate keys and duplicate one preferred display asset as the default cog. We should be considering metadata conventions in pgstac-titiler\n\n\nasset_roles\n[\"data\", \"layer\"] data is an appropriate role, we may also choose to add something like layer to indicate that the asset is optimized to be used as a map layer (stac specification for asset roles).\n\n\nasset_media_type\n\"image/tiff; application=geotiff; profile=cloud-optimized (stac best practices for asset media type).\n\n\nproperties\nCMIP6: TODO, CMR: TODO if we don’t store links to the original data, downstream users are not going to be able to pair STAC records with the versioned parent data in CMR"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "href": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#data-provenance-convention",
    "title": "STAC item conventions",
    "section": "Data provenance convention",
    "text": "Data provenance convention\nWhen adding STAC items that were derived from previously published data (such as CMR records), there are multiple ways to preserve the linkage between the item and the more complete source metadata. We should provide at a minimum metadata assets for any items derived from previously published data. Here are three examples from HLS:\nmetadata are assets The CMR properties question in the table above (how to refer the STAC Item to it’s CMR source metadata) could instead be solved by adding a metadata asset. This does not require creating a new extension for CMR, it just involves creating an asset from the CMR granule metadata which should be in the event context for CMR search driven ingests. The example below is from documentation for using HLS cloud optimized data.\n\"assets\": {\n  \"metadata\": {\n    \"href\": \"https://cmr.earthdata.nasa.gov/search/concepts/G2099379244-LPCLOUD.xml\",\n    \"type\": \"application/xml\"\n    },\n    \"thumbnail\": { ...}\n}\nstac-spec scieintific extension\n\"properties\": {\n   \"sci:doi\": \"10.5067/HLS/HLSS30.002\",\n   ...\n}\nItem links to metadata Use a cite-as Item link to the DOI for the source data.\n\"links\": [\n  {\n    \"rel\": \"cite-as\",\n    \"href\": \"https://doi.org/10.5067/HLS/HLSS30.002\"\n  },\n  ...\n]"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "href": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#stac-item-validation-convention",
    "title": "STAC item conventions",
    "section": "STAC Item validation convention",
    "text": "STAC Item validation convention\nWe are producing pystac.items with rio-stac’s create_stac_item method and we should validate them before publishing them to s3. Testing found that it is possible to produce structurally sound but invalid STAC Items with create_stac_item.\nThe built in pystac validator on the pystac.item returned by create_stac_item can be used to easily validate the metadata—item.validate() will raise an exception for invalid metadata. Pystac does need to be installed with the appropriate dependencies for validation."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "href": "instance-management/adding-content/dataset-ingestion/stac-item-conventions.html#convention-for-default-map-layer-assets-for-spectral-data",
    "title": "STAC item conventions",
    "section": "Convention for default map layer assets for spectral data",
    "text": "Convention for default map layer assets for spectral data\nMany of the collections for the dashboard have a clear default map layer asset that we can name cog_default. This convention does not map as well to spectral data with many assets (B01, B02,…). A preferred band asset could be duplicated to define a default map layer asset to be consistent but this needs to be decided."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html",
    "href": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html",
    "title": "Ingestion Workflow for Uploading Data to the VEDA Catalog for the VEDA Dashboard",
    "section": "",
    "text": "This notebook is intented to be used as a reference for data providers who want to add new datasets to the VEDA Dashboard. As always it is important that the data provider has read the documentation for Data Ingestion before moving forward with this notebook example.\nFor example purposes, we will walk the end user through adding the GEOGLAM June 2023 dataset directly to the VEDA Dashboard.\n\nValidate the GeoTIFF\nUpload the file to the staging S3 bucket (veda-data-store-staging)\nUse the workflows-api (staging.openveda.cloud/api/workflows/docs) to generate STAC metadata for the file and add to the staging STAC catalog (staging.openveda.cloud)\n\nWhen the data has been published to the STAC metadata catalog for this geoglam collection, which is already configured for the dashboard, it will be available in the VEDA Dashboard\n\n\nBelow we will import some geospatial tools for validation and define some of the variables to be used including the TARGET_FILENAME for the datafile you want to upload. Note that in this example we will demonstrate the ingestion of GEOGLAM’s June 2023 data. It is important that the file you want to upload (e.g., CropMonitor_2023_06_28.tif ) is located in the same repository folder as this notebook.\n\nimport os\n\nimport rio_cogeo\nimport rasterio\nimport boto3\nimport requests\n\nIn the cell below we are using TARGET_FILENAME to revise the LOCAL_FILE_PATH into the correct file format as advised in the File preparation documentation. See example formats in the link provided.\nIf the LOCAL_FILE_PATH is already properly formatted, then both LOCAL_FILE_PATH and TARGET_FILENAME will be identical.\n\nLOCAL_FILE_PATH = \"CropMonitor_2023_06_28.tif\"\nYEAR, MONTH = 2023, 6\n\nTARGET_FILENAME = f\"CropMonitor_{YEAR}{MONTH:02}.tif\"\n\nThe following code is used to test whether the data format you are planning to upload is Cloud Optimized GeoTiff (COG) that enables more efficient workflows in the cloud environment. If the validation process identifies that it is not a COG, it will convert it into one.\n\nfile_is_a_cog = rio_cogeo.cog_validate(LOCAL_FILE_PATH)\nif not file_is_a_cog:\n    raise ValueError()\n    print(\"File is not a COG - converting\")\n    rio_cogeo.cog_translate(LOCAL_FILE_PATH, LOCAL_FILE_PATH, in_memory=True)\n\n\n\n\nThe following code will upload your COG data into veda-data-store-staging bucket. It will use the TARGET_FILENAME to assign the correct month and year values we have provided earlier in this notebook, under the geoglam bucket on S3.\n\ns3 = boto3.client(\"s3\")\nBUCKET = \"veda-data-store-staging\"\nKEY = f\"{BUCKET}/geoglam/{TARGET_FILENAME}\"\nS3_FILE_LOCATION = f\"s3://{KEY}\"\n\nif False:\n    s3.upload_file(LOCAL_FILE_PATH, KEY)\n\n\n\n\nFor this step, open the workflows API at staging.openveda.cloud/api/workflows/docs in a second browser tab and click the green authorize button at the upper right to authenticate your session with your username and password (you will be temporarily redirected to a login widget and then back to the workflows-api docs). The cells below will guide you through the process of configuiring your request jsons for each endpoint demonstrated and you will copy the cell outputs into the workflows API in your second tab.\n\n\nHere the data provider will construct the dataset definition (and supporting metadata) that will be used for dataset ingestion. It is imperative that these values are correct and align to the data the provider is planning to upload to the VEDA Platform. For example, make sure that the startdate and enddate are realistic (e.g., an \"enddate\":\"2023-06-31T23:59:59Z\" would be an incorrect value for June, as it contains only 31 days).\nFor further detail on metadata required for entries in the VEDA STAC to work with the VEDA Dashboard, see documentation here. In particular, note recommendations for the fields is_periodic and time_density. For example, in the code block below we define the is_periodic field as False because we are ingesting only one month of data. Even though we know that the monthly observations are provided routinely by GEOGLAM, we will only have a single file to ingest and so do not have a temporal range of items in the collection with a monthly time density to generate a time picker from the available data.\n\nNote Several OPTIONAL properties are added to this dataset config for completeness. Your dataset json does NOT need to include these optional properties * assets * item_assets * renders\n\n\nimport json\n\ndataset = {\n    \"collection\": \"geoglam\",\n    \"title\": \"GEOGLAM Crop Monitor\",\n    \"data_type\": \"cog\",\n    \"spatial_extent\": {\n    \"xmin\": -180,\n    \"ymin\": -90,\n    \"xmax\": 180,\n    \"ymax\": 90\n    },\n    \"temporal_extent\": {\n    \"startdate\": \"2020-01-01T00:00:00Z\",\n    \"enddate\": \"2023-06-30T23:59:59Z\"\n    },\n    \"license\": \"MIT\",\n    \"description\": \"The Crop Monitors were designed to provide a public good of open, timely, science-driven information on crop conditions in support of market transparency for the G20 Agricultural Market Information System (AMIS). Reflecting an international, multi-source, consensus assessment of crop growing conditions, status, and agro-climatic factors likely to impact global production, focusing on the major producing and trading countries for the four primary crops monitored by AMIS (wheat, maize, rice, and soybeans). The Crop Monitor for AMIS brings together over 40 partners from national, regional (i.e. sub-continental), and global monitoring systems, space agencies, agriculture organizations and universities. Read more: https://cropmonitor.org/index.php/about/aboutus/\",\n    \"is_periodic\": False,\n    \"time_density\": \"month\",\n    ## NOTE: email the veda team at veda@uah.edu to upload a new thumbnail for your dataset\n    \"assets\": {\n        \"thumbnail\": {\n            \"href\": \"https://thumbnails.openveda.cloud/geoglam--dataset-cover.jpg\",\n            \"type\": \"image/jpeg\",\n            \"roles\": [\"thumbnail\"],\n            \"title\": \"Thumbnail\",\n            \"description\": \"Photo by [Jean Wimmerlin](https://unsplash.com/photos/RUj5b4YXaHE) (Bird's eye view of fields)\"\n        }\n    },\n    ## RENDERS metadata are OPTIONAL but provided below\n    \"renders\": {\n        \"dashboard\": {\n            \"bidx\": [1],\n            \"title\": \"VEDA Dashboard Render Parameters\",\n            \"assets\": [\n            \"cog_default\"\n            ],\n            \"unscale\": False,\n            \"colormap\": {\n                \"1\": [120, 120, 120],\n                \"2\": [130, 65, 0],\n                \"3\": [66, 207, 56],\n                \"4\": [245, 239, 0],\n                \"5\": [241, 89, 32],\n                \"6\": [168, 0, 0],\n                \"7\": [0, 143, 201]\n            },\n            \"max_size\": 1024,\n            \"resampling\": \"nearest\",\n            \"return_mask\": True\n        }\n    },\n    ## IMPORTANT update providers for a your data, some are specific to each collection\n    \"providers\": [\n    {\n        \"url\": \"https://data.nal.usda.gov/dataset/geoglam-geo-global-agricultural-monitoring-crop-assessment-tool#:~:text=The%20GEOGLAM%20crop%20calendars%20are,USDA%20FAS%2C%20and%20USDA%20NASS.\",\n        \"name\": \"USDA & Global Crop Monitor Group partners\",\n        \"roles\": [\n            \"producer\",\n            \"processor\",\n            \"licensor\"\n        ]\n    },\n        {\n            \"url\": \"https://www.earthdata.nasa.gov/dashboard/\",\n            \"name\": \"NASA VEDA\",\n            \"roles\": [\"host\"]\n        }\n    ],\n    ## item_assets are OPTIONAL but pre-filled here\n    \"item_assets\": {\n        \"cog_default\": {\n            \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n                \"roles\": [\"data\",\"layer\"],\n            \"title\": \"Default COG Layer\",\n            \"description\": \"Cloud optimized default layer to display on map\"\n        }\n    },\n    \"sample_files\": [\n        \"s3://veda-data-store-staging/geoglam/CropMonitor_202306.tif\"\n    ],\n    \"discovery_items\": [\n        {\n          \"discovery\": \"s3\",\n          \"prefix\": \"geoglam/\",\n          \"bucket\": \"veda-data-store-staging\",\n          \"filename_regex\": \"(.*)CropMonitor_202306.tif$\"\n        }\n    ]\n}\n\nprint(json.dumps(dataset, indent=2))\n\n{\n  \"collection\": \"geoglam\",\n  \"title\": \"GEOGLAM Crop Monitor\",\n  \"data_type\": \"cog\",\n  \"spatial_extent\": {\n    \"xmin\": -180,\n    \"ymin\": -90,\n    \"xmax\": 180,\n    \"ymax\": 90\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2020-01-01T00:00:00Z\",\n    \"enddate\": \"2023-06-30T23:59:59Z\"\n  },\n  \"license\": \"MIT\",\n  \"description\": \"The Crop Monitors were designed to provide a public good of open, timely, science-driven information on crop conditions in support of market transparency for the G20 Agricultural Market Information System (AMIS). Reflecting an international, multi-source, consensus assessment of crop growing conditions, status, and agro-climatic factors likely to impact global production, focusing on the major producing and trading countries for the four primary crops monitored by AMIS (wheat, maize, rice, and soybeans). The Crop Monitor for AMIS brings together over 40 partners from national, regional (i.e. sub-continental), and global monitoring systems, space agencies, agriculture organizations and universities. Read more: https://cropmonitor.org/index.php/about/aboutus/\",\n  \"is_periodic\": false,\n  \"time_density\": \"month\",\n  \"assets\": {\n    \"thumbnail\": {\n      \"href\": \"https://thumbnails.openveda.cloud/geoglam--dataset-cover.jpg\",\n      \"type\": \"image/jpeg\",\n      \"roles\": [\n        \"thumbnail\"\n      ],\n      \"title\": \"Thumbnail\",\n      \"description\": \"Photo by [Jean Wimmerlin](https://unsplash.com/photos/RUj5b4YXaHE) (Bird's eye view of fields)\"\n    }\n  },\n  \"renders\": {\n    \"dashboard\": {\n      \"bidx\": [\n        1\n      ],\n      \"title\": \"VEDA Dashboard Render Parameters\",\n      \"assets\": [\n        \"cog_default\"\n      ],\n      \"unscale\": false,\n      \"colormap\": {\n        \"1\": [\n          120,\n          120,\n          120\n        ],\n        \"2\": [\n          130,\n          65,\n          0\n        ],\n        \"3\": [\n          66,\n          207,\n          56\n        ],\n        \"4\": [\n          245,\n          239,\n          0\n        ],\n        \"5\": [\n          241,\n          89,\n          32\n        ],\n        \"6\": [\n          168,\n          0,\n          0\n        ],\n        \"7\": [\n          0,\n          143,\n          201\n        ]\n      },\n      \"max_size\": 1024,\n      \"resampling\": \"nearest\",\n      \"return_mask\": true\n    }\n  },\n  \"providers\": [\n    {\n      \"url\": \"https://data.nal.usda.gov/dataset/geoglam-geo-global-agricultural-monitoring-crop-assessment-tool#:~:text=The%20GEOGLAM%20crop%20calendars%20are,USDA%20FAS%2C%20and%20USDA%20NASS.\",\n      \"name\": \"USDA & Global Crop Monitor Group partners\",\n      \"roles\": [\n        \"producer\",\n        \"processor\",\n        \"licensor\"\n      ]\n    },\n    {\n      \"url\": \"https://www.earthdata.nasa.gov/dashboard/\",\n      \"name\": \"NASA VEDA\",\n      \"roles\": [\n        \"host\"\n      ]\n    }\n  ],\n  \"item_assets\": {\n    \"cog_default\": {\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"roles\": [\n        \"data\",\n        \"layer\"\n      ],\n      \"title\": \"Default COG Layer\",\n      \"description\": \"Cloud optimized default layer to display on map\"\n    }\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/geoglam/CropMonitor_202306.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"prefix\": \"geoglam/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)CropMonitor_202306.tif$\"\n    }\n  ]\n}\n\n\n\n\n\nAfter composing your dataset definition, copy the printed json and paste it into the /dataset/validate input in the workflows-api docs page in the second tab. Note that if you navigate away from this page you will need to click authorize again.\nChoose POST dataset/validate in the Dataset section of the API docs at staging.openveda.cloud/api/workflows/docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nIf the json is valid, the response will confirm that it is ready to be published on the VEDA Platform.\n\n\n\nNow that you have validated your dataset, you can initiate a workflow and publish the dataset to the VEDA Platform.\nChoose POST dataset/publish in the Dataset section of the API docs at staging.openveda.cloud/api/workflows/docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nOn success, you will recieve a success message containing the id of your workflow, for example\n{\"message\":\"Successfully published collection: geoglam. 1  workflows initiated.\",\"workflows_ids\":[\"db6a2097-3e4c-45a3-a772-0c11e6da8b44\"]}\nCongratulations! You have now successfully uploaded a COG dataset to the VEDA Dashboard. You can now explore the data catalog to verify the ingestion process has worked successfully, as now uploaded data should be ready for viewing and exploration."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#validate-data-format",
    "href": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#validate-data-format",
    "title": "Ingestion Workflow for Uploading Data to the VEDA Catalog for the VEDA Dashboard",
    "section": "",
    "text": "Below we will import some geospatial tools for validation and define some of the variables to be used including the TARGET_FILENAME for the datafile you want to upload. Note that in this example we will demonstrate the ingestion of GEOGLAM’s June 2023 data. It is important that the file you want to upload (e.g., CropMonitor_2023_06_28.tif ) is located in the same repository folder as this notebook.\n\nimport os\n\nimport rio_cogeo\nimport rasterio\nimport boto3\nimport requests\n\nIn the cell below we are using TARGET_FILENAME to revise the LOCAL_FILE_PATH into the correct file format as advised in the File preparation documentation. See example formats in the link provided.\nIf the LOCAL_FILE_PATH is already properly formatted, then both LOCAL_FILE_PATH and TARGET_FILENAME will be identical.\n\nLOCAL_FILE_PATH = \"CropMonitor_2023_06_28.tif\"\nYEAR, MONTH = 2023, 6\n\nTARGET_FILENAME = f\"CropMonitor_{YEAR}{MONTH:02}.tif\"\n\nThe following code is used to test whether the data format you are planning to upload is Cloud Optimized GeoTiff (COG) that enables more efficient workflows in the cloud environment. If the validation process identifies that it is not a COG, it will convert it into one.\n\nfile_is_a_cog = rio_cogeo.cog_validate(LOCAL_FILE_PATH)\nif not file_is_a_cog:\n    raise ValueError()\n    print(\"File is not a COG - converting\")\n    rio_cogeo.cog_translate(LOCAL_FILE_PATH, LOCAL_FILE_PATH, in_memory=True)"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#upload-file-to-s3",
    "href": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#upload-file-to-s3",
    "title": "Ingestion Workflow for Uploading Data to the VEDA Catalog for the VEDA Dashboard",
    "section": "",
    "text": "The following code will upload your COG data into veda-data-store-staging bucket. It will use the TARGET_FILENAME to assign the correct month and year values we have provided earlier in this notebook, under the geoglam bucket on S3.\n\ns3 = boto3.client(\"s3\")\nBUCKET = \"veda-data-store-staging\"\nKEY = f\"{BUCKET}/geoglam/{TARGET_FILENAME}\"\nS3_FILE_LOCATION = f\"s3://{KEY}\"\n\nif False:\n    s3.upload_file(LOCAL_FILE_PATH, KEY)"
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#use-the-workflows-api-to-add-this-geoglam-item-to-the-staging-catalog",
    "href": "instance-management/adding-content/dataset-ingestion/example-template/example-geoglam-ingest.html#use-the-workflows-api-to-add-this-geoglam-item-to-the-staging-catalog",
    "title": "Ingestion Workflow for Uploading Data to the VEDA Catalog for the VEDA Dashboard",
    "section": "",
    "text": "For this step, open the workflows API at staging.openveda.cloud/api/workflows/docs in a second browser tab and click the green authorize button at the upper right to authenticate your session with your username and password (you will be temporarily redirected to a login widget and then back to the workflows-api docs). The cells below will guide you through the process of configuiring your request jsons for each endpoint demonstrated and you will copy the cell outputs into the workflows API in your second tab.\n\n\nHere the data provider will construct the dataset definition (and supporting metadata) that will be used for dataset ingestion. It is imperative that these values are correct and align to the data the provider is planning to upload to the VEDA Platform. For example, make sure that the startdate and enddate are realistic (e.g., an \"enddate\":\"2023-06-31T23:59:59Z\" would be an incorrect value for June, as it contains only 31 days).\nFor further detail on metadata required for entries in the VEDA STAC to work with the VEDA Dashboard, see documentation here. In particular, note recommendations for the fields is_periodic and time_density. For example, in the code block below we define the is_periodic field as False because we are ingesting only one month of data. Even though we know that the monthly observations are provided routinely by GEOGLAM, we will only have a single file to ingest and so do not have a temporal range of items in the collection with a monthly time density to generate a time picker from the available data.\n\nNote Several OPTIONAL properties are added to this dataset config for completeness. Your dataset json does NOT need to include these optional properties * assets * item_assets * renders\n\n\nimport json\n\ndataset = {\n    \"collection\": \"geoglam\",\n    \"title\": \"GEOGLAM Crop Monitor\",\n    \"data_type\": \"cog\",\n    \"spatial_extent\": {\n    \"xmin\": -180,\n    \"ymin\": -90,\n    \"xmax\": 180,\n    \"ymax\": 90\n    },\n    \"temporal_extent\": {\n    \"startdate\": \"2020-01-01T00:00:00Z\",\n    \"enddate\": \"2023-06-30T23:59:59Z\"\n    },\n    \"license\": \"MIT\",\n    \"description\": \"The Crop Monitors were designed to provide a public good of open, timely, science-driven information on crop conditions in support of market transparency for the G20 Agricultural Market Information System (AMIS). Reflecting an international, multi-source, consensus assessment of crop growing conditions, status, and agro-climatic factors likely to impact global production, focusing on the major producing and trading countries for the four primary crops monitored by AMIS (wheat, maize, rice, and soybeans). The Crop Monitor for AMIS brings together over 40 partners from national, regional (i.e. sub-continental), and global monitoring systems, space agencies, agriculture organizations and universities. Read more: https://cropmonitor.org/index.php/about/aboutus/\",\n    \"is_periodic\": False,\n    \"time_density\": \"month\",\n    ## NOTE: email the veda team at veda@uah.edu to upload a new thumbnail for your dataset\n    \"assets\": {\n        \"thumbnail\": {\n            \"href\": \"https://thumbnails.openveda.cloud/geoglam--dataset-cover.jpg\",\n            \"type\": \"image/jpeg\",\n            \"roles\": [\"thumbnail\"],\n            \"title\": \"Thumbnail\",\n            \"description\": \"Photo by [Jean Wimmerlin](https://unsplash.com/photos/RUj5b4YXaHE) (Bird's eye view of fields)\"\n        }\n    },\n    ## RENDERS metadata are OPTIONAL but provided below\n    \"renders\": {\n        \"dashboard\": {\n            \"bidx\": [1],\n            \"title\": \"VEDA Dashboard Render Parameters\",\n            \"assets\": [\n            \"cog_default\"\n            ],\n            \"unscale\": False,\n            \"colormap\": {\n                \"1\": [120, 120, 120],\n                \"2\": [130, 65, 0],\n                \"3\": [66, 207, 56],\n                \"4\": [245, 239, 0],\n                \"5\": [241, 89, 32],\n                \"6\": [168, 0, 0],\n                \"7\": [0, 143, 201]\n            },\n            \"max_size\": 1024,\n            \"resampling\": \"nearest\",\n            \"return_mask\": True\n        }\n    },\n    ## IMPORTANT update providers for a your data, some are specific to each collection\n    \"providers\": [\n    {\n        \"url\": \"https://data.nal.usda.gov/dataset/geoglam-geo-global-agricultural-monitoring-crop-assessment-tool#:~:text=The%20GEOGLAM%20crop%20calendars%20are,USDA%20FAS%2C%20and%20USDA%20NASS.\",\n        \"name\": \"USDA & Global Crop Monitor Group partners\",\n        \"roles\": [\n            \"producer\",\n            \"processor\",\n            \"licensor\"\n        ]\n    },\n        {\n            \"url\": \"https://www.earthdata.nasa.gov/dashboard/\",\n            \"name\": \"NASA VEDA\",\n            \"roles\": [\"host\"]\n        }\n    ],\n    ## item_assets are OPTIONAL but pre-filled here\n    \"item_assets\": {\n        \"cog_default\": {\n            \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n                \"roles\": [\"data\",\"layer\"],\n            \"title\": \"Default COG Layer\",\n            \"description\": \"Cloud optimized default layer to display on map\"\n        }\n    },\n    \"sample_files\": [\n        \"s3://veda-data-store-staging/geoglam/CropMonitor_202306.tif\"\n    ],\n    \"discovery_items\": [\n        {\n          \"discovery\": \"s3\",\n          \"prefix\": \"geoglam/\",\n          \"bucket\": \"veda-data-store-staging\",\n          \"filename_regex\": \"(.*)CropMonitor_202306.tif$\"\n        }\n    ]\n}\n\nprint(json.dumps(dataset, indent=2))\n\n{\n  \"collection\": \"geoglam\",\n  \"title\": \"GEOGLAM Crop Monitor\",\n  \"data_type\": \"cog\",\n  \"spatial_extent\": {\n    \"xmin\": -180,\n    \"ymin\": -90,\n    \"xmax\": 180,\n    \"ymax\": 90\n  },\n  \"temporal_extent\": {\n    \"startdate\": \"2020-01-01T00:00:00Z\",\n    \"enddate\": \"2023-06-30T23:59:59Z\"\n  },\n  \"license\": \"MIT\",\n  \"description\": \"The Crop Monitors were designed to provide a public good of open, timely, science-driven information on crop conditions in support of market transparency for the G20 Agricultural Market Information System (AMIS). Reflecting an international, multi-source, consensus assessment of crop growing conditions, status, and agro-climatic factors likely to impact global production, focusing on the major producing and trading countries for the four primary crops monitored by AMIS (wheat, maize, rice, and soybeans). The Crop Monitor for AMIS brings together over 40 partners from national, regional (i.e. sub-continental), and global monitoring systems, space agencies, agriculture organizations and universities. Read more: https://cropmonitor.org/index.php/about/aboutus/\",\n  \"is_periodic\": false,\n  \"time_density\": \"month\",\n  \"assets\": {\n    \"thumbnail\": {\n      \"href\": \"https://thumbnails.openveda.cloud/geoglam--dataset-cover.jpg\",\n      \"type\": \"image/jpeg\",\n      \"roles\": [\n        \"thumbnail\"\n      ],\n      \"title\": \"Thumbnail\",\n      \"description\": \"Photo by [Jean Wimmerlin](https://unsplash.com/photos/RUj5b4YXaHE) (Bird's eye view of fields)\"\n    }\n  },\n  \"renders\": {\n    \"dashboard\": {\n      \"bidx\": [\n        1\n      ],\n      \"title\": \"VEDA Dashboard Render Parameters\",\n      \"assets\": [\n        \"cog_default\"\n      ],\n      \"unscale\": false,\n      \"colormap\": {\n        \"1\": [\n          120,\n          120,\n          120\n        ],\n        \"2\": [\n          130,\n          65,\n          0\n        ],\n        \"3\": [\n          66,\n          207,\n          56\n        ],\n        \"4\": [\n          245,\n          239,\n          0\n        ],\n        \"5\": [\n          241,\n          89,\n          32\n        ],\n        \"6\": [\n          168,\n          0,\n          0\n        ],\n        \"7\": [\n          0,\n          143,\n          201\n        ]\n      },\n      \"max_size\": 1024,\n      \"resampling\": \"nearest\",\n      \"return_mask\": true\n    }\n  },\n  \"providers\": [\n    {\n      \"url\": \"https://data.nal.usda.gov/dataset/geoglam-geo-global-agricultural-monitoring-crop-assessment-tool#:~:text=The%20GEOGLAM%20crop%20calendars%20are,USDA%20FAS%2C%20and%20USDA%20NASS.\",\n      \"name\": \"USDA & Global Crop Monitor Group partners\",\n      \"roles\": [\n        \"producer\",\n        \"processor\",\n        \"licensor\"\n      ]\n    },\n    {\n      \"url\": \"https://www.earthdata.nasa.gov/dashboard/\",\n      \"name\": \"NASA VEDA\",\n      \"roles\": [\n        \"host\"\n      ]\n    }\n  ],\n  \"item_assets\": {\n    \"cog_default\": {\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"roles\": [\n        \"data\",\n        \"layer\"\n      ],\n      \"title\": \"Default COG Layer\",\n      \"description\": \"Cloud optimized default layer to display on map\"\n    }\n  },\n  \"sample_files\": [\n    \"s3://veda-data-store-staging/geoglam/CropMonitor_202306.tif\"\n  ],\n  \"discovery_items\": [\n    {\n      \"discovery\": \"s3\",\n      \"prefix\": \"geoglam/\",\n      \"bucket\": \"veda-data-store-staging\",\n      \"filename_regex\": \"(.*)CropMonitor_202306.tif$\"\n    }\n  ]\n}\n\n\n\n\n\nAfter composing your dataset definition, copy the printed json and paste it into the /dataset/validate input in the workflows-api docs page in the second tab. Note that if you navigate away from this page you will need to click authorize again.\nChoose POST dataset/validate in the Dataset section of the API docs at staging.openveda.cloud/api/workflows/docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nIf the json is valid, the response will confirm that it is ready to be published on the VEDA Platform.\n\n\n\nNow that you have validated your dataset, you can initiate a workflow and publish the dataset to the VEDA Platform.\nChoose POST dataset/publish in the Dataset section of the API docs at staging.openveda.cloud/api/workflows/docs. Click ’Try it Out` and paste your json into the Request body and then Execute\nOn success, you will recieve a success message containing the id of your workflow, for example\n{\"message\":\"Successfully published collection: geoglam. 1  workflows initiated.\",\"workflows_ids\":[\"db6a2097-3e4c-45a3-a772-0c11e6da8b44\"]}\nCongratulations! You have now successfully uploaded a COG dataset to the VEDA Dashboard. You can now explore the data catalog to verify the ingestion process has worked successfully, as now uploaded data should be ready for viewing and exploration."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/index.html",
    "href": "instance-management/adding-content/dataset-ingestion/index.html",
    "title": "Dataset Ingestion",
    "section": "",
    "text": "VEDA uses a centralized Spatio-Temporal Asset Catalog (STAC) for data dissemination and prefers to host datasets in cloud-object storage (AWS S3 in the region us-west-2) in the cloud-optimized file formats Cloud-Optimized GeoTIFF (COG) and Zarr, which enables viewing and efficient access in the cloud directly from the original datafiles without copies or multiple versions."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "href": "instance-management/adding-content/dataset-ingestion/index.html#steps-for-ingesting-a-dataset",
    "title": "Dataset Ingestion",
    "section": "Steps for ingesting a dataset",
    "text": "Steps for ingesting a dataset\nFor dataset ingestion, generally four steps are required. Depending on the capacity of the dataset provider, some of the steps can be completed by the VEDA team on request.\nThe data ingestion process requires Cognito credentials (username and password). In order to retrieve these credentials, you’ll need to contact a member of the VEDA Data Services Team at veda@uah.edu who can set up an account and credentials for you. The first time you log in using the Cognito Client, you will be prompted to set a new password.\nComplete as many steps of the process as you have capacity or authorization to. You will initially publish to a staging catalog where you can review the data before publishing to the public production catalog. Please follow the steps and guides outlined below:\n\nTransform datasets to conform with cloud-optimized file formats - see file preparation\nUpload files to storage (may be skipped, if data is cloud-optimized and in us-west-2)\nLoad those records into the STAGING VEDA STAC - see catalog ingestion\nFinally, when you are satisfied with how your data look in the staging catalog, open a veda-data pull request with the configuration you used to publish to the staging catalog. When this PR is approved, the data will be published to the production catalog at openveda.cloud!\n\n\n\n\n\n\n\nIf the above instructions do not cover the data you want to ingest and/or if you could use extra help getting started\n\n\n\nOpen a dedicated pull request in the veda-data repository. Please read through these docs fully first as you they will help supply the information required to complete the PR. Use this “new dataset” template to open a new issue and get started."
  },
  {
    "objectID": "instance-management/adding-content/dataset-ingestion/index.html#end-to-end-ingest-example",
    "href": "instance-management/adding-content/dataset-ingestion/index.html#end-to-end-ingest-example",
    "title": "Dataset Ingestion",
    "section": "End to end ingest example",
    "text": "End to end ingest example\nFor a walk through of the full process outlined above, please refer to this example notebook. This notebook uses the GEOGLAM June 2023 to ingest this file CropMonitor_2023_06_28.tif into VEDA’s staging STAC catalog.\n\n\n\n\n\n\nThis is an actual ingest and will update the staging STAC catalog\n\n\n\nPlease use this as a guide for the ingestion process (and required dataset defintions), replacing the GEOGLAM dataset metadata and file with your own data.\n\n\nStuck on how to develop compliant metadata records for your dataset?\nCheckout the following notebooks and resources to help provide you with the STAC metadata required to create the dataset definitions needed for catalog ingestion.\n\nHow to create STAC Collections: see this example notebook and related STAC conventions\nHow to create STAC Items: see this example notebook and conventions."
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/story-configuration.html",
    "href": "instance-management/adding-content/dashboard-configuration/story-configuration.html",
    "title": "Story Configuration",
    "section": "",
    "text": "By this point, you should have a few things:\n🧑‍🏫 We recommend you follow the video walkthrough on how to setup a virtual environment to facilitate story creation."
  },
  {
    "objectID": "instance-management/adding-content/dashboard-configuration/story-configuration.html#sec-video-walkthrough",
    "href": "instance-management/adding-content/dashboard-configuration/story-configuration.html#sec-video-walkthrough",
    "title": "Story Configuration",
    "section": "Video Walkthrough",
    "text": "Video Walkthrough\n\nSetting up GitHub codespaces\nCodespaces will allow you to have a development environment in the cloud without the need to setup anything on your local machine. VIDEO\n\n\nCreating a story\nWalkthrough of how to use GitHub codespaces to create a story. From creating the needed files to the Pull Request that will eventually get the content published. VIDEO\nIf you have any questions along the way, we prefer that you open tickets in veda-config. Alternatively, you can reach the VEDA team at veda@uah.edu."
  },
  {
    "objectID": "instance-management/adding-content/index.html",
    "href": "instance-management/adding-content/index.html",
    "title": "Adding Content",
    "section": "",
    "text": "Please see the sections below for documentation on\n\nDataset ingestion into the VEDA data store and STAC\n\nFile preparation\nCatalog ingestion\nIngest workflow example notebook and sample cloud optimized geotiff\nSTAC collection conventions\nSTAC item conventions\n\nConfiguration of Dataset information pages and Stories on the VEDA Dashboard\nUsage example notebooks that illustrate the use of datasets or compute methods\n\nNote: If you move a page or notebook, make sure to add a redirect link according to the quarto docs"
  },
  {
    "objectID": "instance-management/notebooks/templates/template-github-auth.html",
    "href": "instance-management/notebooks/templates/template-github-auth.html",
    "title": "VEDA Documentation",
    "section": "",
    "text": "import gh_scoped_creds  # noqa\n\n%ghscopedcreds"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#run-this-notebook",
    "title": "Visualize zarr",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailing aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#approach",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#approach",
    "title": "Visualize zarr",
    "section": "Approach",
    "text": "Approach\n\nUse intake to open a STAC collection using with xarray and dask\nPlot the data using hvplot"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#about-the-data",
    "title": "Visualize zarr",
    "section": "About the data",
    "text": "About the data\nThis is the Gridded Daily OCO-2 Carbon Dioxide assimilated dataset. More information can be found at: OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2 V10r (OCO2_GEOS_L3CO2_DAY)\nThe data has been converted to zarr format and published to the development version of the VEDA STAC Catalog.\n\nimport intake\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#declare-your-collection-of-interest",
    "title": "Visualize zarr",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\ncollection_id = \"oco2-geos-l3-daily\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#get-stac-collection",
    "title": "Visualize zarr",
    "section": "Get STAC collection",
    "text": "Get STAC collection\nUse intake to get the entire STAC collection.\n\ncollection = intake.open_stac_collection(f\"{STAC_API_URL}/collections/{collection_id}\")\ncollection\n\noco2-geos-l3-daily:\n  args:\n    stac_obj: https://openveda.cloud/api/stac/collections/oco2-geos-l3-daily\n  description: ''\n  driver: intake_stac.catalog.StacCollection\n  metadata:\n    assets:\n      zarr:\n        href: s3://veda-data-store/oco2-geos-l3-daily/OCO2_GEOS_L3CO2_day.zarr\n        roles:\n        - data\n        title: zarr\n        type: application/vnd+zarr\n    cube:dimensions:\n      lat:\n        axis: y\n        description: latitude\n        extent:\n        - -90.0\n        - 90.0\n        reference_system: 4326\n        type: spatial\n      lon:\n        axis: x\n        description: longitude\n        extent:\n        - -180.0\n        - 179.375\n        reference_system: 4326\n        type: spatial\n      time:\n        description: time\n        extent:\n        - '2015-01-01T12:00:00Z'\n        - '2021-11-04T12:00:00Z'\n        step: P1DT0H0M0S\n        type: temporal\n    cube:variables:\n      XCO2:\n        attrs:\n          long_name: Assimilated dry-air column average CO2 daily mean\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Assimilated dry-air column average CO2 daily mean\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n      XCO2PREC:\n        attrs:\n          long_name: Precision of dry-air column average CO2 daily mean from Desroziers\n            et al. (2005) diagnostic\n          units: mol CO2/mol dry\n        chunks:\n        - 100\n        - 100\n        - 100\n        description: Precision of dry-air column average CO2 daily mean from Desroziers\n          et al. (2005) diagnostic\n        dimensions:\n        - time\n        - lat\n        - lon\n        shape:\n        - 2500\n        - 361\n        - 576\n        type: data\n        unit: mol CO2/mol dry\n    dashboard:is_periodic: true\n    dashboard:time_density: day\n    description: \"The OCO-2 mission provides the highest quality space-based XCO2\\\n      \\ retrievals to date. However, the instrument data are characterized by large\\\n      \\ gaps in coverage due to OCO-2\\u2019s narrow 10-km ground track and an inability\\\n      \\ to see through clouds and thick aerosols. This global gridded dataset is produced\\\n      \\ using a data assimilation technique commonly referred to as state estimation\\\n      \\ within the geophysical literature. Data assimilation synthesizes simulations\\\n      \\ and observations, adjusting the state of atmospheric constituents like CO2\\\n      \\ to reflect observed values, thus gap-filling observations when and where they\\\n      \\ are unavailable based on previous observations and short transport simulations\\\n      \\ by GEOS. Compared to other methods, data assimilation has the advantage that\\\n      \\ it makes estimates based on our collective scientific understanding, notably\\\n      \\ of the Earth's carbon cycle and atmospheric transport. OCO-2 GEOS (Goddard\\\n      \\ Earth Observing System) Level 3 data are produced by ingesting OCO-2 L2 retrievals\\\n      \\ every 6 hours with GEOS CoDAS, a modeling and data assimilation system maintained\\\n      \\ by NASA's Global Modeling and Assimilation Office (GMAO). GEOS CoDAS uses\\\n      \\ a high-performance computing implementation of the Gridpoint Statistical Interpolation\\\n      \\ approach for solving the state estimation problem. GSI finds the analyzed\\\n      \\ state that minimizes the three-dimensional variational (3D-Var) cost function\\\n      \\ formulation of the state estimation problem.\"\n    extent:\n      spatial:\n        bbox:\n        - - -180.0\n          - -90.0\n          - 180.0\n          - 90.0\n      temporal:\n        interval:\n        - - null\n          - null\n    id: oco2-geos-l3-daily\n    license: CC0-1.0\n    providers:\n    - name: NASA VEDA\n      roles:\n      - host\n      url: https://www.earthdata.nasa.gov/dashboard/\n    stac_extensions:\n    - https://stac-extensions.github.io/datacube/v2.2.0/schema.json\n    stac_version: 1.0.0\n    title: Gridded Daily OCO-2 Carbon Dioxide assimilated dataset\n    type: Collection"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#read-from-zarr-to-xarray",
    "title": "Visualize zarr",
    "section": "Read from zarr to xarray",
    "text": "Read from zarr to xarray\nIntake lets you go straight from the asset to an xarray dataset backed by a dask array.\n\nsource = collection.get_asset(\"zarr\")\n\nds = source.to_dask()\nds\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/intake_xarray/base.py:21: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  'dims': dict(self._ds.dims),\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 8GB\nDimensions:   (time: 2500, lat: 361, lon: 576)\nCoordinates:\n  * lat       (lat) float64 3kB -90.0 -89.5 -89.0 -88.5 ... 88.5 89.0 89.5 90.0\n  * lon       (lon) float64 5kB -180.0 -179.4 -178.8 ... 178.1 178.8 179.4\n  * time      (time) datetime64[ns] 20kB 2015-01-01T12:00:00 ... 2021-11-04T1...\nData variables:\n    XCO2      (time, lat, lon) float64 4GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n    XCO2PREC  (time, lat, lon) float64 4GB dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\nAttributes: (12/25)\n    BuildId:                        B10.2.06\n    Contact:                        Brad Weir (brad.weir@nasa.gov)\n    Conventions:                    CF-1\n    DataResolution:                 0.5x0.625\n    EastBoundingCoordinate:         179.375\n    Format:                         NetCDF-4/HDF-5\n    ...                             ...\n    ShortName:                      OCO2_GEOS_L3CO2_DAY_10r\n    SouthBoundingCoordinate:        -90.0\n    SpatialCoverage:                global\n    Title:                          OCO-2 GEOS Level 3 daily, 0.5x0.625 assim...\n    VersionID:                      V10r\n    WestBoundingCoordinate:         -180.0xarray.DatasetDimensions:time: 2500lat: 361lon: 576Coordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2015-01-01T12:00:00 ... 2021-11-...begin_date :20170801begin_time :120000long_name :timearray(['2015-01-01T12:00:00.000000000', '2015-01-02T12:00:00.000000000',\n       '2015-01-03T12:00:00.000000000', ..., '2021-11-02T12:00:00.000000000',\n       '2021-11-03T12:00:00.000000000', '2021-11-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (2)XCO2(time, lat, lon)float64dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Assimilated dry-air column average CO2 daily meanunits :mol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nDask graph\n600 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                             576 361 2500\n\n\n\n\nXCO2PREC(time, lat, lon)float64dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;long_name :Precision of dry-air column average CO2 daily mean from Desroziers et al. (2005) diagnosticunits :mol CO2/mol dry\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nDask graph\n600 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                             576 361 2500\n\n\n\n\nIndexes: (3)latPandasIndexPandasIndex(Index([-90.0, -89.5, -89.0, -88.5, -88.0, -87.5, -87.0, -86.5, -86.0, -85.5,\n       ...\n        85.5,  86.0,  86.5,  87.0,  87.5,  88.0,  88.5,  89.0,  89.5,  90.0],\n      dtype='float64', name='lat', length=361))lonPandasIndexPandasIndex(Index([  -180.0, -179.375,  -178.75, -178.125,   -177.5, -176.875,  -176.25,\n       -175.625,   -175.0, -174.375,\n       ...\n         173.75,  174.375,    175.0,  175.625,   176.25,  176.875,    177.5,\n        178.125,   178.75,  179.375],\n      dtype='float64', name='lon', length=576))timePandasIndexPandasIndex(DatetimeIndex(['2015-01-01 12:00:00', '2015-01-02 12:00:00',\n               '2015-01-03 12:00:00', '2015-01-04 12:00:00',\n               '2015-01-05 12:00:00', '2015-01-06 12:00:00',\n               '2015-01-07 12:00:00', '2015-01-08 12:00:00',\n               '2015-01-09 12:00:00', '2015-01-10 12:00:00',\n               ...\n               '2021-10-26 12:00:00', '2021-10-27 12:00:00',\n               '2021-10-28 12:00:00', '2021-10-29 12:00:00',\n               '2021-10-30 12:00:00', '2021-10-31 12:00:00',\n               '2021-11-01 12:00:00', '2021-11-02 12:00:00',\n               '2021-11-03 12:00:00', '2021-11-04 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=2500, freq=None))Attributes: (25)BuildId :B10.2.06Contact :Brad Weir (brad.weir@nasa.gov)Conventions :CF-1DataResolution :0.5x0.625EastBoundingCoordinate :179.375Format :NetCDF-4/HDF-5History :Original file generated: Tue Mar 15 12:02:48 2022 GMTIdentifierProductDOI :10.5067/Y9M4NM9MPCGHIdentifierProductDOIAuthority :http://doi.org/Institution :NASA GSFC Global Modeling and Assimilation Office and OCO-2 Project, Jet Propulsion LaboratoryLatitudeResolution :0.5LongName :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2LongitudeResolution :0.625NorthBoundingCoordinate :90.0ProductionDateTime :2022-03-15T12:02:48ZRangeBeginningDate :2017-08-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2017-08-01RangeEndingTime :23:59:99.999999ShortName :OCO2_GEOS_L3CO2_DAY_10rSouthBoundingCoordinate :-90.0SpatialCoverage :globalTitle :OCO-2 GEOS Level 3 daily, 0.5x0.625 assimilated CO2VersionID :V10rWestBoundingCoordinate :-180.0\n\n\nIn xarray you can inspect just one data variable using dot notation:\n\nds.XCO2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'XCO2' (time: 2500, lat: 361, lon: 576)&gt; Size: 4GB\ndask.array&lt;open_dataset-XCO2, shape=(2500, 361, 576), dtype=float64, chunksize=(100, 100, 100), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 3kB -90.0 -89.5 -89.0 -88.5 ... 88.5 89.0 89.5 90.0\n  * lon      (lon) float64 5kB -180.0 -179.4 -178.8 -178.1 ... 178.1 178.8 179.4\n  * time     (time) datetime64[ns] 20kB 2015-01-01T12:00:00 ... 2021-11-04T12...\nAttributes:\n    long_name:  Assimilated dry-air column average CO2 daily mean\n    units:      mol CO2/mol dryxarray.DataArray'XCO2'time: 2500lat: 361lon: 576dask.array&lt;chunksize=(100, 100, 100), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.87 GiB\n7.63 MiB\n\n\nShape\n(2500, 361, 576)\n(100, 100, 100)\n\n\nDask graph\n600 chunks in 2 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                             576 361 2500\n\n\n\n\nCoordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2015-01-01T12:00:00 ... 2021-11-...begin_date :20170801begin_time :120000long_name :timearray(['2015-01-01T12:00:00.000000000', '2015-01-02T12:00:00.000000000',\n       '2015-01-03T12:00:00.000000000', ..., '2021-11-02T12:00:00.000000000',\n       '2021-11-03T12:00:00.000000000', '2021-11-04T12:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([-90.0, -89.5, -89.0, -88.5, -88.0, -87.5, -87.0, -86.5, -86.0, -85.5,\n       ...\n        85.5,  86.0,  86.5,  87.0,  87.5,  88.0,  88.5,  89.0,  89.5,  90.0],\n      dtype='float64', name='lat', length=361))lonPandasIndexPandasIndex(Index([  -180.0, -179.375,  -178.75, -178.125,   -177.5, -176.875,  -176.25,\n       -175.625,   -175.0, -174.375,\n       ...\n         173.75,  174.375,    175.0,  175.625,   176.25,  176.875,    177.5,\n        178.125,   178.75,  179.375],\n      dtype='float64', name='lon', length=576))timePandasIndexPandasIndex(DatetimeIndex(['2015-01-01 12:00:00', '2015-01-02 12:00:00',\n               '2015-01-03 12:00:00', '2015-01-04 12:00:00',\n               '2015-01-05 12:00:00', '2015-01-06 12:00:00',\n               '2015-01-07 12:00:00', '2015-01-08 12:00:00',\n               '2015-01-09 12:00:00', '2015-01-10 12:00:00',\n               ...\n               '2021-10-26 12:00:00', '2021-10-27 12:00:00',\n               '2021-10-28 12:00:00', '2021-10-29 12:00:00',\n               '2021-10-30 12:00:00', '2021-10-31 12:00:00',\n               '2021-11-01 12:00:00', '2021-11-02 12:00:00',\n               '2021-11-03 12:00:00', '2021-11-04 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=2500, freq=None))Attributes: (2)long_name :Assimilated dry-air column average CO2 daily meanunits :mol CO2/mol dry"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-zarr.html#plot-data",
    "href": "instance-management/notebooks/quickstarts/visualize-zarr.html#plot-data",
    "title": "Visualize zarr",
    "section": "Plot data",
    "text": "Plot data\nWe can plot the XCO2 variable as an interactive map (with date slider) using hvplot.\n\nds.XCO2.hvplot(\n    x=\"lon\",\n    y=\"lat\",\n    groupby=\"time\",\n    coastline=True,\n    rasterize=True,\n    aggregator=\"mean\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#run-this-notebook",
    "title": "Get timeseries from COGs",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#approach",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#approach",
    "title": "Get timeseries from COGs",
    "section": "Approach",
    "text": "Approach\n\nUsing a list of STAC items and a bouding box fetch stats from /cog/statistics endpoint\nGenerate a timeseries plot using statistics from each time step\nSpeed up workflow using Dask\n\n\nimport requests\nimport folium\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#declare-your-collection-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://staging.openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://staging.openveda.cloud/api/raster\"\n\ncollection_id = \"no2-monthly\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-collection",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC collection",
    "text": "Fetch STAC collection\nWe will use requests to fetch all the metadata about the collection of interest from STAC.\n\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_id}\").json()\ncollection\n\n{'id': 'no2-monthly',\n 'type': 'Collection',\n 'links': [{'rel': 'items',\n   'type': 'application/geo+json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/no2-monthly/items'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/no2-monthly'}],\n 'title': 'NO₂',\n 'extent': {'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'temporal': {'interval': [['2016-01-01 00:00:00+00',\n     '2023-12-31 00:00:00+00']]}},\n 'license': 'MIT',\n 'renders': {'dashboard': {'bidx': [1],\n   'title': 'VEDA Dashboard Render Parameters',\n   'assets': ['cog_default'],\n   'rescale': [[0, 15000000000000000]],\n   'resampling': 'bilinear',\n   'color_formula': 'gamma r 1.05',\n   'colormap_name': 'rdbu_r'}},\n 'providers': [{'url': 'https://disc.gsfc.nasa.gov/',\n   'name': 'NASA Goddard Earth Sciences Data and Information Services Center',\n   'roles': ['producer', 'processor']},\n  {'url': 'https://www.earthdata.nasa.gov/dashboard/',\n   'name': 'NASA VEDA',\n   'roles': ['host']}],\n 'summaries': {'datetime': ['2016-01-01T00:00:00Z', '2023-12-31T00:00:00Z']},\n 'description': 'Darker colors indicate higher nitrogen dioxide (NO₂) levels and more activity. Lighter colors indicate lower levels of NO₂ and less activity. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'item_assets': {'cog_default': {'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map'}},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/item-assets/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/render/v1.0.0/schema.json'],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month'}\n\n\n\nDescribe the periodic nature of the data\nIn the collection above we will pay particular attention to the fields that define the periodicity of the data.\n\ncollection[\"dashboard:is_periodic\"]\n\nTrue\n\n\n\ncollection[\"dashboard:time_density\"]\n\n'month'\n\n\n\ncollection[\"summaries\"]\n\n{'datetime': ['2016-01-01T00:00:00Z', '2023-12-31T00:00:00Z']}"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#fetch-stac-items",
    "title": "Get timeseries from COGs",
    "section": "Fetch STAC items",
    "text": "Fetch STAC items\nGet the list of all the STAC items within this collection.\n\nitems = requests.get(\n    f\"{STAC_API_URL}/collections/{collection_id}/items?limit=100\"\n).json()[\"features\"]\nlen(items)\n\n96\n\n\nWe can inspect one of these items to get a sense of what metadata is available.\n\nitems[0]\n\n{'id': 'OMI_trno2_0.10x0.10_202312_Col3_V4.nc',\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/no2-monthly'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/no2-monthly'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202312_Col3_V4.nc'},\n  {'title': 'Map of Item',\n   'href': 'https://staging.openveda.cloud/api/raster/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202312_Col3_V4.nc/map?bidx=1&assets=cog_default&rescale=0%2C15000000000000000&resampling=bilinear&color_formula=gamma+r+1.05&colormap_name=rdbu_r',\n   'rel': 'preview',\n   'type': 'text/html'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly/OMI_trno2_0.10x0.10_202312_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -1.2676506002282294e+30,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 32579336984854530,\n      'min': -6736120943476736.0,\n      'count': 11.0,\n      'buckets': [30.0,\n       409205.0,\n       23559.0,\n       1376.0,\n       430.0,\n       277.0,\n       119.0,\n       35.0,\n       18.0,\n       5.0]},\n     'statistics': {'mean': 331445868796687.44,\n      'stddev': 935114136625544.4,\n      'maximum': 32579336984854530,\n      'minimum': -6736120943476736.0,\n      'valid_percent': 82.97996520996094}}]},\n  'rendered_preview': {'title': 'Rendered preview',\n   'href': 'https://staging.openveda.cloud/api/raster/collections/no2-monthly/items/OMI_trno2_0.10x0.10_202312_Col3_V4.nc/preview.png?bidx=1&assets=cog_default&rescale=0%2C15000000000000000&resampling=bilinear&color_formula=gamma+r+1.05&colormap_name=rdbu_r',\n   'rel': 'preview',\n   'roles': ['overview'],\n   'type': 'image/png'}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-180, -90],\n    [180, -90],\n    [180, 90],\n    [-180, 90],\n    [-180, -90]]]},\n 'collection': 'no2-monthly',\n 'properties': {'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1800.0, 3600.0],\n  'end_datetime': '2023-12-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'start_datetime': '2023-12-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#define-an-area-of-interest",
    "title": "Get timeseries from COGs",
    "section": "Define an area of interest",
    "text": "Define an area of interest\nWe will be using a bounding box over metropolitan france. We’ll use that bounding box to subset the data when calculating the timeseries.\n\nfrance_bounding_box = {\n    \"type\": \"Feature\",\n    \"properties\": {\"ADMIN\": \"France\", \"ISO_A3\": \"FRA\"},\n    \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n            [\n                [-5.183429, 42.332925],\n                [8.233998, 42.332925],\n                [8.233998, 51.066135],\n                [-5.183429, 51.066135],\n                [-5.183429, 42.332925],\n            ]\n        ],\n    },\n}\n\nLet’s take a look at that box.\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=3,\n)\n\nfolium.GeoJson(france_bounding_box, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#use-cogstatistics-to-get-data-for-the-aoi",
    "title": "Get timeseries from COGs",
    "section": "Use /cog/statistics to get data for the AOI",
    "text": "Use /cog/statistics to get data for the AOI\nFirst, we create a generate_stats function and then we call it with the bounding box defined for France.\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/collections/{collection_id}/items/{item['id']}/statistics\",\n        json=geojson\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": item[\"properties\"][\"start_datetime\"],\n    }\n\n\nGenerate stats\nThis may take some time due to the complexity of the shape we’re requesting. See the end of this notebook for tips on how to speed this up.\n\n%%time\nstats = [generate_stats(item, france_bounding_box) for item in items]\n\nCPU times: user 317 ms, sys: 64.9 ms, total: 382 ms\nWall time: 1min 26s\n\n\n\n\nInspect one result\n\nstats[0]\n\n{'statistics': {'cog_default_b1': {'min': -387779578036224.0,\n   'max': 5971174383157248.0,\n   'mean': 1636077207936257.2,\n   'count': 11880.0,\n   'sum': 1.9436597230282736e+19,\n   'std': 808762440624020.6,\n   'median': 1506456557846528.0,\n   'majority': 1143241374171136.0,\n   'minority': -387779578036224.0,\n   'unique': 11875.0,\n   'histogram': [[120.0,\n     1616.0,\n     4274.0,\n     3441.0,\n     1384.0,\n     624.0,\n     244.0,\n     125.0,\n     40.0,\n     12.0],\n    [-387779578036224.0,\n     248115831504896.0,\n     884011241046016.0,\n     1519906650587136.0,\n     2155802060128256.0,\n     2791697603887104.0,\n     3427592744992768.0,\n     4063488422969344.0,\n     4699383564075008.0,\n     5335279242051584.0,\n     5971174383157248.0]],\n   'valid_percent': 100.0,\n   'masked_pixels': 0.0,\n   'valid_pixels': 11880.0,\n   'percentile_2': 395780397465600.0,\n   'percentile_98': 3847018581590016.0}},\n 'ADMIN': 'France',\n 'ISO_A3': 'FRA',\n 'start_datetime': '2023-09-01T00:00:00+00:00'}"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#plot-timeseries",
    "title": "Get timeseries from COGs",
    "section": "Plot timeseries",
    "text": "Plot timeseries\nIt is easier to interact with these results as a pandas dataframe. The following function takes the json, passes it to pandas, cleans up the column names and parses the date column.\n\nimport pandas as pd\n\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.cog_default_b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"])\n    return df\n\n\ndf = clean_stats(stats)\n\n\nConstruct the plot\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(df[\"date\"], df[\"mean\"], \"black\", label=\"Mean monthly NO2 values\")\n\nplt.fill_between(\n    df[\"date\"],\n    df[\"mean\"] + df[\"std\"],\n    df[\"mean\"] - df[\"std\"],\n    facecolor=\"lightgray\",\n    interpolate=False,\n    label=\"+/- one standard devation\",\n)\n\nplt.plot(\n    df[\"date\"],\n    df[\"min\"],\n    color=\"blue\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Min monthly NO2 values\",\n)\nplt.plot(\n    df[\"date\"],\n    df[\"max\"],\n    color=\"red\",\n    linestyle=\"-\",\n    linewidth=0.5,\n    label=\"Max monhtly NO2 values\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\n\n\n\n\nIn this graph we can see the yearly cycles in NO2 values due to seasonal variations, as well as a slight downward slope in maximum NO2 values"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#complex-aoi",
    "title": "Get timeseries from COGs",
    "section": "Complex AOI",
    "text": "Complex AOI\nThe values plotted above don’t correspond exactly to Fance, since the bounding box excludes Corsica and overseas territories such as Mayotte and French Polynesia, and covers parts of neighboring countries including Spain, Italy, Germany and the entirety of Luxembourg. We can fetch GeoJSON from an authoritative online source (https://gadm.org/download_country.html).\nWhile the NO2 values above correspond more or less to those of in France, we can be much more precise by using a complex geojson that represents the boundaries of France exactly, including overseas territories in the Carribean and Indian Oceans, and South America.\nNote: In this notebook we write out the whole perimeter as a MultiPolygon in geojson. In practice you will often be reading this kind of shape from a file (usually with the help of geopandas).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]\n\nLet’s take a look at this AOI on a map\n\nm = folium.Map(\n    location=[40, 0],\n    zoom_start=3,\n)\n\nfolium.GeoJson(france_aoi, name=\"France\").add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe can now request the NO2 values for this complex AOI the same way as for the bounding box.\nNotice, however, that due to the complexity of the shape, it takes much longer to gather the requested data about 4 times as long as for the bounding box example above.\n\n%%time\naoi_df = clean_stats([generate_stats(item, france_aoi) for item in items])\n\nCPU times: user 4.02 s, sys: 73.9 ms, total: 4.09 s\nWall time: 2min 27s\n\n\nWe can compare the mean monthly NO2 values calculated when using the bounding box and when using the country’s exact borders\n\nfig = plt.figure(figsize=(20, 10))\n\nplt.plot(\n    df[\"date\"],\n    df[\"mean\"],\n    color=\"blue\",\n    label=\"Mean monthly NO2 values using bounding box\",\n)\nplt.plot(\n    aoi_df[\"date\"],\n    aoi_df[\"mean\"],\n    color=\"red\",\n    label=\"Mean monthly NO2 values using complex AOI\",\n)\n\nplt.legend()\nplt.title(\"NO2 Values in France (2016-2022)\")\n\nText(0.5, 1.0, 'NO2 Values in France (2016-2022)')\n\n\n\n\n\n\n\n\n\nWhile the difference is small, it is very interesting to note that the NO2 values calculated using the exact borders are systematically less than when using the bounding box. This may be due to the fact that the bounding box includes parts of western Germany and northern Italy that have a lot industrial activity, whereas the areas included when using the exact borders that are not included in the bounding box case, are overseas territories much less industrial activity."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "href": "instance-management/notebooks/quickstarts/timeseries-stac-api.html#speed-things-up-parallelize-computation-with-dask",
    "title": "Get timeseries from COGs",
    "section": "Speed things up: parallelize computation with Dask",
    "text": "Speed things up: parallelize computation with Dask\nWe can drastically reduce the time it takes to generate the timeseries, even with the complex AOI above, by parallelizing our code. The cogs/statistics API is powered by AWS Lambda which executes each request in a separate instance. This means the requests are highly scalable. Since each statistics request is for a single timestamp, we can request statistics for multiple timesteps concurrently, and greatly reduce the amount of time needed. We will demonstrate this by using the Dask.\n\nCreate a Dask client\nFirst we will create a Dask client. In this case we will use the threads on the same server that is running this jupyter notebook.\n\nimport dask.distributed\n\nclient = dask.distributed.Client()\n\n\n\nSubmit work\nWe will submit the generate_stats function for each item in our list and collect a list of futures. This will immediately kick off work in dask. We can then gather all the results.\n\n%%time\nfutures = [client.submit(generate_stats, item, france_aoi) for item in items]\nstats = client.gather(futures)\n\nCPU times: user 23.6 s, sys: 483 ms, total: 24.1 s\nWall time: 45.2 s\n\n\n\n\nClose the Dask client\nIt is good practice to close the client when you are done.\nclient.shutdown()\n\n\nAlternate approach\nIf you are familiar with the concurrent.futures library you can use that instead of Dask."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html",
    "href": "instance-management/notebooks/quickstarts/list-collections.html",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/list-collections.html#run-this-notebook",
    "title": "List STAC collections",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html#approach",
    "href": "instance-management/notebooks/quickstarts/list-collections.html#approach",
    "title": "List STAC collections",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog\nIterate over collections and print the title of each collection"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "href": "instance-management/notebooks/quickstarts/list-collections.html#open-stac-catalog",
    "title": "List STAC collections",
    "section": "Open STAC catalog",
    "text": "Open STAC catalog\n\nfrom pystac_client import Client\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac/\"\ncatalog = Client.open(STAC_API_URL)"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html#list-collections",
    "href": "instance-management/notebooks/quickstarts/list-collections.html#list-collections",
    "title": "List STAC collections",
    "section": "List collections",
    "text": "List collections\n\ncollections = list(catalog.get_collections())\nfor collection in sorted(collections, key=lambda x: x.title):\n    print(collection.title)\n\n\n0-100 cm Volumetric Soil Moisture (%)\nAerosol Optical Depth (AOD)\nAnnual LAI maps for 2003 and 2021 (Bangladesh)\nAnnual land cover maps for 2001 and 2020\nBlack Marble High Definition Nightlights 1 band Dataset\nBlack Marble High Definition Nightlights Monthly Dataset\nBurn Area Reflectance Classification for Thomas Fire\nCLIMDEX ACCESS CM2 SSP125 tmaxXF\nCLIMDEX ACCESS CM2 SSP245 tmaxXF\nCLIMDEX ACCESS CM2 SSP370 tmaxXF\nCLIMDEX ACCESS CM2 SSP585 tmaxXF\nCMIP6 Daily GISS-E2-1-G TAS Kerchunk (DEMO)\nCO₂ (Avg)\nCO₂ (Diff)\nCaldor Fire Behavior\nCaldor Fire Burn Severity\nCamp Fire Domain: Land Cover\nCamp Fire Domain: MODIS LST Day Difference\nCamp Fire Domain: MODIS LST Night Difference\nCamp Fire Domain: MODIS NDVI Difference\nCamp Fire Domain: MODIS WSA Albedo Difference\nChange in ET for 2020 fires using LIS outputs\nChange in transpiration for 2020 fires using LIS outputs\nDamage Probability Derived from UCONN GERs Lab After Hurricane Ian\nECCO sea-surface height change from 1992 to 2017\nEMIT Landfill Plumes\nEvapotranspiration - LIS 10km Global DA\nFLDAS Surface Soil Moisture Anomalies\nFalse Color Pre and Post Flood\nFire Perimeters\nGEOGLAM Crop Monitor\nGPM IMERG data of 2023 Medicane Daniel\nGRDI BUILT Constituent Raster\nGRDI CDR Constituent Raster\nGRDI Filled Missing Values Count\nGRDI IMR Constituent Raster\nGRDI SHDI Constituent Raster\nGRDI V1 raster\nGRDI VNL Constituent Raster\nGRDI VNL Slope Constituent Raster\nGlobal TWS Non-Stationarity Index\nGridded 2012 EPA Methane Emissions - Abandoned Coal Mines\nGridded 2012 EPA Methane Emissions - Composting\nGridded 2012 EPA Methane Emissions - Domestic Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Enteric Fermentation\nGridded 2012 EPA Methane Emissions - Ferroalloy Production\nGridded 2012 EPA Methane Emissions - Field Burning\nGridded 2012 EPA Methane Emissions - Field Burning (monthly)\nGridded 2012 EPA Methane Emissions - Forest Fires\nGridded 2012 EPA Methane Emissions - Forest Fires (daily)\nGridded 2012 EPA Methane Emissions - Industrial Landfills\nGridded 2012 EPA Methane Emissions - Industrial Wastewater Treatment\nGridded 2012 EPA Methane Emissions - Manure Management\nGridded 2012 EPA Methane Emissions - Manure Management (monthly)\nGridded 2012 EPA Methane Emissions - Mobile Combustion\nGridded 2012 EPA Methane Emissions - Municipal Landfills\nGridded 2012 EPA Methane Emissions - Natural Gas Distribution\nGridded 2012 EPA Methane Emissions - Natural Gas Processing\nGridded 2012 EPA Methane Emissions - Natural Gas Production\nGridded 2012 EPA Methane Emissions - Natural Gas Production (monthly)\nGridded 2012 EPA Methane Emissions - Natural Gas Transmission\nGridded 2012 EPA Methane Emissions - Petrochemical Production\nGridded 2012 EPA Methane Emissions - Petroleum\nGridded 2012 EPA Methane Emissions - Petroleum (monthly)\nGridded 2012 EPA Methane Emissions - Rice Cultivation\nGridded 2012 EPA Methane Emissions - Rice Cultivation (monthly)\nGridded 2012 EPA Methane Emissions - Stationary Combustion\nGridded 2012 EPA Methane Emissions - Stationary Combustion (monthly)\nGridded 2012 EPA Methane Emissions - Surface Coal Mines\nGridded 2012 EPA Methane Emissions - Underground Coal Mines\nGridded Daily OCO-2 Carbon Dioxide assimilated dataset\nGross Primary Productivity - LIS 10km Global DA\nGross Primary Productivity Trend - LIS 10km Global DA\nGroundwater Storage - LIS 10km Global DA\nHLS SWIR FalseColor Composite\nHLS-calculated BAIS2 burned area\nHLS-derived NDVI difference for Assessing Impacts from Hurricane Iann\nHLS-derived entropy difference for Assessing impacts from Hurricane Ian\nHLSL30.002 Environmental Justice Events\nHLSS30.002 Environmental Justice Events\nHouston AOD: Difference Between 2000-2009 & 2010-2019\nHouston LST (Diff)\nHouston Land Cover\nHouston NDVI: decadal average\nHouston land surface temperature at night time - decadal average\nHouston land surface temperature during daytime - decadal average\nHurricane Ida - Blue Tarps PlanetScope Image\nHurricane Ida - Detected Blue Tarps\nICESat-2 L4 Monthly Gridded Sea Ice Thickness (COGs)\nLandsat 8 Nighttime Thermal Imagery\nMTBS Burn Severity\nMaximum Fire Radiative Power for Thomas Fire\nNCEO Africa Aboveground Woody Biomass 2017\nNO₂\nNO₂ (Diff)\nNew Urbanization from 2001-2019 (NLCD)\nNormalized difference vegetation index from HLS\nOMI/Aura Sulfur Dioxide (SO2) Total Column L3 1 day Best Pixel in 0.25 degree x 0.25 degree V3 as Cloud-Optimized GeoTIFFs (COGs)\nOMI_trno2 - 0.10 x 0.10 Annual as Cloud-Optimized GeoTIFFs (COGs)\nPopulation Density Maps using satellite imagery built by Meta\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) average daily air temperature\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjected changes to winter (January, February, and March) cumulative daily precipitation\nProjections of Snow Water Equivalent (SWE) - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) - SSP5-8.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP2-4.5\nProjections of Snow Water Equivalent (SWE) Losses - SSP5-8.5\nRecovery Proxy Maps\nSelected Landsat 7 through 9 Surface Reflectance Scenes for Lake Balaton\nSelected Landsat 7 through 9 Surface Reflectance Scenes for Lake Biwa\nSelected Landsat 7 through 9 Surface Reflectance Scenes for Tonlé Sap\nSelected Landsat 7 through 9 Surface Reflectance Scenes for Vänern\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Aral Sea\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Pine Island Glacier\nSelected Landsat 7 through 9 Surface Reflectance Scenes for the Thwaites Glacier\nSlowdown Proxy Maps\nSnow Water Equivalent - LIS 10km Global DA\nSocial Vulnerability Index (Household)\nSocial Vulnerability Index (Household) (Masked)\nSocial Vulnerability Index (Housing)\nSocial Vulnerability Index (Housing) (Masked)\nSocial Vulnerability Index (Minority)\nSocial Vulnerability Index (Minority) (Masked)\nSocial Vulnerability Index (Overall)\nSocial Vulnerability Index (Overall) (Masked)\nSocial Vulnerability Index (SocioEconomic)\nSocial Vulnerability Index (SocioEconomic) (Masked)\nStream network across the Contiguous United States\nStreamflow - LIS 10km Global DA\nSubsurface Runoff - LIS 10km Global DA\nSurface runoff - LIS 10km Global DA\nTerrestrial Water Storage (TWS) Anomalies\nTerrestrial Water Storage - LIS 10km Global DA\nTerrestrial Water Storage Trend - LIS 10km Global DA\nTogo Agriculture\nTotal Precipitation - LIS 10km Global DA\nTrend in Terrestrial Water Storage (TWS) Anomalies\ndisalexi-etsuppression"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/list-collections.html#alternate-approachs",
    "href": "instance-management/notebooks/quickstarts/list-collections.html#alternate-approachs",
    "title": "List STAC collections",
    "section": "Alternate approachs",
    "text": "Alternate approachs\nInstead of exploring STAC catalog programatically, you can discover available collections the following ways:\n\nJSON API: https://openveda.cloud/api/stac/collections\nSTAC Browser: http://openveda.cloud"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#run-this-notebook",
    "title": "Open and plot COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#approach",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#approach",
    "title": "Open and plot COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nOpen the collection with xarray and stackstac\nPlot the data using hvplot"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#about-the-data",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#about-the-data",
    "title": "Open and plot COGs",
    "section": "About the data",
    "text": "About the data\nCDC’s Social Vulnerability Index (SVI) uses 15 variables at the census tract level. The data comes from the U.S. decennial census for the years 2000 & 2010, and the American Community Survey (ACS) for the years 2014, 2016, and 2018. It is a hierarchical additive index (Tate, 2013), with the component elements of CDC’s SVI including the following for 4 themes: Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation.\nSVI indicates the relative vulnerability of every U.S. Census tract–subdivisions of counties for which the Census collects statistical data. SVI ranks the tracts on 15 social factors, including unemployment, minority status, and disability, and further groups them into four related themes. Thus, each tract receives a ranking for each Census variable and for each of the four themes, as well as an overall ranking.\n\nScientific research\nThe SVI Overall Score provides the overall, summed social vulnerability score for a given tract. The Overall Score SVI Grid is part of the U.S. Census Grids collection, and displays the Center for Disease Control & Prevention (CDC) SVI score. Funding for the final development, processing and dissemination of this data set by the Socioeconomic Data and Applications Center (SEDAC) was provided under the U.S. National Aeronautics and Space Administration (NASA)¹.\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nInterpreting the data\nThe Overall SVI Score describes the vulnerability in a given county tract based on the combined percentile ranking of the four SVI scores (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). The summed percentile ranking from the four themes is ordered, and then used to calculate an overall percentile ranking, ranging from 0 (less vulnerable) to 1 (more vulnerable)². Tracts with higher Overall SVI Scores typically rank high in other SVI domains, and reveal communities that may require extra support, resources, and preventative care in order to better prepare for and manage emergency situations.\n\n\nCredits\n\nCenter for International Earth Science Information Network, (CIESIN), Columbia University. 2021. Documentation for the U.S. Social Vulnerability Index Grids. Palisades, NY: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/fjr9-a973. Accessed 13 May 2022.\nCenters for Disease Control and Prevention/ Agency for Toxic Substances and Disease Registry/ Geospatial Research, Analysis, and Services Program. CDC/ATSDR Social Vulnerability Index Database. https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/pdf/SVI2018Documentation_01192022_1.pdf\n\n\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#declare-your-collection-of-interest",
    "title": "Open and plot COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac/\"\ncollection = \"social-vulnerability-index-overall-nopop\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#find-items-in-collection",
    "title": "Open and plot COGs",
    "section": "Find items in collection",
    "text": "Find items in collection\nUse pystac_client to search the STAC collection.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection])\n\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 5 items"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#read-data",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#read-data",
    "title": "Open and plot COGs",
    "section": "Read data",
    "text": "Read data\nRead in data using xarray using a combination of xpystac, stackstac, and rasterio.\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": da.start_datetime}).squeeze()\n# da\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-6c45bf56d29299d91eb93a5bc48c5746' (time: 5,\n                                                                y: 6298,\n                                                                x: 13354)&gt; Size: 3GB\ndask.array&lt;getitem, shape=(5, 6298, 13354), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n    id              (time) &lt;U38 760B 'svi_2018_tract_overall_wgs84_nopop_cog'...\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 107kB -178.2 -178.2 -178.2 ... -66.97 -66.97\n  * y               (y) float64 50kB 71.38 71.37 71.37 ... 18.92 18.92 18.91\n    end_datetime    (time) &lt;U25 500B '2018-12-31T00:00:00+00:00' ... '2000-12...\n    start_datetime  (time) &lt;U25 500B '2018-01-01T00:00:00+00:00' ... '2000-01...\n    ...              ...\n    proj:geometry   object 8B {'type': 'Polygon', 'coordinates': [[[-178.2333...\n    proj:shape      object 8B {6297, 13353}\n    title           &lt;U17 68B 'Default COG Layer'\n    proj:bbox       object 8B {-178.23333334, 18.908332897999998, -66.9583337...\n    epsg            int64 8B 4326\n  * time            (time) &lt;U25 500B '2018-01-01T00:00:00+00:00' ... '2000-01...\nAttributes:\n    spec:           RasterSpec(epsg=4326, bounds=(-178.24166595386018, 18.899...\n    crs:            epsg:4326\n    transform:      | 0.01, 0.00,-178.24|\\n| 0.00,-0.01, 71.38|\\n| 0.00, 0.00...\n    resolution_xy:  (0.00833333330000749, 0.00833333329998412)xarray.DataArray'stackstac-6c45bf56d29299d91eb93a5bc48c5746'time: 5y: 6298x: 13354dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.13 GiB\n8.00 MiB\n\n\nShape\n(5, 6298, 13354)\n(1, 1024, 1024)\n\n\nDask graph\n490 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                       13354 6298 5\n\n\n\n\nCoordinates: (17)id(time)&lt;U38'svi_2018_tract_overall_wgs84_no...array(['svi_2018_tract_overall_wgs84_nopop_cog',\n       'svi_2016_tract_overall_wgs84_nopop_cog',\n       'svi_2014_tract_overall_wgs84_nopop_cog',\n       'svi_2010_tract_overall_wgs84_nopop_cog',\n       'svi_2000_tract_overall_wgs84_nopop_cog'], dtype='&lt;U38')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-178.2 -178.2 ... -66.97 -66.97array([-178.241666, -178.233333, -178.224999, ...,  -66.983333,  -66.975   ,\n        -66.966666])y(y)float6471.38 71.37 71.37 ... 18.92 18.91array([71.383333, 71.375   , 71.366666, ..., 18.925   , 18.916667, 18.908333])end_datetime(time)&lt;U25'2018-12-31T00:00:00+00:00' ... ...array(['2018-12-31T00:00:00+00:00', '2016-12-31T00:00:00+00:00',\n       '2014-12-31T00:00:00+00:00', '2010-12-31T00:00:00+00:00',\n       '2000-12-31T00:00:00+00:00'], dtype='&lt;U25')start_datetime(time)&lt;U25'2018-01-01T00:00:00+00:00' ... ...array(['2018-01-01T00:00:00+00:00', '2016-01-01T00:00:00+00:00',\n       '2014-01-01T00:00:00+00:00', '2010-01-01T00:00:00+00:00',\n       '2000-01-01T00:00:00+00:00'], dtype='&lt;U25')proj:projjson()object{'id': {'code': 4326, 'authority...array({'id': {'code': 4326, 'authority': 'EPSG'}, 'name': 'WGS 84', 'type': 'GeographicCRS', 'datum': {'name': 'World Geodetic System 1984', 'type': 'GeodeticReferenceFrame', 'ellipsoid': {'name': 'WGS 84', 'semi_major_axis': 6378137, 'inverse_flattening': 298.257223563}}, '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json', 'coordinate_system': {'axis': [{'name': 'Geodetic latitude', 'unit': 'degree', 'direction': 'north', 'abbreviation': 'Lat'}, {'name': 'Geodetic longitude', 'unit': 'degree', 'direction': 'east', 'abbreviation': 'Lon'}], 'subtype': 'ellipsoidal'}},\n      dtype=object)proj:epsg()int644326array(4326)proj:wkt2()&lt;U277'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984...array('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n      dtype='&lt;U277')proj:transform()object{0.00833333330000749, 0.0, 1.0, ...array({0.00833333330000749, 0.0, 1.0, -0.00833333329998412, 71.383332688, -178.23333334},\n      dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-178.23333334, 18.908332897999998], [-66.958333785, 18.908332897999998], [-66.958333785, 71.383332688], [-178.23333334, 71.383332688], [-178.23333334, 18.908332897999998]]]},\n      dtype=object)proj:shape()object{6297, 13353}array({6297, 13353}, dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')proj:bbox()object{-178.23333334, 18.9083328979999...array({-178.23333334, 18.908332897999998, -66.958333785, 71.383332688},\n      dtype=object)epsg()int644326array(4326)time(time)&lt;U25'2018-01-01T00:00:00+00:00' ... ...array(['2018-01-01T00:00:00+00:00', '2016-01-01T00:00:00+00:00',\n       '2014-01-01T00:00:00+00:00', '2010-01-01T00:00:00+00:00',\n       '2000-01-01T00:00:00+00:00'], dtype='&lt;U25')Indexes: (3)xPandasIndexPandasIndex(Index([-178.24166595386018, -178.23333262056016, -178.22499928726018,\n       -178.21666595396016, -178.20833262066014, -178.19999928736013,\n       -178.19166595406014, -178.18333262076013,  -178.1749992874601,\n       -178.16666595416012,\n       ...\n        -67.04166639856027,  -67.03333306526025,  -67.02499973196025,\n        -67.01666639866023,  -67.00833306536023,  -66.99999973206023,\n        -66.99166639876022,  -66.98333306546022,   -66.9749997321602,\n         -66.9666663988602],\n      dtype='float64', name='x', length=13354))yPandasIndexPandasIndex(Index([ 71.38333304766397,  71.37499971436398,    71.366666381064,\n        71.35833304776402,  71.34999971446403,  71.34166638116405,\n        71.33333304786406,  71.32499971456407,   71.3166663812641,\n        71.30833304796411,\n       ...\n       18.983333257363824, 18.974999924063845, 18.966666590763857,\n        18.95833325746387,  18.94999992416389, 18.941666590863903,\n       18.933333257563923, 18.924999924263936, 18.916666590963956,\n        18.90833325766397],\n      dtype='float64', name='y', length=6298))timePandasIndexPandasIndex(Index(['2018-01-01T00:00:00+00:00', '2016-01-01T00:00:00+00:00',\n       '2014-01-01T00:00:00+00:00', '2010-01-01T00:00:00+00:00',\n       '2000-01-01T00:00:00+00:00'],\n      dtype='object', name='time'))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-178.24166595386018, 18.899999924363982, -66.95833306556018, 71.38333304766397), resolutions_xy=(0.00833333330000749, 0.00833333329998412))crs :epsg:4326transform :| 0.01, 0.00,-178.24|\n| 0.00,-0.01, 71.38|\n| 0.00, 0.00, 1.00|resolution_xy :(0.00833333330000749, 0.00833333329998412)\n\n\nThere are 5 items representing the 5 years of data in the collection (2000, 2010, 2014, 2016, and 2018)."
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/open-and-plot.html#plot-data",
    "href": "instance-management/notebooks/quickstarts/open-and-plot.html#plot-data",
    "title": "Open and plot COGs",
    "section": "Plot data",
    "text": "Plot data\nPlot data using hvplot. By using rasterize=True we tell hvplot to use datashader behind the scenes to make the plot render more quickly and re-render on zoom.\n\n%%time\nda.hvplot(x=\"x\", y=\"y\", rasterize=True, clim=(0, 1), coastline=True, cmap=\"viridis\", widget_location=\"bottom\")\n\nCPU times: user 1.33 s, sys: 87.9 ms, total: 1.42 s\nWall time: 1.61 s"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#run-this-notebook",
    "title": "Open and visualize COGs",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#approach",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#approach",
    "title": "Open and visualize COGs",
    "section": "Approach",
    "text": "Approach\n\nUse pystac_client to open the STAC catalog and retrieve the items in the collection\nUse stackstac to create an xarray dataset containing all the items\nUse rioxarray to crop data to AOI\nUse hvplot to render the COG at every timestep\n\n\nimport requests\nfrom pystac_client import Client\nimport pandas as pd\nimport stackstac\n\nimport rioxarray  # noqa\nimport hvplot.xarray  # noqa"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#declare-your-collection-of-interest",
    "title": "Open and visualize COGs",
    "section": "Declare your collection of interest",
    "text": "Declare your collection of interest\nYou can discover available collections the following ways:\n\nProgrammatically: see example in the list-collections.ipynb notebook\nJSON API: https://staging-stac.delta-backend.com/collections\nSTAC Browser: http://veda-staging-stac-browser.s3-website-us-west-2.amazonaws.com\n\n\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\ncollection_id = \"no2-monthly\""
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#discover-items-in-collection-for-region-and-time-of-interest",
    "title": "Open and visualize COGs",
    "section": "Discover items in collection for region and time of interest",
    "text": "Discover items in collection for region and time of interest\nUse pystac_client to search the STAC collection for a particular area of interest within specified datetime bounds.\n\ncatalog = Client.open(STAC_API_URL)\nsearch = catalog.search(collections=[collection_id], sortby=\"start_datetime\")\n\nitem_collection = search.item_collection()\nprint(f\"Found {len(item_collection)} items\")\n\nFound 93 items"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#define-an-aoi",
    "title": "Open and visualize COGs",
    "section": "Define an AOI",
    "text": "Define an AOI\nWe can fetch GeoJSON for metropolitan France and Corsica (excluding overseas territories) from an authoritative online source (https://gadm.org/download_country.html).\n\nresponse = requests.get(\n    \"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_FRA_0.json\"\n)\n\n# If anything goes wrong with this request output error contents\nassert response.ok, response.text\n\nresult = response.json()\nprint(f\"There are {len(result['features'])} features in this collection\")\n\nThere are 1 features in this collection\n\n\nThat is the geojson for a feature collection, but since there is only one feature in it we can grab just that.\n\nfrance_aoi = result[\"features\"][0]"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#read-data",
    "title": "Open and visualize COGs",
    "section": "Read data",
    "text": "Read data\nCreate an xarray.DataArray using stackstac\n\nda = stackstac.stack(item_collection, epsg=4326)\nda = da.assign_coords({\"time\": da.start_datetime}).squeeze()\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-63f208ae3e2c01863c43588cc3899b3c' (time: 93,\n                                                                y: 1800, x: 3600)&gt; Size: 5GB\ndask.array&lt;getitem, shape=(93, 1800, 3600), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n    id              (time) &lt;U37 14kB 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ...\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 29kB -180.0 -179.9 -179.8 ... 179.7 179.8 179.9\n  * y               (y) float64 14kB 90.0 89.9 89.8 89.7 ... -89.7 -89.8 -89.9\n    start_datetime  (time) &lt;U25 9kB '2016-01-01T00:00:00+00:00' ... '2023-09-...\n    end_datetime    (time) &lt;U25 9kB '2016-01-31T00:00:00+00:00' ... '2023-09-...\n    ...              ...\n    proj:geometry   object 8B {'type': 'Polygon', 'coordinates': [[[-180.0, -...\n    description     &lt;U47 188B 'Cloud optimized default layer to display on map'\n    title           &lt;U17 68B 'Default COG Layer'\n    proj:epsg       int64 8B 4326\n    epsg            int64 8B 4326\n  * time            (time) &lt;U25 9kB '2016-01-01T00:00:00+00:00' ... '2023-09-...\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    crs:         epsg:4326\n    transform:   | 0.10, 0.00,-180.00|\\n| 0.00,-0.10, 90.00|\\n| 0.00, 0.00, 1...\n    resolution:  0.1xarray.DataArray'stackstac-63f208ae3e2c01863c43588cc3899b3c'time: 93y: 1800x: 3600dask.array&lt;chunksize=(1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.49 GiB\n8.00 MiB\n\n\nShape\n(93, 1800, 3600)\n(1, 1024, 1024)\n\n\nDask graph\n744 chunks in 4 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                     3600 1800 93\n\n\n\n\nCoordinates: (17)id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202301_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202302_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202303_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202304_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202305_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202306_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202307_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202308_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202309_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-180.0 -179.9 ... 179.8 179.9array([-180. , -179.9, -179.8, ...,  179.7,  179.8,  179.9])y(y)float6490.0 89.9 89.8 ... -89.8 -89.9array([ 90. ,  89.9,  89.8, ..., -89.7, -89.8, -89.9])start_datetime(time)&lt;U25'2016-01-01T00:00:00+00:00' ... ...array(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n...\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'], dtype='&lt;U25')end_datetime(time)&lt;U25'2016-01-31T00:00:00+00:00' ... ...array(['2016-01-31T00:00:00+00:00', '2016-02-29T00:00:00+00:00',\n       '2016-03-31T00:00:00+00:00', '2016-04-30T00:00:00+00:00',\n       '2016-05-31T00:00:00+00:00', '2016-06-30T00:00:00+00:00',\n       '2016-07-31T00:00:00+00:00', '2016-08-31T00:00:00+00:00',\n       '2016-09-30T00:00:00+00:00', '2016-10-31T00:00:00+00:00',\n       '2016-11-30T00:00:00+00:00', '2016-12-31T00:00:00+00:00',\n       '2017-01-31T00:00:00+00:00', '2017-02-28T00:00:00+00:00',\n       '2017-03-31T00:00:00+00:00', '2017-04-30T00:00:00+00:00',\n       '2017-05-31T00:00:00+00:00', '2017-06-30T00:00:00+00:00',\n       '2017-07-31T00:00:00+00:00', '2017-08-31T00:00:00+00:00',\n       '2017-09-30T00:00:00+00:00', '2017-10-31T00:00:00+00:00',\n       '2017-11-30T00:00:00+00:00', '2017-12-31T00:00:00+00:00',\n       '2018-01-31T00:00:00+00:00', '2018-02-28T00:00:00+00:00',\n       '2018-03-31T00:00:00+00:00', '2018-04-30T00:00:00+00:00',\n       '2018-05-31T00:00:00+00:00', '2018-06-30T00:00:00+00:00',\n       '2018-07-31T00:00:00+00:00', '2018-08-31T00:00:00+00:00',\n       '2018-09-30T00:00:00+00:00', '2018-10-31T00:00:00+00:00',\n       '2018-11-30T00:00:00+00:00', '2018-12-31T00:00:00+00:00',\n       '2019-01-31T00:00:00+00:00', '2019-02-28T00:00:00+00:00',\n       '2019-03-31T00:00:00+00:00', '2019-04-30T00:00:00+00:00',\n...\n       '2020-07-31T00:00:00+00:00', '2020-08-31T00:00:00+00:00',\n       '2020-09-30T00:00:00+00:00', '2020-10-31T00:00:00+00:00',\n       '2020-11-30T00:00:00+00:00', '2020-12-31T00:00:00+00:00',\n       '2021-01-31T00:00:00+00:00', '2021-02-28T00:00:00+00:00',\n       '2021-03-31T00:00:00+00:00', '2021-04-30T00:00:00+00:00',\n       '2021-05-31T00:00:00+00:00', '2021-06-30T00:00:00+00:00',\n       '2021-07-31T00:00:00+00:00', '2021-08-31T00:00:00+00:00',\n       '2021-09-30T00:00:00+00:00', '2021-10-31T00:00:00+00:00',\n       '2021-11-30T00:00:00+00:00', '2021-12-31T00:00:00+00:00',\n       '2022-01-31T00:00:00+00:00', '2022-02-28T00:00:00+00:00',\n       '2022-03-31T00:00:00+00:00', '2022-04-30T00:00:00+00:00',\n       '2022-05-31T00:00:00+00:00', '2022-06-30T00:00:00+00:00',\n       '2022-07-31T00:00:00+00:00', '2022-08-31T00:00:00+00:00',\n       '2022-09-30T00:00:00+00:00', '2022-10-31T00:00:00+00:00',\n       '2022-11-30T00:00:00+00:00', '2022-12-31T00:00:00+00:00',\n       '2023-01-31T00:00:00+00:00', '2023-02-28T00:00:00+00:00',\n       '2023-03-31T00:00:00+00:00', '2023-04-30T00:00:00+00:00',\n       '2023-05-31T00:00:00+00:00', '2023-06-30T00:00:00+00:00',\n       '2023-07-31T00:00:00+00:00', '2023-08-31T00:00:00+00:00',\n       '2023-09-30T00:00:00+00:00'], dtype='&lt;U25')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:projjson()object{'id': {'code': 4326, 'authority...array({'id': {'code': 4326, 'authority': 'EPSG'}, 'name': 'WGS 84', 'type': 'GeographicCRS', 'datum': {'name': 'World Geodetic System 1984', 'type': 'GeodeticReferenceFrame', 'ellipsoid': {'name': 'WGS 84', 'semi_major_axis': 6378137, 'inverse_flattening': 298.257223563}}, '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json', 'coordinate_system': {'axis': [{'name': 'Geodetic latitude', 'unit': 'degree', 'direction': 'north', 'abbreviation': 'Lat'}, {'name': 'Geodetic longitude', 'unit': 'degree', 'direction': 'east', 'abbreviation': 'Lon'}], 'subtype': 'ellipsoidal'}},\n      dtype=object)proj:shape()object{1800, 3600}array({1800, 3600}, dtype=object)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:wkt2()&lt;U277'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984...array('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n      dtype='&lt;U277')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')proj:epsg()int644326array(4326)epsg()int644326array(4326)time(time)&lt;U25'2016-01-01T00:00:00+00:00' ... ...array(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'], dtype='&lt;U25')Indexes: (3)xPandasIndexPandasIndex(Index([            -180.0,             -179.9,             -179.8,\n                   -179.7,             -179.6,             -179.5,\n                   -179.4,             -179.3,             -179.2,\n                   -179.1,\n       ...\n                    179.0, 179.10000000000002, 179.20000000000005,\n                    179.3, 179.40000000000003,              179.5,\n       179.60000000000002, 179.70000000000005,              179.8,\n       179.90000000000003],\n      dtype='float64', name='x', length=3600))yPandasIndexPandasIndex(Index([              90.0,               89.9,               89.8,\n                     89.7,               89.6,               89.5,\n                     89.4,               89.3,               89.2,\n                     89.1,\n       ...\n                    -89.0, -89.10000000000002, -89.20000000000002,\n       -89.30000000000001,              -89.4,              -89.5,\n       -89.60000000000002, -89.70000000000002, -89.80000000000001,\n                    -89.9],\n      dtype='float64', name='y', length=1800))timePandasIndexPandasIndex(Index(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'],\n      dtype='object', name='time'))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))crs :epsg:4326transform :| 0.10, 0.00,-180.00|\n| 0.00,-0.10, 90.00|\n| 0.00, 0.00, 1.00|resolution :0.1"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#clip-the-data-to-aoi",
    "title": "Open and visualize COGs",
    "section": "Clip the data to AOI",
    "text": "Clip the data to AOI\n\nsubset = da.rio.clip([france_aoi[\"geometry\"]])\nsubset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-63f208ae3e2c01863c43588cc3899b3c' (time: 93,\n                                                                y: 97, x: 143)&gt; Size: 10MB\ndask.array&lt;getitem, shape=(93, 97, 143), dtype=float64, chunksize=(1, 97, 143), chunktype=numpy.ndarray&gt;\nCoordinates: (12/18)\n    id              (time) &lt;U37 14kB 'OMI_trno2_0.10x0.10_201601_Col3_V4.nc' ...\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 1kB -4.7 -4.6 -4.5 -4.4 -4.3 ... 9.2 9.3 9.4 9.5\n  * y               (y) float64 776B 51.0 50.9 50.8 50.7 ... 41.7 41.6 41.5 41.4\n    start_datetime  (time) &lt;U25 9kB '2016-01-01T00:00:00+00:00' ... '2023-09-...\n    end_datetime    (time) &lt;U25 9kB '2016-01-31T00:00:00+00:00' ... '2023-09-...\n    ...              ...\n    description     &lt;U47 188B 'Cloud optimized default layer to display on map'\n    title           &lt;U17 68B 'Default COG Layer'\n    proj:epsg       int64 8B 4326\n    epsg            int64 8B 4326\n  * time            (time) &lt;U25 9kB '2016-01-01T00:00:00+00:00' ... '2023-09-...\n    spatial_ref     int64 8B 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), r...\n    resolution:  0.1xarray.DataArray'stackstac-63f208ae3e2c01863c43588cc3899b3c'time: 93y: 97x: 143dask.array&lt;chunksize=(1, 97, 143), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n9.84 MiB\n108.37 kiB\n\n\nShape\n(93, 97, 143)\n(1, 97, 143)\n\n\nDask graph\n93 chunks in 8 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                             143 97 93\n\n\n\n\nCoordinates: (18)id(time)&lt;U37'OMI_trno2_0.10x0.10_201601_Col3...array(['OMI_trno2_0.10x0.10_201601_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201602_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201603_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201604_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201605_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201606_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201607_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201608_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201609_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201610_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201611_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201612_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201701_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201702_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201703_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201704_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201705_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201706_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201707_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_201708_Col3_V4.nc',\n...\n       'OMI_trno2_0.10x0.10_202202_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202203_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202204_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202205_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202206_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202207_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202208_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202209_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202210_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202211_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202301_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202302_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202303_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202304_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202305_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202306_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202307_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202308_Col3_V4.nc',\n       'OMI_trno2_0.10x0.10_202309_Col3_V4.nc'], dtype='&lt;U37')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-4.7 -4.6 -4.5 -4.4 ... 9.3 9.4 9.5axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4. , -3.9, -3.8, -3.7, -3.6,\n       -3.5, -3.4, -3.3, -3.2, -3.1, -3. , -2.9, -2.8, -2.7, -2.6, -2.5, -2.4,\n       -2.3, -2.2, -2.1, -2. , -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2,\n       -1.1, -1. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,\n        1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,\n        2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,\n        3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,\n        4.9,  5. ,  5.1,  5.2,  5.3,  5.4,  5.5,  5.6,  5.7,  5.8,  5.9,  6. ,\n        6.1,  6.2,  6.3,  6.4,  6.5,  6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,\n        7.3,  7.4,  7.5,  7.6,  7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,\n        8.5,  8.6,  8.7,  8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5])y(y)float6451.0 50.9 50.8 ... 41.6 41.5 41.4axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([51. , 50.9, 50.8, 50.7, 50.6, 50.5, 50.4, 50.3, 50.2, 50.1, 50. , 49.9,\n       49.8, 49.7, 49.6, 49.5, 49.4, 49.3, 49.2, 49.1, 49. , 48.9, 48.8, 48.7,\n       48.6, 48.5, 48.4, 48.3, 48.2, 48.1, 48. , 47.9, 47.8, 47.7, 47.6, 47.5,\n       47.4, 47.3, 47.2, 47.1, 47. , 46.9, 46.8, 46.7, 46.6, 46.5, 46.4, 46.3,\n       46.2, 46.1, 46. , 45.9, 45.8, 45.7, 45.6, 45.5, 45.4, 45.3, 45.2, 45.1,\n       45. , 44.9, 44.8, 44.7, 44.6, 44.5, 44.4, 44.3, 44.2, 44.1, 44. , 43.9,\n       43.8, 43.7, 43.6, 43.5, 43.4, 43.3, 43.2, 43.1, 43. , 42.9, 42.8, 42.7,\n       42.6, 42.5, 42.4, 42.3, 42.2, 42.1, 42. , 41.9, 41.8, 41.7, 41.6, 41.5,\n       41.4])start_datetime(time)&lt;U25'2016-01-01T00:00:00+00:00' ... ...array(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n...\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'], dtype='&lt;U25')end_datetime(time)&lt;U25'2016-01-31T00:00:00+00:00' ... ...array(['2016-01-31T00:00:00+00:00', '2016-02-29T00:00:00+00:00',\n       '2016-03-31T00:00:00+00:00', '2016-04-30T00:00:00+00:00',\n       '2016-05-31T00:00:00+00:00', '2016-06-30T00:00:00+00:00',\n       '2016-07-31T00:00:00+00:00', '2016-08-31T00:00:00+00:00',\n       '2016-09-30T00:00:00+00:00', '2016-10-31T00:00:00+00:00',\n       '2016-11-30T00:00:00+00:00', '2016-12-31T00:00:00+00:00',\n       '2017-01-31T00:00:00+00:00', '2017-02-28T00:00:00+00:00',\n       '2017-03-31T00:00:00+00:00', '2017-04-30T00:00:00+00:00',\n       '2017-05-31T00:00:00+00:00', '2017-06-30T00:00:00+00:00',\n       '2017-07-31T00:00:00+00:00', '2017-08-31T00:00:00+00:00',\n       '2017-09-30T00:00:00+00:00', '2017-10-31T00:00:00+00:00',\n       '2017-11-30T00:00:00+00:00', '2017-12-31T00:00:00+00:00',\n       '2018-01-31T00:00:00+00:00', '2018-02-28T00:00:00+00:00',\n       '2018-03-31T00:00:00+00:00', '2018-04-30T00:00:00+00:00',\n       '2018-05-31T00:00:00+00:00', '2018-06-30T00:00:00+00:00',\n       '2018-07-31T00:00:00+00:00', '2018-08-31T00:00:00+00:00',\n       '2018-09-30T00:00:00+00:00', '2018-10-31T00:00:00+00:00',\n       '2018-11-30T00:00:00+00:00', '2018-12-31T00:00:00+00:00',\n       '2019-01-31T00:00:00+00:00', '2019-02-28T00:00:00+00:00',\n       '2019-03-31T00:00:00+00:00', '2019-04-30T00:00:00+00:00',\n...\n       '2020-07-31T00:00:00+00:00', '2020-08-31T00:00:00+00:00',\n       '2020-09-30T00:00:00+00:00', '2020-10-31T00:00:00+00:00',\n       '2020-11-30T00:00:00+00:00', '2020-12-31T00:00:00+00:00',\n       '2021-01-31T00:00:00+00:00', '2021-02-28T00:00:00+00:00',\n       '2021-03-31T00:00:00+00:00', '2021-04-30T00:00:00+00:00',\n       '2021-05-31T00:00:00+00:00', '2021-06-30T00:00:00+00:00',\n       '2021-07-31T00:00:00+00:00', '2021-08-31T00:00:00+00:00',\n       '2021-09-30T00:00:00+00:00', '2021-10-31T00:00:00+00:00',\n       '2021-11-30T00:00:00+00:00', '2021-12-31T00:00:00+00:00',\n       '2022-01-31T00:00:00+00:00', '2022-02-28T00:00:00+00:00',\n       '2022-03-31T00:00:00+00:00', '2022-04-30T00:00:00+00:00',\n       '2022-05-31T00:00:00+00:00', '2022-06-30T00:00:00+00:00',\n       '2022-07-31T00:00:00+00:00', '2022-08-31T00:00:00+00:00',\n       '2022-09-30T00:00:00+00:00', '2022-10-31T00:00:00+00:00',\n       '2022-11-30T00:00:00+00:00', '2022-12-31T00:00:00+00:00',\n       '2023-01-31T00:00:00+00:00', '2023-02-28T00:00:00+00:00',\n       '2023-03-31T00:00:00+00:00', '2023-04-30T00:00:00+00:00',\n       '2023-05-31T00:00:00+00:00', '2023-06-30T00:00:00+00:00',\n       '2023-07-31T00:00:00+00:00', '2023-08-31T00:00:00+00:00',\n       '2023-09-30T00:00:00+00:00'], dtype='&lt;U25')proj:bbox()object{90.0, 180.0, -90.0, -180.0}array({90.0, 180.0, -90.0, -180.0}, dtype=object)proj:projjson()object{'id': {'code': 4326, 'authority...array({'id': {'code': 4326, 'authority': 'EPSG'}, 'name': 'WGS 84', 'type': 'GeographicCRS', 'datum': {'name': 'World Geodetic System 1984', 'type': 'GeodeticReferenceFrame', 'ellipsoid': {'name': 'WGS 84', 'semi_major_axis': 6378137, 'inverse_flattening': 298.257223563}}, '$schema': 'https://proj.org/schemas/v0.4/projjson.schema.json', 'coordinate_system': {'axis': [{'name': 'Geodetic latitude', 'unit': 'degree', 'direction': 'north', 'abbreviation': 'Lat'}, {'name': 'Geodetic longitude', 'unit': 'degree', 'direction': 'east', 'abbreviation': 'Lon'}], 'subtype': 'ellipsoidal'}},\n      dtype=object)proj:shape()object{1800, 3600}array({1800, 3600}, dtype=object)proj:transform()object{0.1, 0.0, 1.0, -0.1, -180.0, 90.0}array({0.1, 0.0, 1.0, -0.1, -180.0, 90.0}, dtype=object)proj:wkt2()&lt;U277'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984...array('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n      dtype='&lt;U277')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-180.0, -90.0], [180.0, -90.0], [180.0, 90.0], [-180.0, 90.0], [-180.0, -90.0]]]},\n      dtype=object)description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')proj:epsg()int644326array(4326)epsg()int644326array(4326)time(time)&lt;U25'2016-01-01T00:00:00+00:00' ... ...array(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'], dtype='&lt;U25')spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-4.7499999999999885 0.09999999999999992 0.0 51.05 0.0 -0.10000000000000002array(0)Indexes: (3)xPandasIndexPandasIndex(Index([ -4.699999999999989,  -4.599999999999994,                -4.5,\n        -4.399999999999977,  -4.299999999999983,  -4.199999999999989,\n        -4.099999999999994,                -4.0, -3.8999999999999773,\n        -3.799999999999983,\n       ...\n         8.600000000000023,   8.700000000000017,   8.800000000000011,\n         8.900000000000006,                 9.0,   9.100000000000023,\n         9.200000000000017,   9.300000000000011,   9.400000000000006,\n                       9.5],\n      dtype='float64', name='x', length=143))yPandasIndexPandasIndex(Index([              51.0,               50.9,               50.8,\n       50.699999999999996, 50.599999999999994,               50.5,\n                     50.4,               50.3, 50.199999999999996,\n       50.099999999999994,               50.0,               49.9,\n                     49.8, 49.699999999999996, 49.599999999999994,\n                     49.5,               49.4,               49.3,\n       49.199999999999996, 49.099999999999994,               49.0,\n                     48.9,               48.8, 48.699999999999996,\n       48.599999999999994,               48.5,               48.4,\n                     48.3, 48.199999999999996, 48.099999999999994,\n                     48.0,               47.9,               47.8,\n       47.699999999999996, 47.599999999999994,               47.5,\n                     47.4,               47.3, 47.199999999999996,\n       47.099999999999994,               47.0,               46.9,\n                     46.8, 46.699999999999996, 46.599999999999994,\n                     46.5,               46.4,               46.3,\n       46.199999999999996, 46.099999999999994,               46.0,\n                     45.9,               45.8, 45.699999999999996,\n       45.599999999999994,               45.5,               45.4,\n                     45.3, 45.199999999999996, 45.099999999999994,\n                     45.0,               44.9,               44.8,\n       44.699999999999996, 44.599999999999994,               44.5,\n                     44.4,               44.3, 44.199999999999996,\n       44.099999999999994,               44.0,               43.9,\n                     43.8, 43.699999999999996, 43.599999999999994,\n                     43.5,               43.4,               43.3,\n       43.199999999999996, 43.099999999999994,               43.0,\n                     42.9,               42.8, 42.699999999999996,\n       42.599999999999994,               42.5,               42.4,\n                     42.3, 42.199999999999996, 42.099999999999994,\n                     42.0,               41.9,               41.8,\n       41.699999999999996, 41.599999999999994,               41.5,\n                     41.4],\n      dtype='float64', name='y'))timePandasIndexPandasIndex(Index(['2016-01-01T00:00:00+00:00', '2016-02-01T00:00:00+00:00',\n       '2016-03-01T00:00:00+00:00', '2016-04-01T00:00:00+00:00',\n       '2016-05-01T00:00:00+00:00', '2016-06-01T00:00:00+00:00',\n       '2016-07-01T00:00:00+00:00', '2016-08-01T00:00:00+00:00',\n       '2016-09-01T00:00:00+00:00', '2016-10-01T00:00:00+00:00',\n       '2016-11-01T00:00:00+00:00', '2016-12-01T00:00:00+00:00',\n       '2017-01-01T00:00:00+00:00', '2017-02-01T00:00:00+00:00',\n       '2017-03-01T00:00:00+00:00', '2017-04-01T00:00:00+00:00',\n       '2017-05-01T00:00:00+00:00', '2017-06-01T00:00:00+00:00',\n       '2017-07-01T00:00:00+00:00', '2017-08-01T00:00:00+00:00',\n       '2017-09-01T00:00:00+00:00', '2017-10-01T00:00:00+00:00',\n       '2017-11-01T00:00:00+00:00', '2017-12-01T00:00:00+00:00',\n       '2018-01-01T00:00:00+00:00', '2018-02-01T00:00:00+00:00',\n       '2018-03-01T00:00:00+00:00', '2018-04-01T00:00:00+00:00',\n       '2018-05-01T00:00:00+00:00', '2018-06-01T00:00:00+00:00',\n       '2018-07-01T00:00:00+00:00', '2018-08-01T00:00:00+00:00',\n       '2018-09-01T00:00:00+00:00', '2018-10-01T00:00:00+00:00',\n       '2018-11-01T00:00:00+00:00', '2018-12-01T00:00:00+00:00',\n       '2019-01-01T00:00:00+00:00', '2019-02-01T00:00:00+00:00',\n       '2019-03-01T00:00:00+00:00', '2019-04-01T00:00:00+00:00',\n       '2019-05-01T00:00:00+00:00', '2019-06-01T00:00:00+00:00',\n       '2019-07-01T00:00:00+00:00', '2019-08-01T00:00:00+00:00',\n       '2019-09-01T00:00:00+00:00', '2019-10-01T00:00:00+00:00',\n       '2019-11-01T00:00:00+00:00', '2019-12-01T00:00:00+00:00',\n       '2020-01-01T00:00:00+00:00', '2020-02-01T00:00:00+00:00',\n       '2020-03-01T00:00:00+00:00', '2020-04-01T00:00:00+00:00',\n       '2020-05-01T00:00:00+00:00', '2020-06-01T00:00:00+00:00',\n       '2020-07-01T00:00:00+00:00', '2020-08-01T00:00:00+00:00',\n       '2020-09-01T00:00:00+00:00', '2020-10-01T00:00:00+00:00',\n       '2020-11-01T00:00:00+00:00', '2020-12-01T00:00:00+00:00',\n       '2021-01-01T00:00:00+00:00', '2021-02-01T00:00:00+00:00',\n       '2021-03-01T00:00:00+00:00', '2021-04-01T00:00:00+00:00',\n       '2021-05-01T00:00:00+00:00', '2021-06-01T00:00:00+00:00',\n       '2021-07-01T00:00:00+00:00', '2021-08-01T00:00:00+00:00',\n       '2021-09-01T00:00:00+00:00', '2021-10-01T00:00:00+00:00',\n       '2021-11-01T00:00:00+00:00', '2021-12-01T00:00:00+00:00',\n       '2022-01-01T00:00:00+00:00', '2022-02-01T00:00:00+00:00',\n       '2022-03-01T00:00:00+00:00', '2022-04-01T00:00:00+00:00',\n       '2022-05-01T00:00:00+00:00', '2022-06-01T00:00:00+00:00',\n       '2022-07-01T00:00:00+00:00', '2022-08-01T00:00:00+00:00',\n       '2022-09-01T00:00:00+00:00', '2022-10-01T00:00:00+00:00',\n       '2022-11-01T00:00:00+00:00', '2022-12-01T00:00:00+00:00',\n       '2023-01-01T00:00:00+00:00', '2023-02-01T00:00:00+00:00',\n       '2023-03-01T00:00:00+00:00', '2023-04-01T00:00:00+00:00',\n       '2023-05-01T00:00:00+00:00', '2023-06-01T00:00:00+00:00',\n       '2023-07-01T00:00:00+00:00', '2023-08-01T00:00:00+00:00',\n       '2023-09-01T00:00:00+00:00'],\n      dtype='object', name='time'))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-180.0, -90.0, 180.0, 90.0), resolutions_xy=(0.1, 0.1))resolution :0.1"
  },
  {
    "objectID": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "href": "instance-management/notebooks/quickstarts/visualize-multiple-times.html#compute-and-plot",
    "title": "Open and visualize COGs",
    "section": "Compute and plot",
    "text": "Compute and plot\nSo far we have just been setting up a calculation lazily in Dask. Now we can trigger computation using .compute().\n\n%%time\n\nimage_stack = subset.compute()\n\nCPU times: user 2.96 s, sys: 658 ms, total: 3.62 s\nWall time: 8.35 s\n\n\n\n# get the 2% and 98% percentiles for min and max bounds of color\nvmin, vmax = image_stack.quantile(0.02).item(), image_stack.quantile(0.98).item()\n\nimage_stack.hvplot(\n    groupby=\"time\",\n    tiles=True,\n    colorbar=False,\n    clim=(vmin, vmax),\n    cmap=\"viridis\",\n    alpha=0.8,\n    frame_height=512,\n    widget_location=\"bottom\",\n)"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "href": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "href": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#approach",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Approach",
    "text": "Approach\nCloud-optimized GeoTIFF (COG) is a geospatial raster (image) data format optimized for on-the-fly analytics and visualization of raster data in cloud applications.\nConverting NetCDF (climate data) to COG can be relevant when the data should be included in GIS or web map applications.\nThis tutorial shows how this conversion can be done using Xarray and rioxarray, in-memory, avoiding temporary files on-disk.\n\nStep-by-step guide to conversion from NetCDF to Cloud-Optimized GeoTIFF\nCombined workflow including upload to S3"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "href": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#step-by-step",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Step by step",
    "text": "Step by step\n\nStep 0 - Installs\n\nimport boto3\nimport rasterio  # noqa\nimport rio_cogeo.cogeo\nimport rioxarray  # noqa\nimport s3fs\nimport xarray as xr\nfrom rasterio.io import MemoryFile\n\n\n\nStep 1 - Inspect source NetCDF\n\nSOURCE_URI = (\n    \"cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc\"\n)\n\n\nfs = s3fs.S3FileSystem()\nfileobj = fs.open(SOURCE_URI)\nds = xr.open_dataset(fileobj, engine=\"h5netcdf\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 40MB\nDimensions:  (lat: 600, lon: 1440)\nCoordinates:\n  * lat      (lat) float64 5kB -59.88 -59.62 -59.38 -59.12 ... 89.38 89.62 89.88\n  * lon      (lon) float64 12kB 0.125 0.375 0.625 0.875 ... 359.4 359.6 359.9\nData variables: (12/13)\n    FFMC     (lat, lon) float32 3MB ...\n    DMC      (lat, lon) float32 3MB ...\n    DC       (lat, lon) float32 3MB ...\n    ISI      (lat, lon) float32 3MB ...\n    BUI      (lat, lon) float32 3MB ...\n    FWI      (lat, lon) float32 3MB ...\n    ...       ...\n    FWI_N30  (lat, lon) uint16 2MB ...\n    FWI_N45  (lat, lon) uint16 2MB ...\n    FWI_P25  (lat, lon) float32 3MB ...\n    FWI_P50  (lat, lon) float32 3MB ...\n    FWI_P75  (lat, lon) float32 3MB ...\n    FWI_P95  (lat, lon) float32 3MB ...xarray.DatasetDimensions:lat: 600lon: 1440Coordinates: (2)lat(lat)float64-59.88 -59.62 ... 89.62 89.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([-59.875, -59.625, -59.375, ...,  89.375,  89.625,  89.875])lon(lon)float640.125 0.375 0.625 ... 359.6 359.9units :degrees_eaststandard_name :longitudelong_name :longitudeaxis :Xarray([1.25000e-01, 3.75000e-01, 6.25000e-01, ..., 3.59375e+02, 3.59625e+02,\n       3.59875e+02])Data variables: (13)FFMC(lat, lon)float32...[864000 values with dtype=float32]DMC(lat, lon)float32...[864000 values with dtype=float32]DC(lat, lon)float32...[864000 values with dtype=float32]ISI(lat, lon)float32...[864000 values with dtype=float32]BUI(lat, lon)float32...[864000 values with dtype=float32]FWI(lat, lon)float32...[864000 values with dtype=float32]FWI_N15(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N30(lat, lon)uint16...[864000 values with dtype=uint16]FWI_N45(lat, lon)uint16...[864000 values with dtype=uint16]FWI_P25(lat, lon)float32...[864000 values with dtype=float32]FWI_P50(lat, lon)float32...[864000 values with dtype=float32]FWI_P75(lat, lon)float32...[864000 values with dtype=float32]FWI_P95(lat, lon)float32...[864000 values with dtype=float32]Indexes: (2)latPandasIndexPandasIndex(Index([-59.875, -59.625, -59.375, -59.125, -58.875, -58.625, -58.375, -58.125,\n       -57.875, -57.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([  0.125,   0.375,   0.625,   0.875,   1.125,   1.375,   1.625,   1.875,\n         2.125,   2.375,\n       ...\n       357.625, 357.875, 358.125, 358.375, 358.625, 358.875, 359.125, 359.375,\n       359.625, 359.875],\n      dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\n\n\nStep 2 - Select data variable\nThe NetCDF contains several data variables. We pick the first one for demo.\n\nVARIABLE_NAME = \"FFMC\"\n\n\nda = ds[VARIABLE_NAME]\n\n\nda.plot();\n\n\n\n\n\n\n\n\n\nda.encoding\n\n{'chunksizes': None,\n 'fletcher32': False,\n 'shuffle': False,\n 'source': '&lt;File-like object S3FileSystem, cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc&gt;',\n 'original_shape': (600, 1440),\n 'dtype': dtype('&lt;f4'),\n '_FillValue': nan}\n\n\n\n\nStep 3. Conform to raster data conventions\nCommon practice in NetCDF lat/lon data the first grid cell is the south-west corner, i.e. latitude and longitude axes increase along the array dimensions.\nCommon practice in raster formats like GeoTIFF is that the y-axis (latitude in this case) decreases from origin, i.e. the first pixel is the north-west corner.\nWe can reverse the latitude dimension like this:\n\nda = da.isel(lat=slice(None, None, -1))\nda.lat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lat' (lat: 600)&gt; Size: 5kB\narray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])\nCoordinates:\n  * lat      (lat) float64 5kB 89.88 89.62 89.38 89.12 ... -59.38 -59.62 -59.88\nAttributes:\n    units:          degrees_north\n    standard_name:  latitude\n    long_name:      latitude\n    axis:           Yxarray.DataArray'lat'lat: 60089.88 89.62 89.38 89.12 88.88 ... -58.88 -59.12 -59.38 -59.62 -59.88array([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Coordinates: (1)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])Indexes: (1)latPandasIndexPandasIndex(Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,  88.125,\n        87.875,  87.625,\n       ...\n       -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125, -59.375,\n       -59.625, -59.875],\n      dtype='float64', name='lat', length=600))Attributes: (4)units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Y\n\n\nWe would also like the longitude axis to range from -180 to 180 degrees east, instead of 0 to 360 degrees east (matter of taste, not convention).\n\nda = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\nda.lon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'lon' (lon: 1440)&gt; Size: 12kB\narray([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])\nCoordinates:\n  * lon      (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9xarray.DataArray'lon'lon: 1440-179.9 -179.6 -179.4 -179.1 -178.9 ... 178.9 179.1 179.4 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Coordinates: (1)lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])Indexes: (1)lonPandasIndexPandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck that the data still looks right, just rotated along x-axis:\n\nda.plot();\n\n\n\n\n\n\n\n\nNow we need to set raster data attributes which are missing from the NetCDF, to help rio-xarray infer the raster information\n\nda.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\nda.rio.write_crs(\"epsg:4326\", inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'FFMC' (lat: 600, lon: 1440)&gt; Size: 3MB\n[864000 values with dtype=float32]\nCoordinates:\n  * lat          (lat) float64 5kB 89.88 89.62 89.38 ... -59.38 -59.62 -59.88\n  * lon          (lon) float64 12kB -179.9 -179.6 -179.4 ... 179.4 179.6 179.9\n    spatial_ref  int64 8B 0xarray.DataArray'FFMC'lat: 600lon: 1440...[864000 values with dtype=float32]Coordinates: (3)lat(lat)float6489.88 89.62 89.38 ... -59.62 -59.88units :degrees_northstandard_name :latitudelong_name :latitudeaxis :Yarray([ 89.875,  89.625,  89.375, ..., -59.375, -59.625, -59.875])lon(lon)float64-179.9 -179.6 ... 179.6 179.9array([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Indexes: (2)latPandasIndexPandasIndex(Index([ 89.875,  89.625,  89.375,  89.125,  88.875,  88.625,  88.375,  88.125,\n        87.875,  87.625,\n       ...\n       -57.625, -57.875, -58.125, -58.375, -58.625, -58.875, -59.125, -59.375,\n       -59.625, -59.875],\n      dtype='float64', name='lat', length=600))lonPandasIndexPandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float64', name='lon', length=1440))Attributes: (0)\n\n\nCheck CRS\n\nda.rio.crs\n\nCRS.from_epsg(4326)\n\n\nCheck affine image transform:\na = width of a pixel\nb = row rotation (typically zero)\nc = x-coordinate of the upper-left corner of the upper-left pixel\nd = column rotation (typically zero)\ne = height of a pixel (typically negative)\nf = y-coordinate of the of the upper-left corner of the upper-left pixel\n\nda.rio.transform()\n\nAffine(0.25, 0.0, -180.0,\n       0.0, -0.25, 90.0)\n\n\n\n\nStep 4 - Write to COG and validate in-memory\nFor the demonstration here, we do not write the file to disk but to a memory file which can be validated and uploaded in-memory\nGeoTIFFs / COGs can be tuned for performance. Here are some defaults we found to work well (check out this blog post for detail).\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\", \"predictor\": 2}\n\n\nwith MemoryFile() as memfile:\n    da.rio.to_raster(memfile.name, **COG_PROFILE)\n\n    cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n\ncog_valid\n\nTrue"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "href": "instance-management/notebooks/tutorials/netcdf-to-cog-cmip6.html#combined-workflow-from-conversion-to-upload",
    "title": "Converting NetCDF to COG (CMIP6)",
    "section": "Combined workflow from conversion to upload",
    "text": "Combined workflow from conversion to upload\n\nSOURCE_URI = (\n    \"cmip6-staging/Sample/FWI/Yearly/MME/MME50_historical_fwi_metrics_yearly_2014.nc\"\n)\n\n\nVARIABLE_NAME = \"FFMC\"\n\n\nCOG_PROFILE = {\"driver\": \"COG\", \"compress\": \"DEFLATE\", \"predictor\": 2}\n\n\nDESTINATION_BUCKET = None\nDESTINATION_KEY = None\n\n\nfs = s3fs.S3FileSystem()\nwith fs.open(SOURCE_URI) as fileobj:\n    with xr.open_dataset(fileobj, engine=\"h5netcdf\") as ds:\n\n        # Read individual metric into data array (only one time in yearly NetCDFs)\n        da = ds[VARIABLE_NAME]\n\n        # Realign the x dimension to -180 origin for dataset\n        da = da.assign_coords(lon=(((da.lon + 180) % 360) - 180)).sortby(\"lon\")\n\n        # Reverse the DataArray's y dimension to comply with raster common practice\n        if da.lat.values[-1] &gt; da.lat.values[0]:\n            da = da.isel(lat=slice(None, None, -1))\n\n        # Set raster attributes\n        da.rio.set_spatial_dims(\"lon\", \"lat\")\n        da.rio.write_crs(\"epsg:4326\", inplace=True)\n\n        with MemoryFile() as memfile:\n            da.rio.to_raster(memfile.name, **COG_PROFILE)\n\n            # Validate COG in-memory\n            cog_valid = rio_cogeo.cogeo.cog_validate(memfile.name)[0]\n            if not cog_valid:\n                raise RuntimeError(\"COG validation failed.\")\n\n            # Upload to S3\n            if DESTINATION_BUCKET is not None:\n                client = boto3.client(\"s3\")\n                r = client.put_object(\n                    Body=memfile,\n                    Bucket=DESTINATION_BUCKET,\n                    Key=DESTINATION_KEY,\n                )\n                if r[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n                    raise RuntimeError(\"Upload failed.\")"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "It is recommended to run through this tutorial using VEDA’s Pangeo Notebook image, which already includes the stac_ipyleaflet library. When working within the notebook, be sure to select the nasa-veda-singleuser kernel.\nTo access VEDA’s JupyterHub Environment, please refer to the “Getting access…” section of our documentation.\nFor reference: stac_ipyleafet repository\n\n\n\n\nimport stac_ipyleaflet\nm = stac_ipyleaflet.StacIpyleaflet()\nm\n\nThe stac ipyleaflet notebook’s user interface consists of a map and a custom set of tools to aid in the discovery and visualization of STAC datasets and pre-determined Basemaps.\n\n\n\n\n\n\n\npress and hold a mouse-click, then drag the map\n\n\n\n\n\nclick the Zoom In / Out buttons in the top left-corner (this will maintain the center)\nuse your mouse’s scroll-wheel - hovering over an area of interest\ndouble-click within the map on an area of interest\nwhile pressing the shift key on your keyboard, press and hold a mouse-click, then drag to draw a rectangle around the area of interest\n\n\n\n\n\n\nPressing the Layers button at the top opens the Layers widget that consists of 2 tabs. This tool currently allows users to: - View Pre-defined Layers at the same time to see different combinations (currently, there are none for VEDA). - Choose between common Basemap Layers that are known favorites. - Have full control over the opacity of any layer or basemap for fine-tuning how the map looks.\n\n\n\nToggle each layer’s visibility by using its checkbox\nAdjust each layer’s opacity by moving its slider\n\n\n\n\n\nSelect a basemap from the dropdown\nAdjust the basemap’s opacity by moving its slider\n\n\n\n\n\n\nPressing the STAC Data button at the top opens the STAC widget that consists of 2 tabs. This tool currently allows users to: - Connect to the VEDA STAC to access collections of mission data. - Discover items per the selected collection, including description, available dates, & direct URL. - Identify valid COG datasets. - Add COG tiles dynamically to the map. - Customize the tiles by changing the selected color palette for the selected item.\n\n\n\nSelect a Collection within the default STAC library.\nBrowse through the Collection’s details.\nSelect an item from the collection to check if it is a valid COG. If it is, the Display button will become active (available) to add the selected item to the map. The displayed STAC layer’s opacity can be adjusted by moving its slider.\n\n\n\n\n\nSelect a category from the dropdown.\nSelect an item from the corresponding color palettes.\nPress the Display button to update the data on the map.\n\n\n\n\n\n\n\nActivate the Interact Tools (click on the top Interact button)\nFrom within the Point tab: Use your mouse to activate the Point tool; then click on the map at a location of interest\n\nCoordinates will be printed within the open tab\nRaster cell values will be identified and printed, if raster layers are on\n\nFrom within the Area tab: Use your mouse to activate the Polygon tool; then click, hold and draw a polygon over the map - releasing to finish\n\nThe area of interest’s Coordinates & BBox within the open tab\nAlternatively Print the area of interest’s bbox from within a cell:\n\n\nClear the point or polygon graphics as needed"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#run-this-notebook",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#run-this-notebook",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "It is recommended to run through this tutorial using VEDA’s Pangeo Notebook image, which already includes the stac_ipyleaflet library. When working within the notebook, be sure to select the nasa-veda-singleuser kernel.\nTo access VEDA’s JupyterHub Environment, please refer to the “Getting access…” section of our documentation.\nFor reference: stac_ipyleafet repository"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#import-the-library-use-the-map",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#import-the-library-use-the-map",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "import stac_ipyleaflet\nm = stac_ipyleaflet.StacIpyleaflet()\nm\n\nThe stac ipyleaflet notebook’s user interface consists of a map and a custom set of tools to aid in the discovery and visualization of STAC datasets and pre-determined Basemaps."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#map-navigation",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#map-navigation",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "press and hold a mouse-click, then drag the map\n\n\n\n\n\nclick the Zoom In / Out buttons in the top left-corner (this will maintain the center)\nuse your mouse’s scroll-wheel - hovering over an area of interest\ndouble-click within the map on an area of interest\nwhile pressing the shift key on your keyboard, press and hold a mouse-click, then drag to draw a rectangle around the area of interest"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#layers-tool",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#layers-tool",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Pressing the Layers button at the top opens the Layers widget that consists of 2 tabs. This tool currently allows users to: - View Pre-defined Layers at the same time to see different combinations (currently, there are none for VEDA). - Choose between common Basemap Layers that are known favorites. - Have full control over the opacity of any layer or basemap for fine-tuning how the map looks.\n\n\n\nToggle each layer’s visibility by using its checkbox\nAdjust each layer’s opacity by moving its slider\n\n\n\n\n\nSelect a basemap from the dropdown\nAdjust the basemap’s opacity by moving its slider"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#stac-discovery-tool",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#stac-discovery-tool",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Pressing the STAC Data button at the top opens the STAC widget that consists of 2 tabs. This tool currently allows users to: - Connect to the VEDA STAC to access collections of mission data. - Discover items per the selected collection, including description, available dates, & direct URL. - Identify valid COG datasets. - Add COG tiles dynamically to the map. - Customize the tiles by changing the selected color palette for the selected item.\n\n\n\nSelect a Collection within the default STAC library.\nBrowse through the Collection’s details.\nSelect an item from the collection to check if it is a valid COG. If it is, the Display button will become active (available) to add the selected item to the map. The displayed STAC layer’s opacity can be adjusted by moving its slider.\n\n\n\n\n\nSelect a category from the dropdown.\nSelect an item from the corresponding color palettes.\nPress the Display button to update the data on the map."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#interact-with-the-map",
    "href": "instance-management/notebooks/tutorials/stac_ipyleaflet.html#interact-with-the-map",
    "title": "Using the stac_ipyleaflet mapping library",
    "section": "",
    "text": "Activate the Interact Tools (click on the top Interact button)\nFrom within the Point tab: Use your mouse to activate the Point tool; then click on the map at a location of interest\n\nCoordinates will be printed within the open tab\nRaster cell values will be identified and printed, if raster layers are on\n\nFrom within the Area tab: Use your mouse to activate the Polygon tool; then click, hold and draw a polygon over the map - releasing to finish\n\nThe area of interest’s Coordinates & BBox within the open tab\nAlternatively Print the area of interest’s bbox from within a cell:\n\n\nClear the point or polygon graphics as needed"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#run-this-notebook",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#run-this-notebook",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#approach",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#approach",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Approach",
    "text": "Approach\n\nUse OWSLib to determine what data is available and inspect the metadata\nUse OWSLib to filter and read the data\nUse geopandas and folium to analyze and plot the data\n\nNote that the default examples environment is missing one requirement: oswlib. We can pip install that before we move on.\n\n%pip install OWSLib==0.28.1 --quiet\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport datetime as dt\n\nimport geopandas as gpd\nfrom owslib.ogcapi.features import Features"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#about-the-data",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#about-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "About the Data",
    "text": "About the Data\nThe fire data shown is generated by the FEDs algorithm. The FEDs algorithm tracks fire movement and severity by ingesting observations from the VIIRS thermal sensors on the Suomi NPP and NOAA-20 satellites. This algorithm uses raw VIIRS observations to generate a polygon of the fire, locations of the active fire line, and estimates of fire mean Fire Radiative Power (FRP). The VIIRS sensors overpass at ~1:30 AM and PM local time, and provide estimates of fire evolution ~ every 12 hours. The data produced by this algorithm describe where fires are in space and how fires evolve through time. This CONUS-wide implementation of the FEDs algorithm is based on Chen et al 2020’s algorithm for California.\nThe data produced by this algorithm is considered experimental."
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Look at the data that is availible through the OGC API",
    "text": "Look at the data that is availible through the OGC API\nThe datasets that are distributed throught the OGC API are organized into collections. We can display the collections with the command:\n\nOGC_URL = \"https://firenrt.delta-backend.com\"\n\nw = Features(url=OGC_URL)\nw.feature_collections()\n\n['public.eis_fire_lf_perimeter_archive',\n 'public.eis_fire_lf_newfirepix_archive',\n 'public.eis_fire_lf_fireline_archive',\n 'public.eis_fire_lf_fireline_nrt',\n 'public.eis_fire_snapshot_fireline_nrt',\n 'public.eis_fire_snapshot_newfirepix_nrt',\n 'public.eis_fire_lf_newfirepix_nrt',\n 'public.eis_fire_perimeter',\n 'public.eis_fire_lf_perimeter_nrt',\n 'public.eis_fire_snapshot_perimeter_nrt',\n 'public.st_squaregrid',\n 'public.st_hexagongrid',\n 'public.st_subdivide']\n\n\nWe will focus on the public.eis_fire_snapshot_fireline_nrt collection, the public.eis_fire_snapshot_perimeter_nrt collection, and the public.eis_fire_lf_perimeter_archive collection here.\n\nInspect the metatdata for public.eis_fire_snapshot_perimeter_nrt collection\nWe can access information that describes the public.eis_fire_snapshot_perimeter_nrt table.\n\nperm = w.collection(\"public.eis_fire_snapshot_perimeter_nrt\")\n\nWe are particularly interested in the spatial and temporal extents of the data.\n\nperm[\"extent\"]\n\n{'spatial': {'bbox': [[-164.04434204101562,\n    24.15606689453125,\n    163.10562133789062,\n    70.45816802978516]],\n  'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84'},\n 'temporal': {'interval': [['2024-07-17T12:00:00+00:00',\n    '2024-08-11T00:00:00+00:00']],\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian'}}\n\n\nIn addition to getting metadata about the data we can access the queryable fields. Each of these fields will represent a column in our dataframe.\n\nperm_q = w.collection_queryables(\"public.eis_fire_snapshot_perimeter_nrt\")\nperm_q[\"properties\"]\n\n{'geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'duration': {'name': 'duration', 'type': 'number'},\n 'farea': {'name': 'farea', 'type': 'number'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'flinelen': {'name': 'flinelen', 'type': 'number'},\n 'fperim': {'name': 'fperim', 'type': 'number'},\n 'geom_counts': {'name': 'geom_counts', 'type': 'string'},\n 'isactive': {'name': 'isactive', 'type': 'number'},\n 'low_confidence_grouping': {'name': 'low_confidence_grouping',\n  'type': 'number'},\n 'meanfrp': {'name': 'meanfrp', 'type': 'number'},\n 'n_newpixels': {'name': 'n_newpixels', 'type': 'number'},\n 'n_pixels': {'name': 'n_pixels', 'type': 'number'},\n 'pixden': {'name': 'pixden', 'type': 'number'},\n 'primarykey': {'name': 'primarykey', 'type': 'string'},\n 'region': {'name': 'region', 'type': 'string'},\n 't': {'name': 't', 'type': 'string'}}"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#filter-the-data",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#filter-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Filter the data",
    "text": "Filter the data\nIt is always a good idea to do any data filtering as early as possible. In this example we know that we want the data for particular spatial and temporal extents. We can apply those and other filters using the OWSLib package.\nIn the below example we are:\n\nchoosing the public.eis_fire_snapshot_perimeter_nrt collection\nsubsetting it by space using the bbox parameter\nsubsetting it by time using the datetime parameter\nfiltering for fires over 5km^2 and over 2 days long using the filter parameter. The filter parameter lets us filter by the columns in ‘public.eis_fire_snapshot_perimeter_nrt’ using SQL-style queries.\n\nNOTE: The limit parameter desginates the maximum number of objects the query will return. The default limit is 10, so if we want to all of the fire perimeters within certain conditions, we need to make sure that the limit is large.\n\n## Get the most recent fire perimeters, and 7 days before most recent fire perimeter\nmost_recent_time = max(*perm[\"extent\"][\"temporal\"][\"interval\"])\nnow = dt.datetime.strptime(most_recent_time, \"%Y-%m-%dT%H:%M:%S+00:00\")\nlast_week = now - dt.timedelta(weeks=1)\nlast_week = dt.datetime.strftime(last_week, \"%Y-%m-%dT%H:%M:%S+00:00\")\nprint(\"Most Recent Time =\", most_recent_time)\nprint(\"Last week =\", last_week)\n\nMost Recent Time = 2024-08-11T00:00:00+00:00\nLast week = 2024-08-04T00:00:00+00:00\n\n\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",  # name of the dataset we want\n    bbox=[\"-106.8\", \"24.5\", \"-72.9\", \"37.3\"],  # coodrinates of bounding box,\n    datetime=[last_week + \"/\" + most_recent_time],  # date range\n    limit=1000,  # max number of items returned\n    filter=\"farea&gt;5 AND duration&gt;2\",  # additional filters based on queryable fields\n)\n\nThe result is a dictionary containing all of the data and some summary fields. We can look at the keys to see what all is in there.\n\nperm_results.keys()\n\ndict_keys(['type', 'id', 'title', 'description', 'numberMatched', 'numberReturned', 'links', 'features'])\n\n\nFor instance you can check the total number of matched items and make sure that it is equal to the number of returned items. This is how you know that the limit you defined above is high enough.\n\nperm_results[\"numberMatched\"] == perm_results[\"numberReturned\"]\n\nTrue\n\n\nYou can also access the data directly in the browser or in an HTTP GET call using the constructed link.\n\nperm_results[\"links\"][1][\"href\"]\n\n'https://firenrt.delta-backend.com/collections/public.eis_fire_snapshot_perimeter_nrt/items?bbox=-106.8%2C24.5%2C-72.9%2C37.3&datetime=2024-08-04T00%3A00%3A00%2B00%3A00%2F2024-08-11T00%3A00%3A00%2B00%3A00&limit=1000&filter=farea%3E5+AND+duration%3E2'"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize Most Recent Fire Perimeters with Firelines",
    "text": "Visualize Most Recent Fire Perimeters with Firelines\nIf we wanted to combine collections to make more informative analyses, we can use some of the same principles.\nFirst we’ll get the queryable fields, and the extents:\n\nfline_q = w.collection_queryables(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_collection = w.collection(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_q[\"properties\"]\n\n{'geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'mergeid': {'name': 'mergeid', 'type': 'number'},\n 'primarykey': {'name': 'primarykey', 'type': 'string'},\n 'region': {'name': 'region', 'type': 'string'},\n 't': {'name': 't', 'type': 'string'}}\n\n\n\nRead\nThen we’ll use those fields to get most recent fire perimeters and fire lines.\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",\n    datetime=most_recent_time,\n    limit=1000,\n)\nperimeters = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\n\n## Get the most recent fire lines\nperimeter_ids = perimeters.fireid.unique()\nperimeter_ids = \",\".join(map(str, perimeter_ids))\n\nfline_results = w.collection_items(\n    \"public.eis_fire_snapshot_fireline_nrt\",\n    limit=1000,\n    filter=\"fireid IN (\"\n    + perimeter_ids\n    + \")\",  # only the fires from the fire perimeter query above\n)\nfline = gpd.GeoDataFrame.from_features(fline_results[\"features\"])\n\n\n\nVisualize\n\nperimeters = perimeters.set_crs(\"epsg:4326\")\nfline = fline.set_crs(\"epsg:4326\")\n\nm = perimeters.explore(zoom_start=5, location=(41, -122))\nm = fline.explore(m=m, color=\"orange\")\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize the Growth of the Camp Fire",
    "text": "Visualize the Growth of the Camp Fire\nWe may be interested in understanding how a fire evolved through time. To do this, we can work with the “Large fire” or “lf” perimeter collections. The public.eis_fire_lf_perimeter_nrt collection has the full spread history of fires from this year. public.eis_fire_lf_perimeter_archive has the full spread history of fires from 2018-2021 that were in the Western United States. The Camp Fire was in 2018, so we will work with the public.eis_fire_lf_perimeter_archive collection.\nWe can start by querying with information specific to the Camp Fire, like it’s genreal region (Northern California), and when it was active (November 2018). With that information, we can get the fireID associated with the Camp Fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",\n    bbox=[\"-124.52\", \"39.2\", \"-120\", \"42\"],  # North California bounding box,\n    datetime=[\"2018-11-01T00:00:00+00:00/2018-11-30T12:00:00+00:00\"],\n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by=\"t\", ascending=False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nprint(perimeters.fireid.unique())\nm = perimeters.explore(\n    style_kwds={\"fillOpacity\": 0}, zoom_start=9, location=(39.7, -121.4)\n)\nm\n\n['F17028' 'F18493']\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nBased on the map, we know that the fireID for the Camp Fire is “F17028”. We can use that to directly query for that particular fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",\n    filter=\"fireid = 'F17028'\",\n    datetime=[\"2018-01-01T00:00:00+00:00/2018-12-31T12:00:00+00:00\"],\n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by=\"t\", ascending=False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nm = perimeters.explore(\n    style_kwds={\"fillOpacity\": 0}, zoom_start=12, location=(39.7, -121.4)\n)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#download-data",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#download-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Download Data",
    "text": "Download Data\nDownloading pre-filtered data may be useful for working locally, or for working with the data in GIS software.\nWe can download the dataframe we made by writing it out into a shapefile or into a GeoJSON file.\nperimeters.to_file('perimeters.shp') \nperimeters.to_file('perimeters.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "instance-management/notebooks/tutorials/mapping-fires.html#collection-information",
    "href": "instance-management/notebooks/tutorials/mapping-fires.html#collection-information",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Collection Information",
    "text": "Collection Information\nThe API hosts 9 different collections. There are four different types of data, and three different time-scales availible for querying through the API. “*snapshot*” collections are useful for visualizing the most recent data. It contains the most recent fires perimeters, active firelines, or VIIRS observations within the last 20 days. “*lf*” collections (short for Large Fire), show every fire perimeter, active fire line, or VIIRS observations for fires over 5 km^2. Collections that end in *archive are for year 2018 - 2021 across the Western United States. Collections with the *nrt ending are for CONUS from this most recent year. FireIDs are consistent only between layers with the same timescale (snapshot, lf_*nrt, and lf_archive*).\npublic.eis_fire_snapshot_perimeter_nrt\nPerimeter of cumulative fire-area. Most recent perimeter from the last 20 days.\npublic.eis_fire_lf_perimeter_nrt\nPerimeter of cumulative fire-area, from fires over 5 km^2. Every fire perimeter from current year to date.\npublic.eis_fire_lf_perimeter_archive\nPerimeter of cumulative fire-area, from fires over 5 km^2 in the Western United States. Every fire perimeter from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nmeanfrp\nMean fire radiative power. The weighted sum of the fire radiative power detected at each new pixel, divided by the number of pixels. If no new pixels are detected, meanfrp is set to zero.\nMW/(pixel area)\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\npixden\nNumber of pixels divided by area of perimeter.\npixels/Km^2\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nKm\n\n\nfperim\nLength of fire perimeter.\nKm\n\n\nfarea\nArea within fire perimeter.\nKm^2\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\nn_pixels\nNumber of pixel-detections in history of fire.\npixels\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\ngeometry\nThe shape of the perimeter.\nGeometry\n\n\n\npublic.eis_fire_snapshot_fireline_nrt\nActive fire line as estimated by new VIIRS detections. Most fire line from the last 20 days.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2. Every fire line from current year to date.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2 in the Western United States. Every fire line from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID\n\n\n\npublic.eis_fire_snapshot_newfirepix_nrt\nNew pixel detections that inform the most recent time-step’s perimeter and fireline calculation from the last 20 days.\npublic.eis_fire_lf_newfirepix_nrt\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible from start of current year to date.\npublic.eis_fire_lf_newfirepix_archive\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible for Western United States from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_compare2_3.html",
    "href": "instance-management/notebooks/datasets/nldas_compare2_3.html",
    "title": "Comparing NLDAS-2 and NLDAS-3 Precipitation Forcing Data",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_compare2_3.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/nldas_compare2_3.html#run-this-notebook",
    "title": "Comparing NLDAS-2 and NLDAS-3 Precipitation Forcing Data",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_compare2_3.html#about-the-data",
    "href": "instance-management/notebooks/datasets/nldas_compare2_3.html#about-the-data",
    "title": "Comparing NLDAS-2 and NLDAS-3 Precipitation Forcing Data",
    "section": "About the Data",
    "text": "About the Data\nNLDAS is a widely used land modeling environment that generates estimates of land surface fluxes and states such as soil moisture, snow, and streamflow. These estimates are critical for drought and flood monitoring, water availability and water resource management, climate assessments, and other uses. NLDAS-3 is the next generation version of NLDAS-2, and offers significant improvements such as improved spatial resolution (12.5km to 1km), expanded domain (CONUS to North and Central America), reduced data latency (3.5 days to near real-time), and assimilation of NASA remote sensing data, among others. (see Earthdata VEDA Data Story (https://www.earthdata.nasa.gov/dashboard/stories/nldas)). Please note that the NLDAS-3 precipitation data provided here is a sample dataset still in development, and will not be the final NLDAS-3 product when it is released.\nThis notebook is intended to visualize and compare the NLDAS-2 and sample NLDAS-3 monthly-averaged precipitation forcing.\n\nApproach\n\nQuery metadata from the VEDA STAC API\nQuery NLDAS-2 and NLDAS-3 Raster data drom the VEDA Raster API\nDisplay the data side-by-side for comparison\n\n\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_compare2_3.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/nldas_compare2_3.html#querying-the-stac-api",
    "title": "Comparing NLDAS-2 and NLDAS-3 Precipitation Forcing Data",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\n# The data can be accessed without credentials via these links\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# The NLDAS-2 and NLDAS-3 are stored in these collections\ncollection_id3 = \"nldas3\"\ncollection_id2 = \"nldas2\"\n\n\n# These lines store the metadata for each collection to allow us to query the STAC easily\ncollection3 = requests.get(f\"{STAC_API_URL}/collections/{collection_id3}\").json()\ncollection2 = requests.get(f\"{STAC_API_URL}/collections/{collection_id2}\").json()\n\n\n# This date can be changed and is the date that will be used to generate the maps\ndate = '2021-02-01'\n\n\n# This line searches for and stores the NLDAS-3 file information that matches the given date\nresponse3 = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id3],\n        \"query\": {\"datetime\": {\"eq\": date+\"T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems3 = response3[\"features\"]\nlen(items3)\n\n1\n\n\n\n# This line searches for and stores the NLDAS-2 file information that matches the given date\nresponse2 = requests.post(\n    f\"{STAC_API_URL}/search\",\n    json={\n        \"collections\": [collection_id2],\n        \"query\": {\"datetime\": {\"eq\": date+\"T00:00:00\"}},\n        \"limit\": 100,\n    },\n).json()\nitems2 = response2[\"features\"]\nlen(items2)\n\n1\n\n\n\n# Get the first matching file for each version and save/visualize it\nitem2 = items2[0]\nitem3 = items3[0]\nitem2, item3\n\n({'id': 'nldas2_LIS_HIST_202102',\n  'bbox': [-168.98000671647551,\n   7.019999961559591,\n   -51.93999145246978,\n   72.06000091582078],\n  'type': 'Feature',\n  'links': [{'rel': 'collection',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas2'},\n   {'rel': 'parent',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas2'},\n   {'rel': 'root',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/'},\n   {'rel': 'self',\n    'type': 'application/geo+json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas2/items/nldas2_LIS_HIST_202102'},\n   {'title': 'Map of Item',\n    'href': 'https://staging.openveda.cloud/api/raster/collections/nldas2/items/nldas2_LIS_HIST_202102/map?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n    'rel': 'preview',\n    'type': 'text/html'}],\n  'assets': {'cog_default': {'href': 's3://veda-data-store-staging/EIS/nldas2monthly/nldas2_LIS_HIST_202102.tif',\n    'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n    'roles': ['data', 'layer'],\n    'title': 'Default COG Layer',\n    'description': 'Cloud optimized default layer to display on map',\n    'raster:bands': [{'scale': 1.0,\n      'nodata': -9999.0,\n      'offset': 0.0,\n      'sampling': 'area',\n      'data_type': 'float32',\n      'histogram': {'max': 574.3709106445312,\n       'min': -4.244315147399902,\n       'count': 11.0,\n       'buckets': [75701.0,\n        14093.0,\n        5868.0,\n        2149.0,\n        419.0,\n        204.0,\n        128.0,\n        63.0,\n        36.0,\n        12.0]},\n      'statistics': {'mean': 40.024527986379255,\n       'stddev': 50.011235463160766,\n       'maximum': 574.3709106445312,\n       'minimum': -4.244315147399902,\n       'valid_percent': 16.90532483552632}}]},\n   'rendered_preview': {'title': 'Rendered preview',\n    'href': 'https://staging.openveda.cloud/api/raster/collections/nldas2/items/nldas2_LIS_HIST_202102/preview.png?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n    'rel': 'preview',\n    'roles': ['overview'],\n    'type': 'image/png'}},\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-168.98000671647551, 7.019999961559591],\n     [-51.93999145246978, 7.019999961559591],\n     [-51.93999145246978, 72.06000091582078],\n     [-168.98000671647551, 72.06000091582078],\n     [-168.98000671647551, 7.019999961559591]]]},\n  'collection': 'nldas2',\n  'properties': {'proj:bbox': [-168.98000671647551,\n    7.019999961559591,\n    -51.93999145246978,\n    72.06000091582078],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1626.0, 2926.0],\n   'end_datetime': '2021-02-28T00:00:00',\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-168.98000671647551, 7.019999961559591],\n      [-51.93999145246978, 7.019999961559591],\n      [-51.93999145246978, 72.06000091582078],\n      [-168.98000671647551, 72.06000091582078],\n      [-168.98000671647551, 7.019999961559591]]]},\n   'proj:transform': [0.04000000521668002,\n    0.0,\n    -168.98000671647551,\n    0.0,\n    -0.0400000005868765,\n    72.06000091582078,\n    0.0,\n    0.0,\n    1.0],\n   'start_datetime': '2021-02-01T00:00:00'},\n  'stac_version': '1.0.0',\n  'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n   'https://stac-extensions.github.io/raster/v1.1.0/schema.json']},\n {'id': 'nldas3_LIS_HIST_202102',\n  'bbox': [-168.98000671647551,\n   7.019999961559591,\n   -51.93999145246978,\n   72.06000091582078],\n  'type': 'Feature',\n  'links': [{'rel': 'collection',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3'},\n   {'rel': 'parent',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3'},\n   {'rel': 'root',\n    'type': 'application/json',\n    'href': 'https://staging.openveda.cloud/api/stac/'},\n   {'rel': 'self',\n    'type': 'application/geo+json',\n    'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3/items/nldas3_LIS_HIST_202102'},\n   {'title': 'Map of Item',\n    'href': 'https://staging.openveda.cloud/api/raster/collections/nldas3/items/nldas3_LIS_HIST_202102/map?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n    'rel': 'preview',\n    'type': 'text/html'}],\n  'assets': {'cog_default': {'href': 's3://veda-data-store-staging/EIS/nldas3monthly/nldas3_LIS_HIST_202102.tif',\n    'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n    'roles': ['data', 'layer'],\n    'title': 'Default COG Layer',\n    'description': 'Cloud optimized default layer to display on map',\n    'raster:bands': [{'scale': 1.0,\n      'nodata': -9999.0,\n      'offset': 0.0,\n      'sampling': 'area',\n      'data_type': 'float32',\n      'histogram': {'max': 428.0843200683594,\n       'min': -0.022795898839831352,\n       'count': 11.0,\n       'buckets': [217417.0,\n        29857.0,\n        15254.0,\n        7576.0,\n        3011.0,\n        1122.0,\n        488.0,\n        77.0,\n        9.0,\n        2.0]},\n      'statistics': {'mean': 30.96841488575868,\n       'stddev': 41.008411449510426,\n       'maximum': 428.0843200683594,\n       'minimum': -0.022795898839831352,\n       'valid_percent': 47.08281935307018}}]},\n   'rendered_preview': {'title': 'Rendered preview',\n    'href': 'https://staging.openveda.cloud/api/raster/collections/nldas3/items/nldas3_LIS_HIST_202102/preview.png?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n    'rel': 'preview',\n    'roles': ['overview'],\n    'type': 'image/png'}},\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-168.98000671647551, 7.019999961559591],\n     [-51.93999145246978, 7.019999961559591],\n     [-51.93999145246978, 72.06000091582078],\n     [-168.98000671647551, 72.06000091582078],\n     [-168.98000671647551, 7.019999961559591]]]},\n  'collection': 'nldas3',\n  'properties': {'proj:bbox': [-168.98000671647551,\n    7.019999961559591,\n    -51.93999145246978,\n    72.06000091582078],\n   'proj:epsg': 4326.0,\n   'proj:shape': [1626.0, 2926.0],\n   'end_datetime': '2021-02-28T00:00:00',\n   'proj:geometry': {'type': 'Polygon',\n    'coordinates': [[[-168.98000671647551, 7.019999961559591],\n      [-51.93999145246978, 7.019999961559591],\n      [-51.93999145246978, 72.06000091582078],\n      [-168.98000671647551, 72.06000091582078],\n      [-168.98000671647551, 7.019999961559591]]]},\n   'proj:transform': [0.04000000521668002,\n    0.0,\n    -168.98000671647551,\n    0.0,\n    -0.0400000005868765,\n    72.06000091582078,\n    0.0,\n    0.0,\n    1.0],\n   'start_datetime': '2021-02-01T00:00:00'},\n  'stac_version': '1.0.0',\n  'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n   'https://stac-extensions.github.io/raster/v1.1.0/schema.json']})\n\n\n\n# Save the relevant statistics for each file\nitem_stats3 = item3['assets']['cog_default']['raster:bands'][0]['statistics']\nrescale_values3 = item_stats3['minimum'], item_stats3['maximum']\n\nitem_stats2 = item2['assets']['cog_default']['raster:bands'][0]['statistics']\nrescale_values2 = item_stats2['minimum'], item_stats2['maximum']\n\n\n# Query the raster API for the NLDAS-3 tile\ntiles3 = requests.get(\n    f\"{RASTER_API_URL}/collections/{collection_id3}/items/{item3['id']}/tilejson.json?\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values3[0]},{rescale_values3[1]}\",\n).json()\ntiles3\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging.openveda.cloud/api/raster/collections/nldas3/items/nldas3_LIS_HIST_202102/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?assets=cog_default&color_formula=gamma+r+1.05&colormap_name=rdbu_r&rescale=-0.022795898839831352%2C428.0843200683594'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-168.98000671647551,\n  7.019999961559591,\n  -51.93999145246978,\n  72.06000091582078],\n 'center': [-110.45999908447266, 39.540000438690186, 0]}\n\n\n\n# Query the raster API for the NLDAS-2 tile\ntiles2 = requests.get(\n    f\"{RASTER_API_URL}/collections/{collection_id2}/items/{item2['id']}/tilejson.json?\"\n    \"&assets=cog_default\"\n    \"&color_formula=gamma+r+1.05&colormap_name=rdbu_r\"\n    f\"&rescale={rescale_values2[0]},{rescale_values2[1]}\",\n).json()\ntiles2\n\n{'tilejson': '2.2.0',\n 'version': '1.0.0',\n 'scheme': 'xyz',\n 'tiles': ['https://staging.openveda.cloud/api/raster/collections/nldas2/items/nldas2_LIS_HIST_202102/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?assets=cog_default&color_formula=gamma+r+1.05&colormap_name=rdbu_r&rescale=-4.244315147399902%2C574.3709106445312'],\n 'minzoom': 0,\n 'maxzoom': 24,\n 'bounds': [-168.98000671647551,\n  7.019999961559591,\n  -51.93999145246978,\n  72.06000091582078],\n 'center': [-110.45999908447266, 39.540000438690186, 0]}"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_compare2_3.html#visualize-nldas-2-and-nldas-3",
    "href": "instance-management/notebooks/datasets/nldas_compare2_3.html#visualize-nldas-2-and-nldas-3",
    "title": "Comparing NLDAS-2 and NLDAS-3 Precipitation Forcing Data",
    "section": "Visualize NLDAS-2 and NLDAS-3",
    "text": "Visualize NLDAS-2 and NLDAS-3\n\nNotice the differences in extent and resolution between the two versions.\n\n# Create and display a DualMap to visualize NLDAS-2 and NLDAS-3 data side-by-side\nmap_layer2 = TileLayer(\n    tiles=tiles2[\"tiles\"][0],\n    attr=\"VEDA\",\n)\nmap_layer3 = TileLayer(\n    tiles=tiles3[\"tiles\"][0],\n    attr=\"VEDA\",\n)\nmap_ = folium.plugins.DualMap(location=[39,-110], zoom_start=3,)\nmap_layer2.add_to(map_.m1)\nmap_layer3.add_to(map_.m2)\nmap_\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_time_series.html",
    "href": "instance-management/notebooks/datasets/nldas_time_series.html",
    "title": "Creating Timeseries with NLDAS-3 Sample Data",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_time_series.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/nldas_time_series.html#run-this-notebook",
    "title": "Creating Timeseries with NLDAS-3 Sample Data",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_time_series.html#about-the-data",
    "href": "instance-management/notebooks/datasets/nldas_time_series.html#about-the-data",
    "title": "Creating Timeseries with NLDAS-3 Sample Data",
    "section": "About the Data",
    "text": "About the Data\nNLDAS is a widely used land modeling environment that generates estimates of land surface fluxes and states such as soil moisture, snow, and streamflow. These estimates are critical for drought and flood monitoring, water availability and water resource management, climate assessments, and other uses. NLDAS-3 is the next generation version of NLDAS-2, and offers significant improvements such as improved spatial resolution (12.5km to 1km), expanded domain (CONUS to North and Central America), reduced data latency (3.5 days to near real-time), and assimilation of NASA remote sensing data, among others. (see Earthdata VEDA Data Story (https://www.earthdata.nasa.gov/dashboard/stories/nldas)). Please note that the NLDAS-3 precipitation data provided here is a sample dataset still in development, and will not be the final NLDAS-3 product when it is released.\nThis notebook is intended to visualize the sample NLDAS-3 monthly-averaged precipitation forcing data via a timeseries.\n\nUsing the VEDA STAC to Create a Timeseries\nIf you need to do analysis over an AOI you can do so via Project VEDA’s STAC.\n\n\nApproach\n\nDefine the URL where the data is stored.\nMake sure the dataset is the size we expected.\nDefine an Area of Interest.\nRetrieve the dataset’s statistics for the AOI.\nPlot the relevant statistic.\n\n\n# Import the following libraries\nimport requests\nimport folium\nimport folium.plugins\nfrom folium import Map, TileLayer\nfrom pystac_client import Client\nimport branca\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_time_series.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/nldas_time_series.html#querying-the-stac-api",
    "title": "Creating Timeseries with NLDAS-3 Sample Data",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\n# Provide the STAC and RASTER API endpoints\n# The endpoint is referring to a location within the API that executes a request on a data collection nesting on the server.\n\n# The STAC API is a catalog of all the existing data collections that are stored in the GHG Center.\nSTAC_API_URL = \"https://openveda.cloud/api/stac\"\n\n# The RASTER API is used to fetch collections for visualization\nRASTER_API_URL = \"https://openveda.cloud/api/raster\"\n\n# The collection name is used to fetch the dataset from the STAC API. First, we define the collection name as a variable\n# Name of the collection for NLDAS-3 data\ncollection_name = \"nldas3\"\n\n# Next, we need to specify the asset name for this collection\n# The asset name is referring to the raster band containing the pixel values for the parameter of interest\nasset_name = \"cog_default\"\n\n\n# Fetch the collection from the STAC API using the appropriate endpoint\n# The 'requests' library allows a HTTP request possible\ncollection = requests.get(f\"{STAC_API_URL}/collections/{collection_name}\").json()\n\n# Print the id of the collection to the console\ncollection['id']\n\n'nldas3'\n\n\n\n# Create a function that would search for a data collection in the VEDA STAC API\n\n# First, we need to define the function\n# The name of the function = \"get_item_count\"\n# The argument that will be passed through the defined function = \"collection_id\"\ndef get_item_count(collection_id):\n   \n    # Set a counter for the number of items existing in the collection\n    count = 0\n\n    # Define the path to retrieve the granules (items) of the collection of interest in the STAC API\n    items_url = f\"{STAC_API_URL}/collections/{collection_id}/items\"\n\n    # Run a while loop to make HTTP requests until there are no more URLs associated with the collection in the STAC API\n    while True:\n\n        # Retrieve information about the granules by sending a \"get\" request to the STAC API using the defined collection path\n        response = requests.get(items_url)\n\n        # If the items do not exist, print an error message and quit the loop\n        if not response.ok:\n            print(\"error getting items\")\n            exit()\n\n        # Return the results of the HTTP response as JSON\n        stac = response.json()\n       \n        # Increase the \"count\" by the number of items (granules) returned in the response\n        count += int(stac[\"context\"].get(\"returned\", 0))\n\n        # Retrieve information about the next URL associated with the collection in the STAC API (if applicable)\n        next = [link for link in stac[\"links\"] if link[\"rel\"] == \"next\"]\n\n        # Exit the loop if there are no other URLs\n        if not next:\n            break\n       \n        # Ensure the information gathered by other STAC API links associated with the collection are added to the original path\n        # \"href\" is the identifier for each of the tiles stored in the STAC API\n        items_url = next[0][\"href\"]\n\n\n    # Return the information about the total number of granules found associated with the collection\n    return count\n\nNow let’s check how many total items are available.\n\n# Apply the function created above \"get_item_count\" to the data collection\nnumber_of_items = get_item_count(collection_name)\n\n# Get the information about the number of granules found in the collection\nitems = requests.get(f\"{STAC_API_URL}/collections/{collection_name}/items?limit=800\").json()[\"features\"]\n\n# Print the total number of items (granules) found\nprint(f\"Found {len(items)} items\")\n\nFound 252 items\n\n\n\n# Examine the first item in the collection\n# Keep in mind that a list starts from 0, 1, 2... therefore items[0] is referring to the first item in the list/collection\nitems[100]\n\n{'id': 'nldas3_LIS_HIST_201308',\n 'bbox': [-168.98000671647551,\n  7.019999961559591,\n  -51.93999145246978,\n  72.06000091582078],\n 'type': 'Feature',\n 'links': [{'rel': 'collection',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3'},\n  {'rel': 'parent',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3'},\n  {'rel': 'root',\n   'type': 'application/json',\n   'href': 'https://staging.openveda.cloud/api/stac/'},\n  {'rel': 'self',\n   'type': 'application/geo+json',\n   'href': 'https://staging.openveda.cloud/api/stac/collections/nldas3/items/nldas3_LIS_HIST_201308'},\n  {'title': 'Map of Item',\n   'href': 'https://staging.openveda.cloud/api/raster/collections/nldas3/items/nldas3_LIS_HIST_201308/map?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n   'rel': 'preview',\n   'type': 'text/html'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/EIS/nldas3monthly/nldas3_LIS_HIST_201308.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'roles': ['data', 'layer'],\n   'title': 'Default COG Layer',\n   'description': 'Cloud optimized default layer to display on map',\n   'raster:bands': [{'scale': 1.0,\n     'nodata': -9999.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'data_type': 'float32',\n     'histogram': {'max': 932.3753051757812,\n      'min': 0.4928821325302124,\n      'count': 11.0,\n      'buckets': [190603.0,\n       65093.0,\n       13685.0,\n       4065.0,\n       1094.0,\n       172.0,\n       54.0,\n       23.0,\n       16.0,\n       8.0]},\n     'statistics': {'mean': 82.75138366816708,\n      'stddev': 64.85307457674915,\n      'maximum': 932.3753051757812,\n      'minimum': 0.4928821325302124,\n      'valid_percent': 47.08281935307018}}]},\n  'rendered_preview': {'title': 'Rendered preview',\n   'href': 'https://staging.openveda.cloud/api/raster/collections/nldas3/items/nldas3_LIS_HIST_201308/preview.png?bidx=1&assets=cog_default&unscale=False&colormap=%7B%221%22%3A+%5B120%2C+120%2C+120%5D%2C+%222%22%3A+%5B130%2C+65%2C+0%5D%2C+%223%22%3A+%5B66%2C+207%2C+56%5D%2C+%224%22%3A+%5B245%2C+239%2C+0%5D%2C+%225%22%3A+%5B241%2C+89%2C+32%5D%2C+%226%22%3A+%5B168%2C+0%2C+0%5D%2C+%227%22%3A+%5B0%2C+143%2C+201%5D%7D&max_size=1024&resampling=nearest&return_mask=True',\n   'rel': 'preview',\n   'roles': ['overview'],\n   'type': 'image/png'}},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-168.98000671647551, 7.019999961559591],\n    [-51.93999145246978, 7.019999961559591],\n    [-51.93999145246978, 72.06000091582078],\n    [-168.98000671647551, 72.06000091582078],\n    [-168.98000671647551, 7.019999961559591]]]},\n 'collection': 'nldas3',\n 'properties': {'proj:bbox': [-168.98000671647551,\n   7.019999961559591,\n   -51.93999145246978,\n   72.06000091582078],\n  'proj:epsg': 4326.0,\n  'proj:shape': [1626.0, 2926.0],\n  'end_datetime': '2013-08-31T00:00:00',\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-168.98000671647551, 7.019999961559591],\n     [-51.93999145246978, 7.019999961559591],\n     [-51.93999145246978, 72.06000091582078],\n     [-168.98000671647551, 72.06000091582078],\n     [-168.98000671647551, 7.019999961559591]]]},\n  'proj:transform': [0.04000000521668002,\n   0.0,\n   -168.98000671647551,\n   0.0,\n   -0.0400000005868765,\n   72.06000091582078,\n   0.0,\n   0.0,\n   1.0],\n  'start_datetime': '2013-08-01T00:00:00'},\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.0.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json']}\n\n\n\n# The bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\n# Create a function that retrieves information regarding a specific granule using its asset name and raster identifier and generates the statistics for it\n\n# The function takes an item (granule) and a JSON (Dallas, TX polygon) as input parameters\ndef generate_stats(item, geojson):\n\n    # A POST request is made to submit the data associated with the item of interest (specific observation) within the Dallas, TX boundaries to compute its statistics\n    result = requests.post(\n\n        # Raster API Endpoint for computing statistics\n        f\"{RASTER_API_URL}/cog/statistics\",\n\n        # Pass the URL to the item, asset name, and raster identifier as parameters\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n\n        # Send the GeoJSON object (Dallas, TX polygon) along with the request\n        json=geojson,\n\n    # Return the response in JSON format\n    ).json()\n\n    # Print the result\n    #print(result)\n\n    # Return a dictionary containing the computed statistics along with the item's datetime information\n    return {\n        **result[\"properties\"],\n        \"datetime\": item[\"properties\"][\"end_datetime\"][:10],\n    }\n\n\n# Generate a for loop that iterates over all the existing items in the collection\nfor item in items:\n\n    # The loop will then retrieve the information for the start datetime of each item in the list\n    print(item[\"properties\"][\"end_datetime\"])\n\n    # Exit the loop after printing the start datetime for the first item in the collection\n    break\n\n2021-12-31T00:00:00\n\n\nLet’s create a bounding box to explore the area of interest (AOI) in Texas\n\n# The Area of Interest (AOI) is set to Dallas, Texas (USA)\ntexas_dallas_aoi = {\n    \"type\": \"Feature\", # Create a feature object\n    \"properties\": {},\n    \"geometry\": { # Set the bounding coordinates for the polygon\n        \"coordinates\": [\n            [\n                # [longitude, latitude]\n                [-96.1, 32.28],  # Southeast Bounding Coordinate\n                [-96.1, 33.28],  # Northeast Bounding Coordinate\n                [-97.58, 33.28], # Northwest Bounding Coordinate\n                [-97.58, 32.28],  # Southwest Bounding Coordinate\n                [-96.1, 32.28]   # Closing the polygon at the Southeast Bounding Coordinate\n            ]\n        ],\n        \"type\": \"Polygon\",\n    },\n}\n\nLet’s visualize the AOI we have just created using folium\n\n# Create a new map to display the generated polygon\naoi_map = Map(\n\n    # Base map is set to OpenStreetMap\n    tiles=\"OpenStreetMap\",\n\n    # Define the spatial properties for the map\n    location=[\n        32.81,-96.93, # coordinates for Dallas, Texas area\n    ],\n\n    # Set the zoom value\n    zoom_start=9, # zoom in or out by increasing or decreasing the value here\n)\n\n# Insert the Dallas, TX polygon to the map\nfolium.GeoJson(texas_dallas_aoi, name=\"Texas, Dallas\").add_to(aoi_map)\n\n# Visualize the map\naoi_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nldas_time_series.html#visualizing-the-data-as-a-time-series",
    "href": "instance-management/notebooks/datasets/nldas_time_series.html#visualizing-the-data-as-a-time-series",
    "title": "Creating Timeseries with NLDAS-3 Sample Data",
    "section": "Visualizing the Data as a Time Series",
    "text": "Visualizing the Data as a Time Series\nWith the function provided above, we can generate statistics for our AOI. In the example below, we’ll explore sample statistics available from one of the tiles.\n\n%%time\n# %%time = Wall time (execution time) for running the code below\n\n# Generate statistics using the created function \"generate_stats\" within the bounding box defined by the \"texas_dallas_aoi\" polygon\nstats = [generate_stats(item, texas_dallas_aoi) for item in items]\n\nstats[0]\n\nCPU times: user 945 ms, sys: 141 ms, total: 1.09 s\nWall time: 3min 10s\n\n\n{'statistics': {'b1': {'min': 5.5441460609436035,\n   'max': 40.02827072143555,\n   'mean': 18.9648380279541,\n   'count': 925.0999755859375,\n   'sum': 17544.37109375,\n   'std': 7.4039084036512905,\n   'median': 18.58078956604004,\n   'majority': 5.5441460609436035,\n   'minority': 5.5441460609436035,\n   'unique': 964.0,\n   'histogram': [[72.0,\n     155.0,\n     139.0,\n     161.0,\n     148.0,\n     120.0,\n     76.0,\n     56.0,\n     22.0,\n     15.0],\n    [5.5441460609436035,\n     8.992558479309082,\n     12.440971374511719,\n     15.889383316040039,\n     19.33779525756836,\n     22.78620719909668,\n     26.234621047973633,\n     29.683032989501953,\n     33.131446838378906,\n     36.579856872558594,\n     40.02827072143555]],\n   'valid_percent': 97.57,\n   'masked_pixels': 24.0,\n   'valid_pixels': 964.0,\n   'percentile_2': 6.959613800048828,\n   'percentile_98': 35.4062614440918}},\n 'datetime': '2021-12-31'}\n\n\n\n# Create a function that converts statistics in JSON format into a pandas DataFrame\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n\n    # Normalize the JSON data\n    df = pd.json_normalize(stats_json)\n\n    # Replace the naming \"statistics.b1\" in the columns\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n\n    # Set the datetime format\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n\n    # Return the cleaned format\n    return df\n\n# Apply the generated function on the stats data\ndf = clean_stats(stats)\n\n# Display the stats for the first 5 granules in the collection in the table\n# Change the value in the parenthesis to show more or a smaller number of rows in the table\ndf.head(5)\n\n\n\n\n\n\n\n\ndatetime\nmin\nmax\nmean\ncount\nsum\nstd\nmedian\nmajority\nminority\nunique\nhistogram\nvalid_percent\nmasked_pixels\nvalid_pixels\npercentile_2\npercentile_98\ndate\n\n\n\n\n0\n2021-12-31\n5.544146\n40.028271\n18.964838\n925.099976\n17544.371094\n7.403908\n18.580790\n5.544146\n5.544146\n964.0\n[[72.0, 155.0, 139.0, 161.0, 148.0, 120.0, 76....\n97.57\n24.0\n964.0\n6.959614\n35.406261\n2021-12-31\n\n\n1\n2021-11-30\n29.220421\n90.284607\n57.790234\n925.099976\n53461.742188\n13.098977\n59.066620\n29.220421\n29.220421\n964.0\n[[54.0, 83.0, 115.0, 113.0, 133.0, 161.0, 166....\n97.57\n24.0\n964.0\n32.481430\n80.630257\n2021-11-30\n\n\n2\n2021-10-31\n71.620056\n130.268082\n95.405159\n925.099976\n88259.312500\n9.503741\n93.906479\n71.620056\n71.620056\n964.0\n[[10.0, 52.0, 171.0, 316.0, 201.0, 106.0, 50.0...\n97.57\n24.0\n964.0\n80.306641\n120.759964\n2021-10-31\n\n\n3\n2021-09-30\n13.006416\n28.573898\n18.290234\n925.099976\n16920.294922\n3.538529\n17.590103\n13.006416\n13.006416\n964.0\n[[133.0, 241.0, 116.0, 110.0, 102.0, 102.0, 74...\n97.57\n24.0\n964.0\n13.719144\n25.504732\n2021-09-30\n\n\n4\n2021-08-31\n80.948761\n148.701004\n109.796944\n925.099976\n101573.148438\n13.211540\n106.925323\n80.948761\n80.948761\n964.0\n[[29.0, 70.0, 175.0, 254.0, 145.0, 101.0, 78.0...\n97.57\n24.0\n964.0\n86.764038\n137.815720\n2021-08-31\n\n\n\n\n\n\n\n\n# Determine the width and height of the plot using the 'matplotlib' library\n# Figure size: 20 representing the width, 10 representing the height\nfig = plt.figure(figsize=(20, 10)) \n\n# Plot the time series analysis of the monthly precipitation changes in Dallas, Texas\nplt.plot(\n    df[\"date\"], # X-axis: date\n    df[\"max\"], # Y-axis: Precipitation value\n    color=\"purple\", # Line color\n    linestyle=\"-\", # Line style\n    linewidth=0.5, # Line width\n    label=\"Precipitation (mm)\", # Legend label\n)\n\nplt.title('Precipitation over time in Dallas TX')\n\n# Display legend\nplt.legend()\n\nplt.show()"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#run-this-notebook",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on the VEDA JupyterHub and as such is designed to be run on a jupyterhub which is associated with an AWS IAM role which has been granted permissions to the VEDA data store via its bucket policy. The instance used provided 16GB of RAM.\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nThe data is in a protected bucket. Please request access by emailng aimee@developmentseed.org or alexandra@developmentseed.org and providing your affiliation, interest in or expected use of the dataset and an AWS IAM role or user Amazon Resource Name (ARN). The team will help you configure the cognito client.\nYou should then run:\n%run -i 'cognito_login.py'"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#approach",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#approach",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Approach",
    "text": "Approach\n\nQuery STAC API and explore item contents for a given collection\nRead and access the data\nVisualize the collection with hvplot\nRun zonal statistics on collection using rasterstats\nVisualize resultant zonal statistics on a choropleth map"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#about-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "About the Data",
    "text": "About the Data\nThe NCEO Aboveground Woody Biomass 2017 dataset is a map for Africa at 100 m spatial resolution which was developed using a combination of LiDAR, Synthetic Aperture Radar (SAR) and optical based data. Aboveground woody biomass (AGB) plays an key role in the study of the Earth’s carbon cycle and response to climate change. Estimation based on Earth Observation measurements is an effective method for regional scale studies and the results are expressed as dry matter in Mg ha-1.\nImportant Note: Users of this dataset should keep in mind that the map is a continental-scale dataset, generated using a combination of different remote sensing data types, with a single method for the whole study area produced in 2017. Users, therefore, should understand that accuracy may vary for different regions and vegetation types."
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#the-case-study---guinea",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "The Case Study - Guinea",
    "text": "The Case Study - Guinea\nMapping and understanding the spatial distribution of AGB is key to understanding carbon dioxide emissions from tropical deforestation through the loss of woody carbon stocks. The resulting carbon fluxes from these land-use changes and vegetation degradation can have negative impacts on the global carbon cycle. Change analysis between AGB maps overtime can display losses in high biomass forests, due to suspected deforestation and forest degredation.\nThe forests of southern Guinea are reported to have some of the highest density AGB of any forest in the world and are one of the most threatened ecoregions in Africa. Importantly, this area was also the epicenter of the 2014 Ebola outbreak, which had a lasting impact on the region. There is more and more evidence that human deforestation activities in this area may have accelerated the spread of the deadly virus as a result of increasing human-bat interactions in the region.\nIn this example we explore the NCEO AGB dataset for 2017, running zonal statistics at the district (administrative 2) level to understand those areas in Guinea that need greatest prioritization for protection and conservation."
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#setting-up-the-environment",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Setting up the Environment",
    "text": "Setting up the Environment\nTo run zonal statistics we’ll need to import a python package called rasterstats into our environment. You can uncomment the following line for installation. This cell needs only needs to be run once.\n\n!pip install rasterstats --quiet"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#querying-the-stac-api",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Querying the STAC API",
    "text": "Querying the STAC API\n\nfrom pystac_client import Client\n\n\n# Provide STAC API endpoint\nSTAC_API_URL = \"https://openveda.cloud/api/stac/\"\n\n# Declare collection of interest - NCEO Biomass\ncollection = \"nceo_africa_2017\"\n\nNow let’s check how many total items are available.\n\nsearch = Client.open(STAC_API_URL).search(collections=[collection])\nitems = list(search.items())\nprint(f\"Found {len(items)} items\")\n\nFound 1 items\n\n\nThis makes sense as there is only one item available: a map for 2017.\n\n# Explore the \"cog_default\" asset of one item to see what it contains\nitems[0].assets[\"cog_default\"].to_dict()\n\n{'href': 's3://nasa-maap-data-store/file-staging/nasa-map/nceo-africa-2017/AGB_map_2017v0m_COG.tif',\n 'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n 'title': 'Default COG Layer',\n 'description': 'Cloud optimized default layer to display on map',\n 'raster:bands': [{'scale': 1,\n   'nodata': 'inf',\n   'offset': 0,\n   'sampling': 'area',\n   'data_type': 'uint16',\n   'histogram': {'max': 429,\n    'min': 0,\n    'count': 11,\n    'buckets': [405348,\n     44948,\n     18365,\n     6377,\n     3675,\n     3388,\n     3785,\n     9453,\n     13108,\n     1186]},\n   'statistics': {'mean': 37.58407913145342,\n    'stddev': 81.36678677343947,\n    'maximum': 429,\n    'minimum': 0,\n    'valid_percent': 50.42436439336373}}],\n 'roles': ['data', 'layer']}\n\n\nExplore through the item’s assets. We can see from the data’s statistics values that the min and max values for the observed values range from 0 to 429 Mg ha-1."
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#reading-and-accessing-the-data",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Reading and accessing the data",
    "text": "Reading and accessing the data\nNow that we’ve explored the dataset through the STAC API, let’s read and access the dataset itself.\n\nimport stackstac\nimport rioxarray\n\n\nda = stackstac.stack(items[0])\nda\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  times = pd.to_datetime(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-52df0ce77b6e6dbb5f28f680fe94f581' (time: 1,\n                                                                band: 1,\n                                                                y: 81025,\n                                                                x: 78078)&gt; Size: 51GB\ndask.array&lt;fetch_raster_window, shape=(1, 1, 81025, 78078), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/16)\n  * time            (time) datetime64[ns] 8B NaT\n    id              (time) &lt;U19 76B 'AGB_map_2017v0m_COG'\n  * band            (band) &lt;U11 44B 'cog_default'\n  * x               (x) float64 625kB -18.27 -18.27 -18.27 ... 51.86 51.86 51.86\n  * y               (y) float64 648kB 37.73 37.73 37.73 ... -35.05 -35.05 -35.05\n    proj:bbox       object 8B {-35.054059016911935, 51.86423292864056, 37.731...\n    ...              ...\n    proj:shape      object 8B {81024, 78077}\n    start_datetime  &lt;U25 100B '2017-01-01T00:00:00+00:00'\n    raster:bands    object 8B {'scale': 1, 'nodata': 'inf', 'offset': 0, 'sam...\n    title           &lt;U17 68B 'Default COG Layer'\n    description     &lt;U47 188B 'Cloud optimized default layer to display on map'\n    epsg            int64 8B 4326\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    crs:         epsg:4326\n    transform:   | 0.00, 0.00,-18.27|\\n| 0.00,-0.00, 37.73|\\n| 0.00, 0.00, 1.00|\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-52df0ce77b6e6dbb5f28f680fe94f581'time: 1band: 1y: 81025x: 78078dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n47.13 GiB\n8.00 MiB\n\n\nShape\n(1, 1, 81025, 78078)\n(1, 1, 1024, 1024)\n\n\nDask graph\n6160 chunks in 3 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         1 1                                                                                                                                                      78078 81025 1\n\n\n\n\nCoordinates: (16)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band(band)&lt;U11'cog_default'array(['cog_default'], dtype='&lt;U11')x(x)float64-18.27 -18.27 ... 51.86 51.86array([-18.274428, -18.27353 , -18.272631, ...,  51.861538,  51.862436,\n        51.863335])y(y)float6437.73 37.73 37.73 ... -35.05 -35.05array([ 37.731937,  37.731039,  37.73014 , ..., -35.051364, -35.052262,\n       -35.053161])proj:bbox()object{-35.054059016911935, 51.8642329...array({-35.054059016911935, 51.86423292864056, 37.73103856358817, -18.273529509559307},\n      dtype=object)end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:transform()object{0, 1, 37.73103856358817, 0.0008...array({0, 1, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:shape()object{81024, 78077}array({81024, 78077}, dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')raster:bands()object{'scale': 1, 'nodata': 'inf', 'o...array({'scale': 1, 'nodata': 'inf', 'offset': 0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429, 'min': 0, 'count': 11, 'buckets': [405348, 44948, 18365, 6377, 3675, 3388, 3785, 9453, 13108, 1186]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429, 'minimum': 0, 'valid_percent': 50.42436439336373}},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['cog_default'], dtype='object', name='band'))xPandasIndexPandasIndex(Index([-18.274427824843425, -18.273529509559307, -18.272631194275185,\n       -18.271732878991067,  -18.27083456370695, -18.269936248422827,\n        -18.26903793313871, -18.268139617854587,  -18.26724130257047,\n        -18.26634298728635,\n       ...\n        51.855249775799365,   51.85614809108348,    51.8570464063676,\n         51.85794472165172,   51.85884303693584,  51.859741352219956,\n        51.860639667504074,   51.86153798278819,  51.862436298072325,\n         51.86333461335644],\n      dtype='float64', name='x', length=78078))yPandasIndexPandasIndex(Index([  37.73193687887226,   37.73103856358814,  37.730140248304025,\n          37.7292419330199,   37.72834361773578,   37.72744530245166,\n        37.726546987167545,   37.72564867188343,   37.72475035659931,\n         37.72385204131518,\n       ...\n        -35.04507586407077, -35.045974179354886, -35.046872494639004,\n        -35.04777080992312,  -35.04866912520724,  -35.04956744049136,\n        -35.05046575577549,  -35.05136407105961,  -35.05226238634373,\n       -35.053160701627846],\n      dtype='float64', name='y', length=81025))Attributes: (4)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))crs :epsg:4326transform :| 0.00, 0.00,-18.27|\n| 0.00,-0.00, 37.73|\n| 0.00, 0.00, 1.00|resolution :0.0008983152841195214\n\n\nIn this example, we’ll explore the data contained in the NCEO AGB collection and analyze it for each of the districts in Guinea. To do this we will need to import district (administrative level 2) boundary layers from below. We will use the Humanitarian Data Exchange (HDX) site to retrieve subnational administrative boundaries for Guinea. Specifically, we will use the geoBoundaries-GIN-ADM2_simplified.geojson which can be accessed here and read them in directly using geopandas.\n\nimport geopandas as gpd\n\nadmin2_gdf = gpd.read_file(\n    \"https://raw.githubusercontent.com/wmgeolab/geoBoundaries/0f0b6f5fb638e7faf115f876da4e77d8f7fa319f/releaseData/gbOpen/GIN/ADM2/geoBoundaries-GIN-ADM2_simplified.geojson\"\n)\n\n\n# check the CRS\nprint(admin2_gdf.crs)\n\nEPSG:4326\n\n\nNow we can use the bounds of the admin boundaries to clip the data to a box containing Guinea.\n\nsubset = da.rio.clip_box(*admin2_gdf.total_bounds)\n\n\n# select the band of interest, as there is only one in this dataset we'll select the default\ndata_band = subset.sel(band=\"cog_default\")\ndata_band\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'stackstac-52df0ce77b6e6dbb5f28f680fe94f581' (time: 1,\n                                                                y: 6104, x: 8289)&gt; Size: 405MB\ndask.array&lt;getitem, shape=(1, 6104, 8289), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/17)\n  * time            (time) datetime64[ns] 8B NaT\n    id              (time) &lt;U19 76B 'AGB_map_2017v0m_COG'\n    band            &lt;U11 44B 'cog_default'\n  * x               (x) float64 66kB -15.09 -15.09 -15.08 ... -7.642 -7.641\n  * y               (y) float64 49kB 12.68 12.68 12.67 ... 7.196 7.195 7.194\n    proj:bbox       object 8B {37.73103856358817, 51.86423292864056, -35.0540...\n    ...              ...\n    start_datetime  &lt;U25 100B '2017-01-01T00:00:00+00:00'\n    raster:bands    object 8B {'scale': 1, 'nodata': 'inf', 'offset': 0, 'sam...\n    title           &lt;U17 68B 'Default COG Layer'\n    description     &lt;U47 188B 'Cloud optimized default layer to display on map'\n    epsg            int64 8B 4326\n    spatial_ref     int64 8B 0\nAttributes:\n    spec:        RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.05405...\n    resolution:  0.0008983152841195214xarray.DataArray'stackstac-52df0ce77b6e6dbb5f28f680fe94f581'time: 1y: 6104x: 8289dask.array&lt;chunksize=(1, 780, 547), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n386.02 MiB\n8.00 MiB\n\n\nShape\n(1, 6104, 8289)\n(1, 1024, 1024)\n\n\nDask graph\n63 chunks in 5 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                     8289 6104 1\n\n\n\n\nCoordinates: (17)time(time)datetime64[ns]NaTarray(['NaT'], dtype='datetime64[ns]')id(time)&lt;U19'AGB_map_2017v0m_COG'array(['AGB_map_2017v0m_COG'], dtype='&lt;U19')band()&lt;U11'cog_default'array('cog_default', dtype='&lt;U11')x(x)float64-15.09 -15.09 ... -7.642 -7.641axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([-15.086307, -15.085409, -15.08451 , ...,  -7.642866,  -7.641968,\n        -7.64107 ])y(y)float6412.68 12.68 12.67 ... 7.195 7.194axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([12.676127, 12.675229, 12.67433 , ...,  7.195505,  7.194607,  7.193709])proj:bbox()object{37.73103856358817, 51.864232928...array({37.73103856358817, 51.86423292864056, -35.054059016911935, -18.273529509559307},\n      dtype=object)end_datetime()&lt;U25'2017-12-31T23:59:59+00:00'array('2017-12-31T23:59:59+00:00', dtype='&lt;U25')proj:geometry()object{'type': 'Polygon', 'coordinates...array({'type': 'Polygon', 'coordinates': [[[-18.273529509559307, -35.054059016911935], [51.86423292864056, -35.054059016911935], [51.86423292864056, 37.73103856358817], [-18.273529509559307, 37.73103856358817], [-18.273529509559307, -35.054059016911935]]]},\n      dtype=object)proj:transform()object{0, 1, 37.73103856358817, 0.0008...array({0, 1, 37.73103856358817, 0.0008983152841195214, -18.273529509559307, -0.0008983152841195214},\n      dtype=object)proj:epsg()int644326array(4326)proj:shape()object{81024, 78077}array({81024, 78077}, dtype=object)start_datetime()&lt;U25'2017-01-01T00:00:00+00:00'array('2017-01-01T00:00:00+00:00', dtype='&lt;U25')raster:bands()object{'scale': 1, 'nodata': 'inf', 'o...array({'scale': 1, 'nodata': 'inf', 'offset': 0, 'sampling': 'area', 'data_type': 'uint16', 'histogram': {'max': 429, 'min': 0, 'count': 11, 'buckets': [405348, 44948, 18365, 6377, 3675, 3388, 3785, 9453, 13108, 1186]}, 'statistics': {'mean': 37.58407913145342, 'stddev': 81.36678677343947, 'maximum': 429, 'minimum': 0, 'valid_percent': 50.42436439336373}},\n      dtype=object)title()&lt;U17'Default COG Layer'array('Default COG Layer', dtype='&lt;U17')description()&lt;U47'Cloud optimized default layer t...array('Cloud optimized default layer to display on map', dtype='&lt;U47')epsg()int644326array(4326)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-15.086756039145303 0.0008983152841195213 0.0 12.676576131852627 0.0 -0.000898315284119521array(0)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['NaT'], dtype='datetime64[ns]', name='time', freq=None))xPandasIndexPandasIndex(Index([-15.086306881503244, -15.085408566219124, -15.084510250935004,\n       -15.083611935650886, -15.082713620366766, -15.081815305082646,\n       -15.080916989798526, -15.080018674514406, -15.079120359230288,\n       -15.078222043946168,\n       ...\n        -7.649154644277727,  -7.648256328993607,  -7.647358013709487,\n        -7.646459698425367,  -7.645561383141249,  -7.644663067857129,\n        -7.643764752573009,  -7.642866437288889,  -7.641968122004771,\n        -7.641069806720651],\n      dtype='float64', name='x', length=8289))yPandasIndexPandasIndex(Index([12.676126974210568,  12.67522865892645, 12.674330343642332,\n        12.67343202835821, 12.672533713074092, 12.671635397789974,\n       12.670737082505852, 12.669838767221734, 12.668940451937612,\n       12.668042136653494,\n       ...\n        7.201793632786206,  7.200895317502088,  7.199997002217966,\n        7.199098686933848,   7.19820037164973,  7.197302056365608,\n         7.19640374108149,  7.195505425797368,   7.19460711051325,\n        7.193708795229131],\n      dtype='float64', name='y', length=6104))Attributes: (2)spec :RasterSpec(epsg=4326, bounds=(-18.274427824843425, -35.054059016911964, 51.86423292864057, 37.73193687887226), resolutions_xy=(0.0008983152841195214, 0.0008983152841195214))resolution :0.0008983152841195214\n\n\n\nimport hvplot.xarray\n\nbiomass = data_band.squeeze()\nbiomass\n\nbiomass.hvplot(\n    x=\"x\",\n    y=\"y\",\n    coastline=True,\n    rasterize=True,\n    cmap=\"viridis\",\n    widget_location=\"bottom\",\n    frame_width=600,\n)"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#zonal-statistics",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Zonal Statistics",
    "text": "Zonal Statistics\nThis map we created above is great, but let’s focus on which districts (administrative level 2 boundaries) should be prioritized for forest conservation.\nZonal statistics is an operation that calculates statistics on the cell values of a raster layer (e.g., the NCEO AGB dataset) within the zones (i.e., polygons) of another dataset. It is an analytical tool that can calculate the mean, median, sum, minimum, maximum, or range in each zone. The zonal extent, often polygons, can be in the form of objects like administrative boundaries, water catchment areas, or field boundaries.\n\nimport pandas as pd\nfrom rasterstats import zonal_stats\n\n\nadmin2_biomass = pd.DataFrame(\n    zonal_stats(\n        admin2_gdf,\n        biomass.values,\n        affine=biomass.rio.transform(),\n        nodata=biomass.rio.nodata,\n        band=1,\n    ),\n    index=admin2_gdf.index,\n)\nadmin2_biomass\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\n\n\n\n\n0\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n0.0\n546.0\n46.846892\n527269\n\n\n2\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n0.0\n345.0\n31.315987\n41660\n\n\n4\n0.0\n494.0\n47.809879\n116515\n\n\n5\n0.0\n422.0\n57.583787\n530195\n\n\n6\n0.0\n548.0\n48.555337\n317191\n\n\n7\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n0.0\n448.0\n44.744092\n403399\n\n\n9\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n0.0\n422.0\n46.973538\n426420\n\n\n11\n0.0\n452.0\n50.016374\n161593\n\n\n12\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n0.0\n563.0\n89.595375\n429232\n\n\n14\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n0.0\n558.0\n65.559085\n955743\n\n\n16\n0.0\n486.0\n48.230132\n897380\n\n\n17\n0.0\n590.0\n90.110284\n630818\n\n\n18\n0.0\n413.0\n47.668705\n364092\n\n\n19\n0.0\n339.0\n37.304653\n544541\n\n\n20\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n0.0\n470.0\n46.776737\n216368\n\n\n22\n0.0\n469.0\n57.435206\n278955\n\n\n23\n0.0\n592.0\n71.918267\n461576\n\n\n24\n0.0\n623.0\n123.937151\n814877\n\n\n25\n0.0\n451.0\n45.058698\n859223\n\n\n26\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n0.0\n592.0\n86.547049\n413435\n\n\n29\n0.0\n560.0\n57.591189\n460766\n\n\n30\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n0.0\n554.0\n57.991285\n771431\n\n\n32\n0.0\n389.0\n49.663329\n615108\n\n\n33\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nNow we’ll join the administrative level 2 boundaries to the zonal statistics results, so that we can map the districts on a choropleth map.\n\nconcat_df = admin2_gdf.join(admin2_biomass)\nconcat_df\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n0\n1\nGN-BE\nBeyla\nADM2\nGIN-ADM2-49546643B63767081\nGIN\nADM2\nPOLYGON ((-8.24559 8.44255, -8.24158 8.45044, ...\n0.0\n565.0\n51.445738\n1276378\n\n\n1\n2\nGN-BF\nBoffa\nADM2\nGIN-ADM2-49546643B69790359\nGIN\nADM2\nMULTIPOLYGON (((-13.77147 9.84445, -13.76994 9...\n0.0\n546.0\n46.846892\n527269\n\n\n2\n3\nGN-BK\nBoke\nADM2\nGIN-ADM2-49546643B67680147\nGIN\nADM2\nMULTIPOLYGON (((-14.57512 10.76872, -14.57633 ...\n0.0\n513.0\n48.862863\n1107831\n\n\n3\n4\nGN-C\nConakry\nADM2\nGIN-ADM2-49546643B26553537\nGIN\nADM2\nMULTIPOLYGON (((-13.78686 9.46592, -13.79013 9...\n0.0\n345.0\n31.315987\n41660\n\n\n4\n5\nGN-CO\nCoyah\nADM2\nGIN-ADM2-49546643B29309121\nGIN\nADM2\nPOLYGON ((-13.49399 9.53945, -13.48050 9.55304...\n0.0\n494.0\n47.809879\n116515\n\n\n5\n6\nGN-DB\nDabola\nADM2\nGIN-ADM2-49546643B70320134\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46752 10.545...\n0.0\n422.0\n57.583787\n530195\n\n\n6\n7\nGN-DL\nDalaba\nADM2\nGIN-ADM2-49546643B47404564\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -12.03171 11.288...\n0.0\n548.0\n48.555337\n317191\n\n\n7\n8\nGN-DI\nDinguiraye\nADM2\nGIN-ADM2-49546643B47728803\nGIN\nADM2\nPOLYGON ((-10.72063 11.13326, -10.72092 11.144...\n0.0\n438.0\n43.717062\n1176897\n\n\n8\n9\nGN-DU\nDubreka\nADM2\nGIN-ADM2-49546643B78750611\nGIN\nADM2\nMULTIPOLYGON (((-13.76504 9.82404, -13.75194 9...\n0.0\n448.0\n44.744092\n403399\n\n\n9\n10\nGN-FA\nFaranah\nADM2\nGIN-ADM2-49546643B99428691\nGIN\nADM2\nPOLYGON ((-11.38731 10.39356, -11.38273 10.350...\n0.0\n533.0\n78.724252\n1315404\n\n\n10\n11\nGN-FO\nForecariah\nADM2\nGIN-ADM2-49546643B32851960\nGIN\nADM2\nMULTIPOLYGON (((-13.32015 9.14776, -13.32062 9...\n0.0\n422.0\n46.973538\n426420\n\n\n11\n12\nGN-FR\nFria\nADM2\nGIN-ADM2-49546643B75641357\nGIN\nADM2\nPOLYGON ((-13.76799 10.27884, -13.73119 10.276...\n0.0\n452.0\n50.016374\n161593\n\n\n12\n13\nGN-GA\nGaoual\nADM2\nGIN-ADM2-49546643B44796554\nGIN\nADM2\nPOLYGON ((-13.84293 11.29667, -13.83242 11.291...\n0.0\n483.0\n52.088640\n1145951\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n14\n15\nGN-KA\nKankan\nADM2\nGIN-ADM2-49546643B19447005\nGIN\nADM2\nPOLYGON ((-8.14727 9.58395, -8.15293 9.58911, ...\n0.0\n503.0\n51.326848\n1769560\n\n\n15\n16\nGN-KE\nKerouane\nADM2\nGIN-ADM2-49546643B28981869\nGIN\nADM2\nPOLYGON ((-8.61661 9.50260, -8.60868 9.51354, ...\n0.0\n558.0\n65.559085\n955743\n\n\n16\n17\nGN-KD\nKindia\nADM2\nGIN-ADM2-49546643B38105311\nGIN\nADM2\nPOLYGON ((-13.11475 9.58669, -13.10890 9.58190...\n0.0\n486.0\n48.230132\n897380\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n18\n19\nGN-KB\nKoubia\nADM2\nGIN-ADM2-49546643B329053\nGIN\nADM2\nPOLYGON ((-11.30453 12.01713, -11.31240 12.021...\n0.0\n413.0\n47.668705\n364092\n\n\n19\n20\nGN-KN\nKoundara\nADM2\nGIN-ADM2-49546643B74925550\nGIN\nADM2\nPOLYGON ((-12.82676 12.14425, -12.76880 12.221...\n0.0\n339.0\n37.304653\n544541\n\n\n20\n21\nGN-KO\nKouroussa\nADM2\nGIN-ADM2-49546643B81289084\nGIN\nADM2\nPOLYGON ((-10.46739 10.53598, -10.46733 10.531...\n0.0\n501.0\n57.187525\n1614332\n\n\n21\n22\nGN-LA\nLabe\nADM2\nGIN-ADM2-49546643B47788034\nGIN\nADM2\nPOLYGON ((-12.01167 11.29091, -11.98685 11.320...\n0.0\n470.0\n46.776737\n216368\n\n\n22\n23\nGN-LE\nLelouma\nADM2\nGIN-ADM2-49546643B80531036\nGIN\nADM2\nPOLYGON ((-12.99636 11.18952, -12.98648 11.187...\n0.0\n469.0\n57.435206\n278955\n\n\n23\n24\nGN-LO\nLola\nADM2\nGIN-ADM2-49546643B51651521\nGIN\nADM2\nPOLYGON ((-8.46455 8.27185, -8.44429 8.25379, ...\n0.0\n592.0\n71.918267\n461576\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n25\n26\nGN-ML\nMali\nADM2\nGIN-ADM2-49546643B68291102\nGIN\nADM2\nPOLYGON ((-12.76304 11.85482, -12.74823 11.857...\n0.0\n451.0\n45.058698\n859223\n\n\n26\n27\nGN-MM\nMamou\nADM2\nGIN-ADM2-49546643B49157402\nGIN\nADM2\nPOLYGON ((-11.15547 11.05524, -11.13717 11.074...\n0.0\n564.0\n74.196642\n1042860\n\n\n27\n28\nGN-MD\nMandiana\nADM2\nGIN-ADM2-49546643B49348937\nGIN\nADM2\nPOLYGON ((-8.13614 10.00000, -8.13498 10.00774...\n0.0\n406.0\n33.353046\n1191380\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435\n\n\n29\n30\nGN-PI\nPita\nADM2\nGIN-ADM2-49546643B22597757\nGIN\nADM2\nPOLYGON ((-12.20899 11.16225, -12.21822 11.152...\n0.0\n560.0\n57.591189\n460766\n\n\n30\n31\nGN-SI\nSiguiri\nADM2\nGIN-ADM2-49546643B98837050\nGIN\nADM2\nPOLYGON ((-10.00475 11.40696, -10.00285 11.401...\n0.0\n392.0\n29.201285\n1813376\n\n\n31\n32\nGN-TE\nTelimele\nADM2\nGIN-ADM2-49546643B10795278\nGIN\nADM2\nPOLYGON ((-13.65247 10.66825, -13.59967 10.711...\n0.0\n554.0\n57.991285\n771431\n\n\n32\n33\nGN-TO\nTougue\nADM2\nGIN-ADM2-49546643B67909893\nGIN\nADM2\nPOLYGON ((-11.74293 10.98745, -11.70851 11.019...\n0.0\n389.0\n49.663329\n615108\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n\n\n\n\n\nBy sorting the results, we can identify those top districts with the highest mean AGB.\n\nconcat_df_sorted = concat_df.sort_values(by=\"mean\", ascending=False)\nconcat_df_sorted.head()\n\n\n\n\n\n\n\n\nOBJECTID\nISO Code\nshapeName\nLevel\nshapeID\nshapeGroup\nshapeType\ngeometry\nmin\nmax\nmean\ncount\n\n\n\n\n33\n34\nGN-YO\nYomou\nADM2\nGIN-ADM2-49546643B32761429\nGIN\nADM2\nPOLYGON ((-9.34981 7.75681, -9.34896 7.75350, ...\n0.0\n588.0\n131.323274\n326021\n\n\n24\n25\nGN-MC\nMacenta\nADM2\nGIN-ADM2-49546643B91718973\nGIN\nADM2\nPOLYGON ((-8.95774 8.77472, -9.01024 8.79308, ...\n0.0\n623.0\n123.937151\n814877\n\n\n17\n18\nGN-KS\nKissidougou\nADM2\nGIN-ADM2-49546643B39508892\nGIN\nADM2\nPOLYGON ((-10.45426 9.10945, -10.45334 9.08925...\n0.0\n590.0\n90.110284\n630818\n\n\n13\n14\nGN-GU\nGueckedou\nADM2\nGIN-ADM2-49546643B59147082\nGIN\nADM2\nPOLYGON ((-10.59971 9.05848, -10.59402 9.05494...\n0.0\n563.0\n89.595375\n429232\n\n\n28\n29\nGN-NZ\nNzerekore\nADM2\nGIN-ADM2-49546643B97455025\nGIN\nADM2\nPOLYGON ((-8.93454 8.25441, -8.93687 8.25503, ...\n0.0\n592.0\n86.547049\n413435"
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#visualizing-the-results-with-a-choropleth-map",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Visualizing the results with a choropleth map",
    "text": "Visualizing the results with a choropleth map\nNow, let’s visualize the results!\n\nimport hvplot.pandas\n\n# renaming the shapeName to District for improved legend\nconcat_df.rename(columns={\"shapeName\": \"District\"}, inplace=True)\n\nagb = concat_df.hvplot(\n    c=\"mean\",\n    width=900,\n    height=500,\n    geo=True,\n    hover_cols=[\"mean\", \"District\"],\n    cmap=\"viridis\",\n    hover_fill_color=\"white\",\n    line_width=1,\n    title=\"Mean Aboveground Woody Biomass per Guinean District (Mg ha-1)\",\n    tiles=\"CartoLight\",\n)\n\nagb\n\n\n\n\n\n  \n\n\n\n\nBy hovering over the map, we can identify the names and mean AGB per district."
  },
  {
    "objectID": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#summary",
    "href": "instance-management/notebooks/datasets/nceo-biomass-statistics.html#summary",
    "title": "Visualizing NCEO Aboveground Woody Biomass 2017 prioritization areas",
    "section": "Summary",
    "text": "Summary\nIn this case study we have successfully performed zonal statistics on the NCEO AGB dataset in Guinea and displayed the results on a choropleth map. The results of this analysis can dispaly those districts which contain the greatest average amount of AGB and should be prioritized for forest protection efforts."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html#run-this-notebook",
    "title": "STAC Item Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html#install-extra-packages",
    "title": "STAC Item Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U rio_stac parse xpystac pystac nbss-upload --quiet\n\n\nfrom datetime import datetime\n\nimport rio_stac\nimport xarray as xr\nfrom parse import parse"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html#create-pystac.item",
    "title": "STAC Item Creation",
    "section": "Create pystac.Item",
    "text": "Create pystac.Item\nIn this section we will be creating a pystac.Item object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nITEM_ID = \"OMI_trno2_0.10x0.10_202212_Col3_V4.nc\"\nSOURCE = \"s3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif\"\n\n\n\nCalculate datetime\nCreate a function that calculates datetime when given an item_id. You can change this to depend on the source instead if that works better.\n\ndef datetime_func(item_id: str) -&gt; datetime:\n    \"\"\"Given the item_id, figure out the datetime\"\"\"\n\n    fields = parse(\"OMI_trno2_0.10x0.10_{year:4}{month:2}_Col3_V4.nc\", item_id)\n    year = int(fields[\"year\"])\n    month = int(fields[\"month\"])\n    day = 1\n    return datetime(year, month, day)\n\nTest out the datetime function:\n\ndatetime_func(ITEM_ID)\n\ndatetime.datetime(2022, 12, 1, 0, 0)\n\n\n\n\nPut it together\nNow take your constants and datetime function and create the STAC Item using rio_stac.\n\nitem = rio_stac.stac.create_stac_item(\n    id=ITEM_ID,\n    source=SOURCE,\n    collection=COLLECTION_ID,\n    input_datetime=datetime_func(ITEM_ID),\n    with_proj=True,\n    with_raster=True,\n    asset_name=\"cog_default\",\n    asset_roles=[\"data\", \"layer\"],\n    asset_media_type=\"image/tiff; application=geotiff; profile=cloud-optimized\",\n)"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html#try-it-out",
    "title": "STAC Item Creation",
    "section": "Try it out!",
    "text": "Try it out!\nNow that you have an item you can try it out and make sure it looks good and passes validation checks.\n\nitem.validate()\n\n['https://schemas.stacspec.org/v1.0.0/item-spec/json-schema/item.json',\n 'https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/raster/v1.1.0/schema.json']\n\n\n\nitem.to_dict()\n\n{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'OMI_trno2_0.10x0.10_202212_Col3_V4.nc',\n 'properties': {'proj:epsg': 4326,\n  'proj:geometry': {'type': 'Polygon',\n   'coordinates': [[[-180.0, -90.0],\n     [180.0, -90.0],\n     [180.0, 90.0],\n     [-180.0, 90.0],\n     [-180.0, -90.0]]]},\n  'proj:bbox': [-180.0, -90.0, 180.0, 90.0],\n  'proj:shape': [1800, 3600],\n  'proj:transform': [0.1, 0.0, -180.0, 0.0, -0.1, 90.0, 0.0, 0.0, 1.0],\n  'proj:projjson': {'$schema': 'https://proj.org/schemas/v0.7/projjson.schema.json',\n   'type': 'GeographicCRS',\n   'name': 'WGS 84',\n   'datum': {'type': 'GeodeticReferenceFrame',\n    'name': 'World Geodetic System 1984',\n    'ellipsoid': {'name': 'WGS 84',\n     'semi_major_axis': 6378137,\n     'inverse_flattening': 298.257223563}},\n   'coordinate_system': {'subtype': 'ellipsoidal',\n    'axis': [{'name': 'Geodetic latitude',\n      'abbreviation': 'Lat',\n      'direction': 'north',\n      'unit': 'degree'},\n     {'name': 'Geodetic longitude',\n      'abbreviation': 'Lon',\n      'direction': 'east',\n      'unit': 'degree'}]},\n   'id': {'authority': 'EPSG', 'code': 4326}},\n  'proj:wkt2': 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]',\n  'datetime': '2022-12-01T00:00:00Z'},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[(-180.0, -90.0),\n    (180.0, -90.0),\n    (180.0, 90.0),\n    (-180.0, 90.0),\n    (-180.0, -90.0)]]},\n 'links': [{'rel': 'collection',\n   'href': 'no2-monthly-diff',\n   'type': 'application/json'}],\n 'assets': {'cog_default': {'href': 's3://veda-data-store-staging/no2-monthly-diff/OMI_trno2_0.10x0.10_202212_Col3_V4.nc.tif',\n   'type': 'image/tiff; application=geotiff; profile=cloud-optimized',\n   'raster:bands': [{'data_type': 'float32',\n     'scale': 1.0,\n     'offset': 0.0,\n     'sampling': 'area',\n     'nodata': -1.2676506002282294e+30,\n     'statistics': {'mean': 12233282717799.0,\n      'minimum': -1.30282195779584e+16,\n      'maximum': 2.082349180465971e+16,\n      'stddev': 416857512760678.5,\n      'valid_percent': 82.7056884765625},\n     'histogram': {'count': 11,\n      'min': -1.30282195779584e+16,\n      'max': 2.082349180465971e+16,\n      'buckets': [20, 138, 881, 421049, 11300, 203, 20, 3, 1, 1]}}],\n   'roles': ['data', 'layer']}},\n 'bbox': [-180.0, -90.0, 180.0, 90.0],\n 'stac_extensions': ['https://stac-extensions.github.io/projection/v1.1.0/schema.json',\n  'https://stac-extensions.github.io/raster/v1.1.0/schema.json'],\n 'collection': 'no2-monthly-diff'}\n\n\n\nPlot it (optional)\nCreate a quick visual to make sure that data loads and visualizes properly.\n\ndata = xr.open_dataset(item).cog_default.isel(time=0)\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'cog_default' (latitude: 1800, longitude: 3600)&gt; Size: 26MB\n[6480000 values with dtype=float32]\nCoordinates:\n  * latitude     (latitude) float64 14kB 89.95 89.85 89.75 ... -89.85 -89.95\n  * longitude    (longitude) float64 29kB -179.9 -179.8 -179.8 ... 179.9 180.0\n    spatial_ref  int32 4B ...\n    time         datetime64[ns] 8B 2022-12-01\nAttributes:\n    nodata:   -1.2676506002282294e+30xarray.DataArray'cog_default'latitude: 1800longitude: 3600...[6480000 values with dtype=float32]Coordinates: (4)latitude(latitude)float6489.95 89.85 89.75 ... -89.85 -89.95units :degrees_northresolution :-0.1crs :EPSG:4326array([ 89.95,  89.85,  89.75, ..., -89.75, -89.85, -89.95])longitude(longitude)float64-179.9 -179.8 ... 179.9 180.0units :degrees_eastresolution :0.1crs :EPSG:4326array([-179.95, -179.85, -179.75, ...,  179.75,  179.85,  179.95])spatial_ref()int32...spatial_ref :GEOGCRS[\"WGS 84\",ENSEMBLE[\"World Geodetic System 1984 ensemble\",MEMBER[\"World Geodetic System 1984 (Transit)\"],MEMBER[\"World Geodetic System 1984 (G730)\"],MEMBER[\"World Geodetic System 1984 (G873)\"],MEMBER[\"World Geodetic System 1984 (G1150)\"],MEMBER[\"World Geodetic System 1984 (G1674)\"],MEMBER[\"World Geodetic System 1984 (G1762)\"],MEMBER[\"World Geodetic System 1984 (G2139)\"],ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ENSEMBLEACCURACY[2.0]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],CS[ellipsoidal,2],AXIS[\"geodetic latitude (Lat)\",north,ORDER[1],ANGLEUNIT[\"degree\",0.0174532925199433]],AXIS[\"geodetic longitude (Lon)\",east,ORDER[2],ANGLEUNIT[\"degree\",0.0174532925199433]],USAGE[SCOPE[\"Horizontal component of 3D system.\"],AREA[\"World.\"],BBOX[-90,-180,90,180]],ID[\"EPSG\",4326]]crs_wkt :GEOGCRS[\"WGS 84\",ENSEMBLE[\"World Geodetic System 1984 ensemble\",MEMBER[\"World Geodetic System 1984 (Transit)\"],MEMBER[\"World Geodetic System 1984 (G730)\"],MEMBER[\"World Geodetic System 1984 (G873)\"],MEMBER[\"World Geodetic System 1984 (G1150)\"],MEMBER[\"World Geodetic System 1984 (G1674)\"],MEMBER[\"World Geodetic System 1984 (G1762)\"],MEMBER[\"World Geodetic System 1984 (G2139)\"],ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ENSEMBLEACCURACY[2.0]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],CS[ellipsoidal,2],AXIS[\"geodetic latitude (Lat)\",north,ORDER[1],ANGLEUNIT[\"degree\",0.0174532925199433]],AXIS[\"geodetic longitude (Lon)\",east,ORDER[2],ANGLEUNIT[\"degree\",0.0174532925199433]],USAGE[SCOPE[\"Horizontal component of 3D system.\"],AREA[\"World.\"],BBOX[-90,-180,90,180]],ID[\"EPSG\",4326]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984 ensemblegrid_mapping_name :latitude_longitudeGeoTransform :-180 0.100000000000000005551115 0 90 0 -0.100000000000000005551115[1 values with dtype=int32]time()datetime64[ns]2022-12-01array('2022-12-01T00:00:00.000000000', dtype='datetime64[ns]')Indexes: (2)latitudePandasIndexPandasIndex(Index([             89.95,  89.85000000000001,              89.75,\n                    89.65,              89.55,              89.45,\n        89.35000000000001,              89.25,              89.15,\n                    89.05,\n       ...\n                   -89.05, -89.15000000000002, -89.25000000000001,\n       -89.35000000000001,             -89.45,             -89.55,\n       -89.65000000000002, -89.75000000000001, -89.85000000000001,\n                   -89.95],\n      dtype='float64', name='latitude', length=1800))longitudePandasIndexPandasIndex(Index([            -179.95,             -179.85,             -179.75,\n       -179.64999999999998, -179.54999999999998,             -179.45,\n                   -179.35,             -179.25, -179.14999999999998,\n       -179.04999999999998,\n       ...\n                    179.05,  179.15000000000003,  179.25000000000006,\n        179.35000000000002,  179.45000000000005,              179.55,\n        179.65000000000003,  179.75000000000006,  179.85000000000002,\n        179.95000000000005],\n      dtype='float64', name='longitude', length=3600))Attributes: (1)nodata :-1.2676506002282294e+30\n\n\n\ndata.hvplot(\"longitude\", \"latitude\")"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "href": "instance-management/notebooks/veda-operations/stac-item-creation.html#upload-this-notebook",
    "title": "STAC Item Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-item.ipynb"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-collection-creation.html",
    "href": "instance-management/notebooks/veda-operations/stac-collection-creation.html",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "href": "instance-management/notebooks/veda-operations/stac-collection-creation.html#run-this-notebook",
    "title": "STAC Collection Creation",
    "section": "",
    "text": "You can launch this notebook in VEDA JupyterHub by clicking the link below.\nLaunch in VEDA JupyterHub (requires access)\n\n\nLearn more\n\n\n\nThis notebook was written on a VEDA JupyterHub instance\nSee (VEDA Analytics JupyterHub Access)[https://nasa-impact.github.io/veda-docs/veda-jh-access.html] for information about how to gain access.\n\n\n\nYou are welcome to run this anywhere you like (Note: alternatively you can run this on https://daskhub.veda.smce.nasa.gov/, MAAP, locally, …), just make sure that the data is accessible, or get in contact with the VEDA team to enable access."
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "href": "instance-management/notebooks/veda-operations/stac-collection-creation.html#install-extra-packages",
    "title": "STAC Collection Creation",
    "section": "Install extra packages",
    "text": "Install extra packages\n\n!pip install -U pystac nbss-upload --quiet\n\n\nfrom datetime import datetime, timezone\nimport pystac"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "href": "instance-management/notebooks/veda-operations/stac-collection-creation.html#create-pystac.collection",
    "title": "STAC Collection Creation",
    "section": "Create pystac.Collection",
    "text": "Create pystac.Collection\nIn this section we will be creating a pystac.Collection object. This is the part of that notebook that you should update.\n\nDeclare constants\nStart by declaring some string and boolean fields.\n\nCOLLECTION_ID = \"no2-monthly-diff\"\nTITLE = \"NO₂ (Diff)\"\nDESCRIPTION = (\n    \"This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors \"\n    \"indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. \"\n    \"Missing pixels indicate areas of no data most likely associated with \"\n    \"cloud cover or snow.\"\n)\nDASHBOARD__IS_PERIODIC = True\nDASHBOARD__TIME_DENSITY = \"month\"\nLICENSE = \"CC0-1.0\"\n\n\n\nExtents\nThe extents indicate the start (and potentially end) times of the data as well as the footprint of the data.\n\n# Time must be in UTC\ndemo_time = datetime.now(tz=timezone.utc)\n\nextent = pystac.Extent(\n    pystac.SpatialExtent([[-180.0, -90.0, 180.0, 90.0]]),\n    pystac.TemporalExtent([[demo_time, None]]),\n)\n\n\n\nProviders\nWe know that the data host, processor, and producter is “VEDA”, but you can include other providers that fill other roles in the data creation pipeline.\n\nproviders = [\n    pystac.Provider(\n        name=\"VEDA\",\n        roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST],\n        url=\"https://github.com/nasa-impact/veda-data-pipelines\",\n    )\n]\n\n\n\nPut it together\nNow take your constants and the extents and providers and create a pystac.Collection\n\ncollection = pystac.Collection(\n    id=COLLECTION_ID,\n    title=TITLE,\n    description=DESCRIPTION,\n    extra_fields={\n        \"dashboard:is_periodic\": DASHBOARD__IS_PERIODIC,\n        \"dashboard:time_density\": DASHBOARD__TIME_DENSITY,\n    },\n    license=LICENSE,\n    extent=extent,\n    providers=providers,\n)\n\n\n\nTry it out!\nNow that you have a collection you can try it out and make sure that it looks how you expect and that it passes validation checks.\n\ncollection.validate()\n\n['https://schemas.stacspec.org/v1.0.0/collection-spec/json-schema/collection.json']\n\n\n\ncollection.to_dict()\n\n{'type': 'Collection',\n 'id': 'no2-monthly-diff',\n 'stac_version': '1.0.0',\n 'description': 'This layer shows changes in nitrogen dioxide (NO₂) levels. Redder colors indicate increases in NO₂. Bluer colors indicate lower levels of NO₂. Missing pixels indicate areas of no data most likely associated with cloud cover or snow.',\n 'links': [],\n 'dashboard:is_periodic': True,\n 'dashboard:time_density': 'month',\n 'title': 'NO₂ (Diff)',\n 'extent': {'spatial': {'bbox': [[-180.0, -90.0, 180.0, 90.0]]},\n  'temporal': {'interval': [['2023-06-12T17:36:30.161697Z', None]]}},\n 'license': 'CC0-1.0',\n 'providers': [{'name': 'VEDA',\n   'roles': [&lt;ProviderRole.PRODUCER: 'producer'&gt;,\n    &lt;ProviderRole.PROCESSOR: 'processor'&gt;,\n    &lt;ProviderRole.HOST: 'host'&gt;],\n   'url': 'https://github.com/nasa-impact/veda-data-pipelines'}]}"
  },
  {
    "objectID": "instance-management/notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "href": "instance-management/notebooks/veda-operations/stac-collection-creation.html#upload-this-notebook",
    "title": "STAC Collection Creation",
    "section": "Upload this notebook",
    "text": "Upload this notebook\nYou can upload the notebook to anyplace you like, but one of the easiest ones is notebook sharing space. Just change the following cell from “Raw” to “Code”, run it and copy the output link.\n\nBefore uploading make sure: 1) you have not hard-coded any secrets or access keys. 2) you have saved this notebook. Hint (ctrl+s) will do it\n\n!nbss-upload new-collection.ipynb"
  },
  {
    "objectID": "instance-management/index.html",
    "href": "instance-management/index.html",
    "title": "Managing Your Own VEDA",
    "section": "",
    "text": "If you are interested in setting up your own instance of VEDA, please email the VEDA team at veda@uah.edu and we will provide direction and support.\nIf you already have your own instance of VEDA, you can find information on the entire process below.\n\nInformation on setting up your own VEDA instance\nInstructions for ingesting data into VEDA Data Services\nHow to add and manage content on your VEDA Dashboard"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VEDA Docs",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is a redeployable data platform to support scientific visualization and analysis. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nThese docs help you use the services provided by the NASA VEDA Platform and learn about the open-source software ecosystem that it is based on.\n\n\n\n\n\n\n\n\n\nAbout the VEDA Project\nWhy use VEDA\nData Access and Management Services\nVEDA Dashboard\nScientific Computing\nGetting Access\nService Status\n\n\n\n\n\n\n\nCreating your Own VEDA Instance\nAdding Content\nUsage Examples and Tutorials\n\n\n\n\n\n\n\nSoftware Architecture\nRepositiories\nExternal Resources",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "VEDA Docs",
    "section": "",
    "text": "VEDA (Visualization, Exploration and Data Analysis) is a redeployable data platform to support scientific visualization and analysis. By combining interactive storytelling with open science principles, VEDA enables researchers to engage new audiences and share their analysis results effectively.\nThese docs help you use the services provided by the NASA VEDA Platform and learn about the open-source software ecosystem that it is based on.\n\n\n\n\n\n\n\n\n\nAbout the VEDA Project\nWhy use VEDA\nData Access and Management Services\nVEDA Dashboard\nScientific Computing\nGetting Access\nService Status\n\n\n\n\n\n\n\nCreating your Own VEDA Instance\nAdding Content\nUsage Examples and Tutorials\n\n\n\n\n\n\n\nSoftware Architecture\nRepositiories\nExternal Resources",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#new-to-veda",
    "href": "index.html#new-to-veda",
    "title": "VEDA Docs",
    "section": "New to VEDA?",
    "text": "New to VEDA?\nIf you are just getting started with open-source geospatial data science (in Python) or want to learn more, you may find the Collection of External Resources useful.\nTo learn from examples, see the VEDA Example Notebooks - examples of open-source data science using VEDA services.\nSelected user groups can also get access to a VEDA-provided JupyterHub service.\nContributions of data, stories, and example code are open to all users affiliated with the project. We strive to make the process as easy as possible. Please see the docs on Contributing.",
    "crumbs": [
      "Welcome"
    ]
  }
]